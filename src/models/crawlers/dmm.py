#!/usr/bin/python
import json
import re
import time  # yapf: disable # NOQA: E402

import urllib3
from lxml import etree

from models.base.web import check_url, get_html, get_url_playwright

urllib3.disable_warnings()  # yapf: disable


# import traceback


def get_title(html):
    result = html.xpath('//h1[@id="title"]/text()')
    if not result:
        result = html.xpath('//h1[@class="item fn bold"]/text()')
    return result[0].strip() if result else ""


def get_actor(html):
    result = html.xpath("//span[@id='performer']/a/text()")
    if not result:
        result = html.xpath("//td[@id='fn-visibleActor']/div/a/text()")
    if not result:
        result = html.xpath("//td[contains(text(),'å‡ºæ¼”è€…')]/following-sibling::td/a/text()")
    return ",".join(result)


def get_actor_photo(actor):
    actor = actor.split(",")
    data = {}
    for i in actor:
        actor_photo = {i: ""}
        data.update(actor_photo)
    return data


def get_mosaic(html):
    result = html.xpath('//li[@class="on"]/a/text()')
    return "é‡Œç•ª" if result and result[0] == "ã‚¢ãƒ‹ãƒ¡" else "æœ‰ç "


def get_studio(html):
    result = html.xpath("//td[contains(text(),'ãƒ¡ãƒ¼ã‚«ãƒ¼')]/following-sibling::td/a/text()")
    return result[0] if result else ""


def get_publisher(html, studio):
    result = html.xpath("//td[contains(text(),'ãƒ¬ãƒ¼ãƒ™ãƒ«')]/following-sibling::td/a/text()")
    return result[0] if result else studio


def get_runtime(html):
    result = html.xpath("//td[contains(text(),'åéŒ²æ™‚é–“')]/following-sibling::td/text()")
    if not result or not re.search(r"\d+", str(result[0])):
        result = html.xpath("//th[contains(text(),'åéŒ²æ™‚é–“')]/following-sibling::td/text()")
    if result and re.search(r"\d+", str(result[0])):
        return re.search(r"\d+", str(result[0])).group()
    return ""


def get_series(html):
    result = html.xpath("//td[contains(text(),'ã‚·ãƒªãƒ¼ã‚º')]/following-sibling::td/a/text()")
    if not result:
        result = html.xpath("//th[contains(text(),'ã‚·ãƒªãƒ¼ã‚º')]/following-sibling::td/a/text()")
    return result[0] if result else ""


def get_year(release):
    return re.search(r"\d{4}", str(release)).group() if release else ""


def get_release(html):
    result = html.xpath("//td[contains(text(),'ç™ºå£²æ—¥')]/following-sibling::td/text()")
    if not result:
        result = html.xpath("//th[contains(text(),'ç™ºå£²æ—¥')]/following-sibling::td/text()")
    if not result:
        result = html.xpath("//td[contains(text(),'é…ä¿¡é–‹å§‹æ—¥')]/following-sibling::td/text()")
    if not result:
        result = html.xpath("//th[contains(text(),'é…ä¿¡é–‹å§‹æ—¥')]/following-sibling::td/text()")

    release = result[0].strip().replace("/", "-") if result else ""
    result = re.findall(r"(\d{4}-\d{1,2}-\d{1,2})", release)
    return result[0] if result else ""


def get_tag(html):
    result = html.xpath("//td[contains(text(),'ã‚¸ãƒ£ãƒ³ãƒ«')]/following-sibling::td/a/text()")
    if not result:
        result = html.xpath(
            "//div[@class='info__item']/table/tbody/tr/th[contains(text(),'ã‚¸ãƒ£ãƒ³ãƒ«')]/following-sibling::td/a/text()"
        )
    return str(result).strip(" ['']").replace("', '", ",")


def get_cover(html, detail_url):
    if "mono/dvd" in detail_url:
        result = html.xpath('//meta[@property="og:image"]/@content')
        if result:
           return result[0]
    elif "dmm.co.jp" in detail_url:
        result = html.xpath('//a[@id="sample-image1"]/img/@src')
        if result:
            # æ›¿æ¢åŸŸåå¹¶è¿”å›ç¬¬ä¸€ä¸ªåŒ¹é…é¡¹
            return re.sub(r'pics.dmm.co.jp', r'awsimgsrc.dmm.co.jp/pics_dig', result[0])
    return ''  # æ— åŒ¹é…æ—¶è¿”å›ç©ºå­—ç¬¦ä¸²


def get_poster(html, cover, detail_url):
    result = html.xpath('//meta[@property="og:image"]/@content')
    if result and "dmm.co.jp/digital" in detail_url:
        result = re.sub(r"pics.dmm.co.jp", r"awsimgsrc.dmm.co.jp/pics_dig", result[0])
        return result
    else:
        return cover.replace("pl.jpg", "ps.jpg")


def get_extrafanart(html, detail_url):
    result = []
    if "mono/dvd" in detail_url:
        result_list = html.xpath("//a[@name='sample-image']/img/@data-lazy")
        i = 1
        for each in result_list:
            each = each.replace("-%s.jpg" % i, "jp-%s.jpg" % i)
            result.append(each)
            i += 1
    elif "dmm.co.jp" in detail_url:
        result_list = html.xpath("//div[@id='sample-image-block']/a/img/@src")
        if not result_list:
            result_list = html.xpath("//a[@name='sample-image']/img/@src")
        i = 0
        for each in result_list:
            each = each.replace("-%s.jpg" % i, "jp-%s.jpg" % i)
            result.append(each)
            i += 1
    return result


def get_director(html):
    result = html.xpath("//td[contains(text(),'ç›£ç£')]/following-sibling::td/a/text()")
    if not result:
        result = html.xpath("//th[contains(text(),'ç›£ç£')]/following-sibling::td/a/text()")
    return result[0] if result else ""

def remove_content(input_string):
    # å®šä¹‰å…³é”®è¯åˆ—è¡¨ï¼Œæ”¯æŒæ™®é€šå­—ç¬¦ä¸²å’Œæ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
    keywords = [
        r"---+",
        "ã€â€»", 
        "ã€ã‚¨ã‚¹ãƒ¯ãƒ³20å‘¨å¹´",
        "ã€åˆ¶ä½œÂ·è‘—ä½œ",
        "ï¼ƒç­é•·P",
        "åˆå›ç„¡æ–™ä½“é¨“ãƒ",
        "ï¼ƒç­é•·P",
        r"â˜….*â˜…",
        "â€» é…ä¿¡",
        "â€»ã“ã¡ã‚‰ã¯",
        "ç‰¹é›†"
    ]
    
    # éå†å…³é”®è¯åˆ—è¡¨ï¼ŒæŒ‰ä¼˜å…ˆçº§é€ä¸€åŒ¹é…
    for keyword in keywords:
        # åˆ¤æ–­æ˜¯å¦æ˜¯æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
        if isinstance(keyword, str) and (keyword.startswith(r"---+") or keyword.startswith(r"â˜….*â˜…")):
            # å¦‚æœæ˜¯æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼ï¼Œç›´æ¥ç¼–è¯‘
            pattern = re.compile(keyword)
        else:
            # å¦‚æœæ˜¯æ™®é€šå­—ç¬¦ä¸²ï¼Œä½¿ç”¨ re.escape è½¬ä¹‰åç¼–è¯‘
            pattern = re.compile(re.escape(keyword))
        
        # æŸ¥æ‰¾åŒ¹é…
        match = pattern.search(input_string)
        if match:
            # print(f"Found match: {keyword}")
            # å¦‚æœæ‰¾åˆ°åŒ¹é…ï¼Œæˆªå–åˆ°åŒ¹é…ç‚¹ä¹‹å‰çš„éƒ¨åˆ†å¹¶è¿”å›
            return input_string[:match.start()].strip()
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•å…³é”®è¯ï¼Œåˆ™è¿”å›åŸå§‹å­—ç¬¦ä¸²
    return input_string.strip()


def get_outline(html, detail_url):
    result = ""
    if "mono/dvd" in detail_url:
        result = html.xpath("normalize-space(string(//div[@class='mg-b20 lh4']/p[@class='mg-b20']))")
        result = remove_content(result)
    elif "dmm.co.jp" in detail_url:
        result = html.xpath(
            "normalize-space(string(//div[@class='wp-smplex']/preceding-sibling::div[contains(@class, 'mg-b20')][1]))"
        )
        result = remove_content(result)
    return result


def get_score(html):
    result = html.xpath("//p[contains(@class,'d-review__average')]/strong/text()")
    return result[0].replace("\\n", "").replace("\n", "").replace("ç‚¹", "") if result else ""


def get_trailer(htmlcode, detail_url):
    trailer_url = ""
    normal_cid = re.findall(r'cid=(.*?)/', detail_url)[0]
    vr_cid = re.findall(r"https://www.dmm.co.jp/digital/-/vr-sample-player/=/cid=([^/]+)", htmlcode)
    if vr_cid:
        cid = vr_cid[0]
        temp_url = "https://cc3001.dmm.co.jp/vrsample/{0}/{1}/{2}/{2}vrlite.mp4".format(cid[:1], cid[:3], cid)
        trailer_url = check_url(temp_url)
    elif normal_cid:
        cid = normal_cid
        if "dmm.co.jp" in detail_url:
            url = (
                "https://www.dmm.co.jp/service/digitalapi/-/html5_player/=/cid=%s/mtype=AhRVShI_/service=digital/floor=videoa/mode=/"
                % cid
            )
        else:
            url = (
                "https://www.dmm.com/service/digitalapi/-/html5_player/=/cid=%s/mtype=AhRVShI_/service=digital/floor=videoa/mode=/"
                % cid
            )

        result, htmlcode = get_html(url)
        try:
            var_params = re.findall(r" = ({[^;]+)", htmlcode)[0].replace(r"\/", "/")
            trailer_url = json.loads(var_params).get("src")
            if trailer_url.startswith("//"):
                trailer_url = "https:" + trailer_url
        except:
            trailer_url = ""
    return trailer_url

def get_detail_url(url_list, number, number2, file_path):
    number_temp = number2.lower().replace("-", "")
    # https://tv.dmm.co.jp/list/?content=mide00726&i3_ref=search&i3_ord=1
    # https://www.dmm.co.jp/digital/videoa/-/detail/=/cid=mide00726/?i3_ref=search&i3_ord=2
    # https://www.dmm.com/mono/dvd/-/detail/=/cid=n_709mmrak089sp/?i3_ref=search&i3_ord=1
    # /cid=snis00900/
    # /cid=snis126/ /cid=snis900/ å›¾ä¸Šé¢æ²¡æœ‰è“å…‰æ°´å°
    # /cid=h_346rebdb00017/
    # /cid=6snis027/ /cid=7snis900/
    number1 = number_temp.replace("000", "")
    number_pre = re.compile(f"(?<=[=0-9]){number_temp[:3]}")
    number_end = re.compile(f"{number_temp[-3:]}(?=(-[0-9])|([a-z]*)?[/&])")
    number_mid = re.compile(f"[^a-z]{number1}[^0-9]")
    temp_list = []
    for each in url_list:
        if (number_pre.search(each) and number_end.search(each)) or number_mid.search(each):
            cid_list = re.findall(r"(cid|content)=([^/&]+)", each)
            if cid_list:
                temp_list.append(each)
                cid = cid_list[0][1]
                if "-" in cid:  # 134cwx001-1
                    if cid[-2:] in file_path:
                        number = cid

    # ç½‘å€æ’åºï¼šdigital(æ•°æ®å®Œæ•´)  >  dvd(æ— å‰ç¼€æ•°å­—ï¼Œå›¾ç‰‡å®Œæ•´)   >   primeï¼ˆæœ‰å‘è¡Œæ—¥æœŸï¼‰   >   premiumï¼ˆæ— å‘è¡Œæ—¥æœŸï¼‰  >  s1ï¼ˆæ— å‘è¡Œæ—¥æœŸï¼‰
    tv_list = []
    digital_list = []
    dvd_list = []
    prime_list = []
    monthly_list = []
    other_list = []
    for i in temp_list:
        if "tv.dmm.co.jp" in i:
            tv_list.append(i)
        elif "/digital/" in i:
            digital_list.append(i)
        elif "/dvd/" in i:
            dvd_list.append(i)
        elif "/prime/" in i:
            prime_list.append(i)
        elif "/monthly/" in i:
            monthly_list.append(i)
        else:
            other_list.append(i)
    dvd_list.sort(reverse=True)
    # ä¸¢å¼ƒ tv_list, å› ä¸ºè·å–å…¶ä¿¡æ¯è°ƒç”¨çš„åç»­ api æ— æ³•è®¿é—®
    detail_url_list = digital_list + dvd_list + prime_list + monthly_list + other_list
    return detail_url_list, number

def main(number, specified_url="", log_info="", req_web="", language="jp", file_path=""):
    start_time = time.time()
    website_name = "dmm"
    req_web += "-> %s" % website_name
    cookies = {"cookie": "uid=abcd786561031111; age_check_done=1;"}
    css_selector = "div[class='flex py-1.5 pl-3'] > a"
    # price_selector = "span.font-bold.text-lg"
    title = ""
    cover_url = ""
    poster_url = ""
    mosaic = "æœ‰ç "
    release = ""
    year = ""
    image_download = False
    image_cut = "right"
    dic = {}
    digits = ""
    if x := re.findall(r"[A-Za-z]+-?(\d+)", number):
        digits = x[0]
        if len(digits) >= 5:
            if digits.startswith("00"):
                number = number.replace(digits, digits[2:])
    number_00 = number.lower().replace(digits, digits.zfill(5)).replace("-", "")  # æ•°å­—ä¸è¶³5ä½åˆ™åœ¨èµ·å§‹ä½è¡¥0, æœç´¢ç»“æœå¤šï¼Œä½†snis-027æ²¡ç»“æœ
    number_no_00 = number.lower().replace("-", "")  # æœç´¢ç»“æœå°‘
    web_info = "\n       "
    log_info += " \n    ğŸŒ dmm"
    debug_info = ""

    if not specified_url:
        search_url = "https://www.dmm.co.jp/search/=/searchstr=%s/sort=ranking/" % number_00  # å¸¦00
        debug_info = "æœç´¢åœ°å€: %s " % search_url
        log_info += web_info + debug_info
    else:
        debug_info = "ç•ªå·åœ°å€: %s " % specified_url
        log_info += web_info + debug_info

    try:
        if "tv.dmm." not in search_url:
            page_url, url_list = get_url_playwright(search_url, cookies=cookies, css_selector=css_selector)
            # print(f"page_url: {page_url}, url_list: {url_list}")
            if not page_url:  # è¯·æ±‚å¤±è´¥
                debug_info = "ç½‘ç»œè¯·æ±‚é”™è¯¯: %s " % search_url
                log_info += web_info + debug_info
                raise Exception(debug_info)

            if re.findall("age_check", page_url):
                debug_info = "å¹´é¾„é™åˆ¶, è¯·ç¡®è®¤cookie æœ‰æ•ˆï¼"
                log_info += web_info + debug_info
                raise Exception(debug_info)
            
            if re.findall("not-available-in-your-region", page_url):  # éæ—¥æœ¬åœ°åŒºé™åˆ¶è®¿é—®
                debug_info = "åœ°åŸŸé™åˆ¶, è¯·ä½¿ç”¨æ—¥æœ¬èŠ‚ç‚¹è®¿é—®ï¼"
                log_info += web_info + debug_info
                raise Exception(debug_info)

            # html = etree.fromstring(htmlcode, etree.HTMLParser())

            # æœªæŒ‡å®šè¯¦æƒ…é¡µåœ°å€æ—¶ï¼Œè·å–è¯¦æƒ…é¡µåœ°å€ï¼ˆåˆšæ‰è¯·æ±‚çš„æ˜¯æœç´¢é¡µï¼‰
            if not specified_url:
                detail_url_list, number = get_detail_url(url_list, number, number, file_path)
                if not detail_url_list:
                    debug_info = "æœç´¢ç»“æœ: æœªåŒ¹é…åˆ°ç•ªå·ï¼"
                    log_info += web_info + debug_info
                    if number_no_00 != number_00:
                        search_url = (
                            "https://www.dmm.co.jp/search/=/searchstr=%s/sort=ranking/" % number_no_00
                        )  # ä¸å¸¦00ï¼Œæ—§ä½œ snis-027
                        debug_info = "å†æ¬¡æœç´¢åœ°å€: %s " % search_url
                        log_info += web_info + debug_info
                        page_url, url_list = get_url_playwright(search_url, cookies=cookies, css_selector=css_selector)
                        if not page_url:  # è¯·æ±‚å¤±è´¥
                            debug_info = "ç½‘ç»œè¯·æ±‚é”™è¯¯: %s " % search_url
                            log_info += web_info + debug_info
                            raise Exception(debug_info)
                        # html = etree.fromstring(htmlcode, etree.HTMLParser())
                        detail_url_list, number = get_detail_url(url_list, number, number_no_00, file_path)
                        if not detail_url_list:
                            debug_info = "æœç´¢ç»“æœ: æœªåŒ¹é…åˆ°ç•ªå·ï¼"
                            log_info += web_info + debug_info

                if not detail_url_list:
                    # å†™çœŸ
                    search_url = "https://www.dmm.com/search/=/searchstr=%s/sort=ranking/" % number_no_00
                    debug_info = "å†æ¬¡æœç´¢åœ°å€: %s " % search_url
                    log_info += web_info + debug_info
                    page_url, url_list = get_url_playwright(search_url, cookies=cookies, css_selector=css_selector)
                    if not page_url:  # è¯·æ±‚å¤±è´¥
                        debug_info = "ç½‘ç»œè¯·æ±‚é”™è¯¯: %s " % search_url
                        log_info += web_info + debug_info
                        raise Exception(debug_info)
                    # html = etree.fromstring(htmlcode, etree.HTMLParser())
                    detail_url_list, number0 = get_detail_url(url_list, number, number_no_00, file_path)
                    if not detail_url_list:
                        debug_info = "æœç´¢ç»“æœ: æœªåŒ¹é…åˆ°ç•ªå·ï¼"
                        log_info += web_info + debug_info

                else:
                    detail_url_list = [re.sub(r"\?.*", "", detail_url) for detail_url in detail_url_list]

        # è·å–è¯¦æƒ…é¡µä¿¡æ¯
        for detail_url in detail_url_list:
            try:
                # è·å– HTML å†…å®¹
                result, htmlcode = get_html(detail_url, cookies=cookies)
                html = etree.fromstring(htmlcode, etree.HTMLParser())
                # æ£€æŸ¥ç½‘ç»œè¯·æ±‚æ˜¯å¦æˆåŠŸ
                if not result:
                    debug_info = "ç½‘ç»œè¯·æ±‚é”™è¯¯: %s " % htmlcode
                    log_info += web_info + debug_info
                    raise Exception(debug_info)
                # æ£€æŸ¥é¡µé¢æ˜¯å¦ä¸º 404
                if "404 Not Found" in str(
                    html.xpath("//span[@class='d-txten']/text()")
                ):  # å¦‚æœé¡µé¢æœ‰ 404ï¼Œè¡¨ç¤ºä¼ å…¥çš„é¡µé¢åœ°å€ä¸å¯¹
                    debug_info = "404! é¡µé¢åœ°å€é”™è¯¯ï¼"
                    log_info += web_info + debug_info
                    raise Exception(debug_info)
                # è·å–æ ‡é¢˜å¹¶æ£€æŸ¥æ˜¯å¦ä¸ºç©º
                title = get_title(html).strip()  # è·å–æ ‡é¢˜
                if not title:
                    debug_info = "æ•°æ®è·å–å¤±è´¥: æœªè·å–åˆ° titleï¼"
                    log_info += web_info + debug_info
                    raise Exception(debug_info)
                # å°è¯•è§£æè¯¦ç»†ä¿¡æ¯
                try:
                    actor = get_actor(html)  # è·å–æ¼”å‘˜
                    cover_url = get_cover(html, detail_url)  # è·å– cover
                    outline = get_outline(html, detail_url)
                    tag = get_tag(html)
                    release = get_release(html)
                    year = get_year(release)
                    runtime = get_runtime(html)
                    score = get_score(html)
                    series = get_series(html)
                    director = get_director(html)
                    studio = get_studio(html)
                    publisher = get_publisher(html, studio)
                    extrafanart = get_extrafanart(html, detail_url)
                    poster_url = get_poster(html, cover_url, detail_url)
                    trailer = get_trailer(htmlcode, detail_url)
                    mosaic = get_mosaic(html)
                    # å¦‚æœæ‰€æœ‰è§£ææˆåŠŸï¼Œç»“æŸå¾ªç¯
                    debug_info = "ç•ªå·åœ°å€: %s " % detail_url
                    log_info += web_info + debug_info
                    break
                except Exception as e:
                    # æ•è·å¼‚å¸¸å¹¶è®°å½•æ—¥å¿—
                    debug_info = "å‡ºé”™: %s" % str(e)
                    log_info += web_info + debug_info
                    raise Exception(debug_info)
            except Exception as e:
                # å¦‚æœå‘ç”Ÿå¼‚å¸¸ï¼Œæ‰“å°æ—¥å¿—å¹¶ç»§ç»­ä¸‹ä¸€ä¸ª URL
                debug_info = "å‡ºé”™: %s" % str(e)
                log_info += web_info + debug_info
                continue
                
        # å¦‚æœå¾ªç¯ç»“æŸåä»æœªæ‰¾åˆ°æœ‰æ•ˆæ•°æ®
        if not title:
            debug_info = "æœªæ‰¾åˆ°æœ‰æ•ˆæ•°æ®: %s" % str(e)
            log_info += web_info + debug_info
            raise Exception(debug_info)
        actor_photo = get_actor_photo(actor)
        if "VR" in title:
            image_download = True
        try:
            dic = {
                "number": number,
                "title": title,
                "originaltitle": title,
                "actor": actor,
                "outline": outline,
                "originalplot": outline,
                "tag": tag,
                "release": release,
                "year": year,
                "runtime": runtime,
                "score": score,
                "series": series,
                "director": director,
                "studio": studio,
                "publisher": publisher,
                "source": "dmm",
                "website": detail_url,
                "actor_photo": actor_photo,
                "cover": cover_url,
                "poster": poster_url,
                "extrafanart": extrafanart,
                "trailer": trailer,
                "image_download": image_download,
                "image_cut": image_cut,
                "log_info": log_info,
                "error_info": "",
                "req_web": req_web
                + "(%ss) "
                % (
                    round(
                        (time.time() - start_time),
                    )
                ),
                "mosaic": mosaic,
                "wanted": "",
            }
            debug_info = "æ•°æ®è·å–æˆåŠŸï¼"
            log_info += web_info + debug_info
            dic["log_info"] = log_info
        except Exception as e:
            debug_info = "æ•°æ®ç”Ÿæˆå‡ºé”™: %s" % str(e)
            log_info += web_info + debug_info
            raise Exception(debug_info)

    except Exception as e:
        # print(traceback.format_exc())
        debug_info = str(e)
        dic = {
            "title": "",
            "cover": "",
            "website": "",
            "log_info": log_info,
            "error_info": debug_info,
            "req_web": req_web
            + "(%ss) "
            % (
                round(
                    (time.time() - start_time),
                )
            ),
        }
    dic = {website_name: {"zh_cn": dic, "zh_tw": dic, "jp": dic}}
    js = json.dumps(dic, ensure_ascii=False, sort_keys=False, indent=4, separators=(",", ": "))  # .encode('UTF-8')
    return js


if __name__ == "__main__":
    # yapf: disable
    # print(main('ipz-825'))    # æ™®é€šï¼Œæœ‰é¢„å‘Šç‰‡
    # print(main('SIVR-160'))     # vrï¼Œæœ‰é¢„å‘Šç‰‡
    # print(main('enfd-5301'))  # å†™çœŸï¼Œæœ‰é¢„å‘Šç‰‡
    # print(main('h_346rebdb00017'))  # æ— é¢„å‘Šç‰‡
    # print(main('', 'https://www.dmm.com/mono/dvd/-/detail/=/cid=n_641enfd5301/'))
    # print(main('', 'https://www.dmm.co.jp/rental/ppr/-/detail/=/cid=4ssis243/?i3_ref=search&i3_ord=1'))
    # print(main('NKD-229'))
    # print(main('rebdb-017'))         # æµ‹è¯•æœç´¢ï¼Œæ— è§†é¢‘
    # print(main('STARS-199'))    # posterå›¾ç‰‡
    # print(main('ssis301'))  # æ™®é€šé¢„å‘Šç‰‡
    # print(main('hnvr00015'))
    # print(main('QNBM-094'))
    # print(main('ssis-243'))
    # print(main('1459525'))
    # print(main('ssni888'))    # detail-sample-movie 1ä¸ª
    # print(main('snis-027'))
    # print(main('gs00002'))
    # print(main('SMBD-05'))
    # print(main('cwx-001', file_path='134cwx001-1.mp4'))
    # print(main('ssis-222'))
    # print(main('snis-036'))
    # print(main('GLOD-148'))
    # print(main('ï¼ˆæŠ±ãæ•ã‚«ãƒãƒ¼ä»˜ãï¼‰è‡ªå®…è­¦å‚™å“¡ 1stãƒŸãƒƒã‚·ãƒ§ãƒ³ ã‚¤ã‚¤ãƒŠãƒªå·¨ä¹³é•·å¥³ãƒ»ã•ã‚„ã‹ï½ç·¨'))    # ç•ªå·æœ€åæœ‰å­—æ¯
    # print(main('ã‚¨ãƒ­ã‚³ãƒ³ãƒ“ãƒ‹åº—é•· æ³£ãã¹ãè“®ã£è‘‰ãƒ»æ ã€œãŠä»•ç½®ãã˜ã‡ã‚‰ã—ãƒãƒŠãƒé€¸æ©Ÿã€œ'))
    # print(main('åˆã‚ã¦ã®ãƒ’ãƒˆãƒ…ãƒ ç¬¬4è©± ãƒ“ãƒƒãƒãªå¥³å­ã®æ‹æ„›ç›¸è«‡'))
    # print(main('ACMDP-1035'))
    # print(main('JUL-066'))
    # print(main('mide-726'))
    # print(main('1dandy520'))
    # print(main('ome-210'))
    # print(main('ftbd-042'))
    # print(main('mmrak-089'))
    # print(main('', 'https://tv.dmm.co.jp/list/?content=juny00018'))
    # print(main('snis-900'))
    # print(main('n1581'))
    # print(main('ssni-888'))
    # print(main('ssni00888'))
    # print(main('ssni-288'))
    # print(main('', 'https://www.dmm.co.jp/digital/videoa/-/detail/=/cid=ssni00288/'))
    # print(main('ä¿ºã‚’ã‚¤ã‚¸ãƒ¡ã¦ãŸåœ°å…ƒãƒ¤ãƒ³ã‚­ãƒ¼ã®å·¨ä¹³å½¼å¥³ã‚’å¯ã¨ã£ã¦å¾©è®ã‚’æœãŸã™è©± The Motion Anime'))  # æ¨¡ç³ŠåŒ¹é… MAXVR-008
    # print(main('', 'https://www.dmm.co.jp/mono/dvd/-/detail/=/cid=h_173dhry23/'))   # åœ°åŸŸé™åˆ¶
    # print(main('ssni00288'))
    # print(main('ssni00999'))
    # print(main('ipx-292'))
    # print(main('wicp-002')) # æ— è§†é¢‘
    # print(main('ssis-080'))
    # print(main('DV-1562'))
    # print(main('mide00139', "https://www.dmm.co.jp/digital/videoa/-/detail/=/cid=mide00139"))
    # print(main('mide00139', ""))
    # print(main('kawd00969'))
    # print(main('', 'https://tv.dmm.com/vod/detail/?title=5533ftbd00042&season=5533ftbd00042'))
    # print(main('stars-779'))
    # print(main('FAKWM-001', 'https://tv.dmm.com/vod/detail/?season=5497fakwm00001'))
    # print(main('FAKWM-064', 'https://tv.dmm.com/vod/detail/?season=5497fakwm00064'))
    # print(main('IPZ-791'))
    # print(main('FPRE-113'))
    # print(main('fpre00113'))
    # print(main('FPRE113'))
    # print(main('ABF-164'))
    # print(main('ABF-203'))
    # print(main('IPZZ-300'))
    # print(main('HODV-21938'))
    # print(main('HAVD-459'))
    # print(main('PRBY-089'))
    pass
