"""
åˆ®å‰Šè¿‡ç¨‹çš„ç½‘ç»œæ“ä½œ
"""

import os
import re
import shutil
import time
import traceback
import urllib
import math

from lxml import etree

from models.base.file import copy_file, delete_file, move_file, split_path
from models.base.image import check_pic, cut_thumb_to_poster
from models.base.pool import Pool
from models.base.utils import get_used_time
from models.base.web import check_url, get_amazon_data, get_big_pic_by_google, get_html, get_imgsize, multi_download
from models.config.config import config
from models.core.flags import Flags
from models.core.utils import convert_half
from models.signals import signal
from datetime import datetime

def get_actorname(number):
    # è·å–çœŸå®æ¼”å‘˜åå­—
    url = f"https://av-wiki.net/?s={number}"
    result, res = get_html(url)
    if not result:
        return False, f"Error: {res}"
    html_detail = etree.fromstring(res, etree.HTMLParser(encoding="utf-8"))
    actor_box = html_detail.xpath('//ul[@class="post-meta clearfix"]')
    for each in actor_box:
        actor_name = each.xpath('li[@class="actress-name"]/a/text()')
        actor_number = each.xpath('li[@class="actress-name"]/following-sibling::li[last()]/text()')
        if actor_number:
            if actor_number[0].upper().endswith(number.upper()) or number.upper().endswith(actor_number[0].upper()):
                return True, ",".join(actor_name)
    return False, "No Result!"


def get_yesjav_title(json_data, movie_number):
    yesjav_url = "http://www.yesjav.info/search.asp?q=%s&" % movie_number
    movie_title = ""
    result, response = get_html(yesjav_url)
    if result and response:
        parser = etree.HTMLParser(encoding="utf-8")
        html = etree.HTML(response, parser)
        movie_title = html.xpath(
            '//dl[@id="zi"]/p/font/a/b[contains(text(), $number)]/../../a[contains(text(), "ä¸­æ–‡å­—å¹•")]/text()',
            number=movie_number,
        )
        if movie_title:
            movie_title = movie_title[0]
            for each in config.char_list:
                movie_title = movie_title.replace(each, "")
            movie_title = movie_title.strip()
    return movie_title


def google_translate(title, outline):
    e1 = None
    e2 = None
    if title:
        title, e1 = _google_translate(title)
    if outline:
        outline, e2 = _google_translate(outline)
    return title, outline, e1 or e2


def _google_translate(msg: str) -> (str, str):
    try:
        msg_unquote = urllib.parse.unquote(msg)
        url = f"https://translate.google.com/translate_a/single?client=gtx&sl=auto&tl=zh-CN&dt=t&q={msg_unquote}"
        result, response = get_html(url, json_data=True)
        if not result:
            return msg, f"è¯·æ±‚å¤±è´¥ï¼å¯èƒ½æ˜¯è¢«å°äº†ï¼Œå¯å°è¯•æ›´æ¢ä»£ç†ï¼é”™è¯¯ï¼š{response}"
        return "".join([sen[0] for sen in response[0]]), ""
    except Exception as e:
        return msg, str(e)


def download_file_with_filepath(json_data, url, file_path, folder_new_path):
    if not url:
        return False

    if not os.path.exists(folder_new_path):
        os.makedirs(folder_new_path)
    try:
        if multi_download(url, file_path):
            return True
    except:
        pass
    json_data["logs"] += f"\n ğŸ¥º Download failed! {url}"
    return False


def _mutil_extrafanart_download_thread(task):
    json_data, extrafanart_url, extrafanart_file_path, extrafanart_folder_path, extrafanart_name = task
    if download_file_with_filepath(json_data, extrafanart_url, extrafanart_file_path, extrafanart_folder_path):
        if check_pic(extrafanart_file_path):
            return True
    else:
        json_data["logs"] += f"\n ğŸ’¡ {extrafanart_name} download failed! ( {extrafanart_url} )"
        return False

def _split_actor(raw_actor_list):
    """
    æ‹†åˆ†å«æœ‰æ‹¬å·çš„æ¼”å‘˜åï¼Œå¹¶è¿”å›å»é‡åçš„æ¼”å‘˜åˆ—è¡¨ã€‚
    å‚æ•°:
        raw_actor_list (list): åŸå§‹æ¼”å‘˜åˆ—è¡¨, å¯èƒ½åŒ…å«å¸¦æ‹¬å·çš„æ¼”å‘˜å
    è¿”å›:
        list: å»é‡åçš„æ¼”å‘˜åˆ—è¡¨ã€‚
    """
    # åˆ›å»ºä¸€ä¸ªç©ºé›†åˆç”¨äºå­˜å‚¨ç»“æœ
    actor_list = []
    # éå†åˆ—è¡¨
    for item in raw_actor_list:
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…æ‹¬å·å†…å®¹
        match = re.match(r"(.+?)[ï¼ˆ\(](.+?)[ï¼‰\)]", item)
        if match:  # å¦‚æœåŒ¹é…æˆåŠŸ
            name_before_bracket = match.group(1).strip()  # æ‹¬å·å‰çš„å†…å®¹
            name_in_bracket = match.group(2).strip()      # æ‹¬å·å†…çš„å†…å®¹
            
            # å°†æ‹†åˆ†åçš„ä¸¤ä¸ªéƒ¨åˆ†æ·»åŠ åˆ°é›†åˆä¸­
            actor_list.append(name_before_bracket)
            actor_list.append(name_in_bracket)
        else:
            # å¦‚æœæ²¡æœ‰æ‹¬å·ï¼Œç›´æ¥æ·»åŠ åˆ°é›†åˆä¸­
            actor_list.append(item.strip())

    # å°†é›†åˆè½¬æ¢ä¸ºåˆ—è¡¨
    return list(dict.fromkeys(actor_list))


def _get_actor_list(json_data, title, raw_actor_list):
    """
    å¯¹å«æœ‰æ‹¬å·çš„æ¼”å‘˜åè¿›è¡Œæ‹†åˆ†æ•´åˆï¼Œè¿”å›å»é‡æ¼”å‘˜åˆ—è¡¨, å¹¶ä¸”å°†æœ€ç¬¦åˆçš„æ¼”å‘˜åç½®äºé¦–ä½ã€‚
    å…¥å‚:
        json_data (dict): åˆ®å‰Šè·å¾—çš„JSONæ•°æ®
        title (str): åˆ®å‰Šè·å¾—çš„åŸæ ‡é¢˜
        raw_actor_list (list): åˆ®å‰Šè·å¾—çš„åŸå§‹æ¼”å‘˜åˆ—è¡¨
    è¿”å›:
        æ•´åˆåçš„æ¼”å‘˜åˆ—è¡¨
    """
    print(f"åŸå§‹æ ‡é¢˜: {title}")
    print(f"åŸå§‹æ¼”å‘˜åˆ—è¡¨: {raw_actor_list}")
    
    raw_actor_in_title = json_data.get("amazon_orginaltitle_actor")
    print(f"åŸå§‹æ ‡é¢˜ä¸­çš„æ¼”å‘˜: {raw_actor_in_title}")
    
    
    # è°ƒç”¨ _split_actor å‡½æ•°å¤„ç†æ¼”å‘˜åˆ—è¡¨
    actor_list = _split_actor(raw_actor_list) if raw_actor_list else []
    actor_in_title_list = _split_actor([raw_actor_in_title]) if raw_actor_in_title else []
    
    # å°†æ ‡é¢˜ä¸­çš„æ¼”å‘˜åæ”¾åœ¨é¦–ä½
    combined_actor_list = actor_in_title_list + actor_list
    
    for actor in combined_actor_list:
        # æ­¤å¤„çš„ titleå·²ç»å»é™¤äº†æœ«å°¾çš„æ¼”å‘˜å, ä½†æ˜¯æ ‡é¢˜ä¸­é—´ä¾ç„¶å¯èƒ½åŒ…å«æ¼”å‘˜å, å¦‚æœåŒ¹é…åˆ™æ”¾ç½®é¦–ä½
        if actor in title:
            print(f"æ ‡é¢˜ä¸­çš„æ¼”å‘˜å: {actor}")
            combined_actor_list.insert(0, actor)
            break
    
    # å»é‡å¹¶ä¿æŒé¡ºåº
    actor_list = list(dict.fromkeys(combined_actor_list))
    # é€‰å–é¦–ä½æœ€ç¬¦åˆçš„æ¼”å‘˜åç”¨ä»¥æ·»åŠ åˆ°æ ‡é¢˜æœ«å°¾
    best_match_actor = actor_list[0] if actor_list else ""
    print(f"æ•´åˆåçš„æ¼”å‘˜åˆ—è¡¨: {actor_list}")
    print(f"æœ€ç¬¦åˆçš„æ¼”å‘˜å: {best_match_actor}")
    return actor_list, best_match_actor

def _add_actor_to_title(title_list, best_match_actor):
    """
    å°†æœ€ç¬¦åˆçš„æ¼”å‘˜åæ·»åŠ åˆ°æ ‡é¢˜æœ«å°¾, ä¸åŸæ ‡é¢˜äº¤é”™æ’å…¥æ–°åˆ—è¡¨ä¸­
    """
    if best_match_actor:
        search_title_list = []
        titles_with_actor = [title + " " + best_match_actor for title in title_list]
        for title, title_with_actor in zip(title_list, titles_with_actor):
            search_title_list.append(title)
            search_title_list.append(title_with_actor)
        return search_title_list
    else:
        return title_list

def _split_title(original_title,
                 actor_list,
                 best_match_actor,
                 min_length=3,
                 pattern=None,
                 separator=" ",
                 extra_separator=None
                 ):
    """
    å…¥å‚ original_title ä¸ºåŸå§‹æ ‡é¢˜å¹¶ä¸”å»é™¤äº†æœ«å°¾çš„æ¼”å‘˜å, å°†å…¶æ‹†åˆ†æ•´åˆç”¨ä½œAmazonæœç´¢
    1. ç§»é™¤æ ‡é¢˜æœ«å°¾çš„æ¼”å‘˜å
    2. æ­£åˆ™åŒ¹é…æ ‡é¢˜ä¸­çš„ pattern å¹¶ç§»é™¤;
    3. å¯¹æ ‡é¢˜è¿›è¡Œæ•æ„Ÿè¯è½¬æ¢ï¼Œè‹¥è½¬æ¢åç»“æœä¸åŒåˆ™åŠ å…¥ç»“æœåˆ—è¡¨;
    4. å¦‚æœæ ‡é¢˜é•¿åº¦ä¸è¶…è¿‡ min_length åˆ™ç›´æ¥æ·»åŠ æ¼”å‘˜å;
    5. æ„å»ºåˆ†éš”ç¬¦æ­£åˆ™è¡¨è¾¾å¼;
    6. è‹¥åŸå§‹æ ‡é¢˜ä¸åŒ…å«ä»»ä½•åˆ†éš”ç¬¦ï¼Œåˆ™ç›´æ¥è¿”å›åŸºç¡€æ ‡é¢˜åˆ—è¡¨;
    7. å¦åˆ™æŒ‰ä¸»åˆ†éš”ç¬¦æ‹†åˆ†å¹¶è¿‡æ»¤æ— æ•ˆå­ä¸²;
    8. è‹¥æœ‰é¢å¤–åˆ†éš”ç¬¦ï¼Œç»§ç»­æŒ‰å…¶æ‹†åˆ†;
    9. æœ€ç»ˆè¿”å›åŸå§‹æ ‡é¢˜ä¸æ•æ„Ÿè¯è½¬æ¢åçš„æ ‡é¢˜åˆ—è¡¨, ä»¥åŠæ‰€æœ‰æœ‰æ•ˆæ ‡é¢˜ç‰‡æ®µçš„å»é‡åˆ—è¡¨ã€‚
    """
    if actor_list:
        for actor in actor_list:
            original_title = original_title.rstrip()
            if original_title.endswith(actor):
                # ç§»é™¤ç»“å°¾çš„æ¼”å‘˜å
                original_title = original_title[:-len(actor)].strip()
    if pattern:
        original_title = re.sub(pattern, "", original_title).strip()
    
    # åˆå§‹åŒ–åŸæ ‡é¢˜åˆ—è¡¨
    no_split_title_list = [original_title]
    
    # æ•æ„Ÿè¯è½¬æ¢
    original_title_convert_list = convert_half(original_title, operation_flags=0b001)
    for title in original_title_convert_list:
        if title != original_title:
            no_split_title_list.append(title)

    # å»é‡å¹¶ä¿æŒé¡ºåº
    no_split_title_list = list(dict.fromkeys(no_split_title_list))
    
    # å¦‚æœæ ‡é¢˜é•¿åº¦ä¸è¶…è¿‡ min_lengthï¼Œåˆ™ç›´æ¥æ·»åŠ æ¼”å‘˜å
    if (
        len(original_title) <= min_length
        and best_match_actor
    ):
        print(f"æ ‡é¢˜é•¿åº¦æœªè¶…è¿‡ min_length = {min_length}, ç›´æ¥æ·»åŠ æ¼”å‘˜å")
        for idx in range(len(no_split_title_list)):
            no_split_title_list[idx] = no_split_title_list[idx] + " " + best_match_actor
        return no_split_title_list, no_split_title_list
        
    # æ„é€ åˆ†éš”ç¬¦çš„æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
    pattern_parts = []
    separator_list = [separator]
    if extra_separator:
        separator_list.extend(extra_separator.split(","))
    for each_sep in separator_list:
        pattern_parts.append(re.escape(each_sep))
    pattern = "|".join(pattern_parts)
    
    # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°åˆ†éš”ç¬¦ï¼Œç›´æ¥è¿”å›åŸºç¡€æ ‡é¢˜åˆ—è¡¨
    if not re.search(pattern, original_title):
        print(f"æ ‡é¢˜æ— éœ€åˆ†å‰², ç›´æ¥è¿”å›åŸºç¡€æ ‡é¢˜åˆ—è¡¨")
        search_title_list = _add_actor_to_title(no_split_title_list, best_match_actor)
        return search_title_list, search_title_list
    
    def is_valid_part(part, actor_list):
        """
        åˆ¤æ–­ä¸€ä¸ªç‰‡æ®µæ˜¯å¦æœ‰æ•ˆ
        1. ç‰‡æ®µä¸èƒ½ä¸ºç©º
        2. ç‰‡æ®µä¸èƒ½ä¸ºæ¼”å‘˜å
        3. ç‰‡æ®µé•¿åº¦å¤§äº8æˆ–é•¿åº¦åœ¨4åˆ°8ä¹‹é—´ä¸”ä¸èƒ½ä¸ºçº¯æ•°å­—æˆ–å­—æ¯æˆ–ç•ªå· ä¾‹å¦‚: ABC-123
        """
        part = part.strip()
        if not part:
            return False
        if part in actor_list:
            return False
        if len(part) > 8:
            return True
        if len(part) >= 4 and not re.search(r"(^[a-zA-Z]+-\d+$)|(^[a-zA-Z0-9]+$)", part):
            return True
        print(f"æ ‡é¢˜ç‰‡æ®µ {part} æ— æ•ˆ, è·³è¿‡")
        return False
    
    def split_and_filter(title, sep):
        """è¾…åŠ©å‡½æ•°ï¼šæŒ‰åˆ†éš”ç¬¦æ‹†åˆ†æ ‡é¢˜å¹¶è¿‡æ»¤æ— æ•ˆå­ä¸²"""
        parts = title.split(sep)
        return [part for part in parts if is_valid_part(part, actor_list)]
    
    # å…ˆä»¥ç©ºæ ¼æ‹†åˆ†
    split_title_with_space = []
    for title in no_split_title_list:
        split_title_with_space.extend(split_and_filter(title, separator))
    
    # å†ä»¥é¢å¤–åˆ†éš”ç¬¦æ‹†åˆ†
    if extra_separator:
        base_titles = split_title_with_space.copy() or no_split_title_list.copy()
        for extra in extra_separator.split(","):
            split_title_with_extra = []
            for title in base_titles:
                split_title_with_extra.extend(split_and_filter(title, extra))
            split_title_with_space.extend(split_title_with_extra)
    
    # åˆå¹¶æ‰€æœ‰æœ‰æ•ˆæ ‡é¢˜ç‰‡æ®µå¹¶å»é‡
    titles_no_actor = list(dict.fromkeys(no_split_title_list + split_title_with_space))
    
    # é€‰å–é¦–ä½æœ€ç¬¦åˆçš„æ¼”å‘˜åæ·»åŠ åˆ°æ ‡é¢˜æœ«å°¾
    no_split_title_list = _add_actor_to_title(no_split_title_list, best_match_actor)
    search_title_list = _add_actor_to_title(titles_no_actor, best_match_actor)
    return no_split_title_list, search_title_list


def _get_compare_title(title, actor_list, pattern=None, operation_flags=0b111):
    """
    æ­£åˆ™åˆ é™¤æ ‡é¢˜ä¸­çš„pattern
    è°ƒç”¨convert_halfå¤„ç†æ ‡é¢˜
    åˆ é™¤æ¼”å‘˜å
    è¿”å›åˆ é™¤æ¼”å‘˜åå‰åçš„æ ‡é¢˜, å¦‚æœæ ‡é¢˜æœ¬èº«æ²¡æœ‰æ¼”å‘˜å, åˆ™è¿”å›çš„ä¸¤ä¸ªæ ‡é¢˜ç›¸åŒ
    """
    if pattern:
        title = re.sub(pattern, "", title).strip()
    compare_title_list = convert_half(title, operation_flags)
    print(f"å¤„ç†åçš„æ ‡é¢˜: {compare_title_list}")
    compare_title_no_actor = compare_title_list[0]
    for actor in actor_list:
        compare_title_no_actor = compare_title_no_actor.replace(actor, "").strip()
    print(f"å¤„ç†åçš„æ ‡é¢˜(å»é™¤æ¼”å‘˜å): {compare_title_no_actor}")
    return compare_title_list[0], compare_title_no_actor


def _check_title_matching(compare_title,
                          amazon_compare_title,
                          no_split_compare_title,
                          length_diff_ratio=5,
                          min_match_length=5,
                          mid_title_length=12,
                          no_split_match_ratio=0.5,
                          split_match_ratio=0.8,
                          long_title_length=60,
                          length_ratio=0.5,
                          golden_ratio=0.618,
                          ):
    """
    åŠŸèƒ½:
        æ£€æµ‹æ ‡é¢˜æ˜¯å¦åŒ¹é…, Amazonæ ‡é¢˜åªéœ€ä¸æœªæ‹†åˆ†æ ‡é¢˜åŒ¹é…, å¿½ç•¥æ‹†åˆ†æ ‡é¢˜
    å…¥å‚:
        no_split_compare_title ä¸ºå¤„ç†è¿‡çš„æœªæ‹†åˆ†æ ‡é¢˜
        amazon_compare_title ä¸ºå¤„ç†è¿‡çš„Amazonæ ‡é¢˜
        äºŒè€…å‡å·²ç»è¿‡ä»¥ä¸‹æ“ä½œ:
        1. æ­£åˆ™å»é™¤ã€.*?ã€‘ç­‰å†…å®¹
        2. å…¨è§’è½¬åŠè§’
        3. å»é™¤æ ‡ç‚¹ç©ºæ ¼
        4. å»é™¤æ•æ„Ÿè¯
    åŒ¹é…æ¡ä»¶(æŒ‰ä¼˜å…ˆçº§æ’åˆ—):
        æœªæ‹†åˆ†æ ‡é¢˜åŒ¹é…, compare_title == no_split_compare_title
            a. len(é•¿æ ‡é¢˜)/len(çŸ­æ ‡é¢˜) <= length_diff_ratio, é¿å…è¿‡çŸ­æ ‡é¢˜åŒ¹é…åˆ°è¿‡é•¿æ ‡é¢˜ (JUX-925)
            b. åŒ¹é…ä½ç½®å¿…é¡»æ˜¯Amazonçš„æ ‡é¢˜é¦–å­—ç¬¦ (ATID-586)
            c. æœªæ‹†åˆ†æ ‡é¢˜é•¿åº¦<=min_match_length, è¦æ±‚çŸ­æ ‡é¢˜å®Œå…¨åŒ¹é…é•¿æ ‡é¢˜ (JUX-925)
            d. æœªæ‹†åˆ†æ ‡é¢˜é•¿åº¦>min_match_length ä¸” <=mid_title_length , è¦æ±‚åŒ¹é…é•¿åº¦>= min(min_match_length, len(çŸ­æ ‡é¢˜)) (ATID-586)
            e. æœªæ‹†åˆ†æ ‡é¢˜é•¿åº¦>mid_title_length, è¦æ±‚åŒ¹é…é•¿åº¦ >= min(math.floor(len(æœªæ‹†åˆ†æ ‡é¢˜é•¿åº¦) * no_split_match_ratio), len(çŸ­æ ‡é¢˜))
        æ‹†åˆ†æ ‡é¢˜åŒ¹é…, compare_title != no_split_compare_title
            a. å…ˆå°†Amazonæ ‡é¢˜ä¸æœªæ‹†åˆ†æ ‡é¢˜åŒ¹é…, åŒ¹é…ä½ç½®å¿…é¡»æ˜¯Amazonçš„æ ‡é¢˜é¦–å­—ç¬¦
            b. å¦‚æœAmazonæ ‡é¢˜é•¿åº¦ <= min_match_length, åˆ™è¦æ±‚Amazonæ ‡é¢˜å®Œå…¨åŒ¹é…æœªæ‹†åˆ†æ ‡é¢˜
            c. å¦‚æœAmazonæ ‡é¢˜é•¿åº¦ > min_match_length
                1). è¦æ±‚åŒ¹é…é•¿åº¦>= min_match_length, è¿™æ ·æ˜¯ä¸ºäº†ä¿è¯æœç´¢çš„ç»“æœçš„å‰ min_match_length ä¸ªå­—ç¬¦ä¸æœªæ‹†åˆ†æ ‡é¢˜ç›¸åŒ
                2). å¦‚æœ len_no_split > long_title_length
                    è¦æ±‚ min(len_no_split, len_amazon)/max(len_no_split, len_amazon) >= length_ratio
                    è¦æ±‚åŒ¹é…é•¿åº¦ >= math.floor(min(len_no_split, len_amazon) * golden_ratio)
                    è¿™æ˜¯ä¸“é—¨é’ˆå¯¹è¶…é•¿æ ‡é¢˜ä¸”æœç´¢ç»“æœé›·åŒçš„ç³»åˆ—å½±ç‰‡ (HUNTA-145)
            d. å†å°†Amazonæ ‡é¢˜ä¸æ‹†åˆ†æ ‡é¢˜åŒ¹é…åŒ¹é…, åŒ¹é…ä½ç½®å¿…é¡»æ˜¯æ‹†åˆ†æ ‡é¢˜é¦–å­—ç¬¦
            e. æ‹†åˆ†æ ‡é¢˜é•¿åº¦<= match.ceil(1.5 * min_match_length), è¦æ±‚çŸ­æ ‡é¢˜å®Œå…¨åŒ¹é…é•¿æ ‡é¢˜
            f. æ‹†åˆ†æ ‡é¢˜é•¿åº¦> match.ceil(1.5 * min_match_length), è¦æ±‚åŒ¹é…é•¿åº¦ >= min(math.ceil(len(æ‹†åˆ†æ ‡é¢˜é•¿åº¦) * split_match_ratio), len(çŸ­æ ‡é¢˜))
    è¿”å›:
        æ»¡è¶³ä»¥ä¸ŠåŒ¹é…æ¡ä»¶è¿”å› True, å¦åˆ™è¿”å› False
    """
    print(f"\nå¼€å§‹åŒ¹é…æ ‡é¢˜")
    len_compare = len(compare_title)
    len_amazon = len(amazon_compare_title)
    len_no_split = len(no_split_compare_title)
    # è·å–çŸ­æ ‡é¢˜å’Œé•¿æ ‡é¢˜
    short_title, long_title = (compare_title, amazon_compare_title) if len_compare < len_amazon else (amazon_compare_title, compare_title)
    len_short = len(short_title)
    len_long = len(long_title)
    # åŒ¹é…æœªæ‹†åˆ†æ ‡é¢˜
    if compare_title == no_split_compare_title:
        print(f"æ ‡é¢˜æœªæ‹†åˆ†, éµå¾ªæ—¢å®šè§„åˆ™åŒ¹é…\næœªæ‹†åˆ†æ ‡é¢˜æ ‡é¢˜: {compare_title}\nAmazonæ ‡é¢˜: {amazon_compare_title}")
        # é•¿å­—ç¬¦ä¸²é•¿åº¦ä¸èƒ½è¶…è¿‡çŸ­å­—ç¬¦ä¸²é•¿åº¦çš„5å€
        if len_long > length_diff_ratio * len_short:
            print(f"æ ‡é¢˜é•¿åº¦å·®å¼‚è¿‡å¤§, åŒ¹é…å¤±è´¥!")
            return False

        if len_compare <= min_match_length:
            if short_title in long_title and amazon_compare_title.startswith(short_title):
                print(f"çŸ­æ ‡é¢˜å®Œå…¨åŒ¹é…é•¿æ ‡é¢˜, åŒ¹é…æˆåŠŸ!")
                return True
            else:
                print(f"çŸ­æ ‡é¢˜ä¸å®Œå…¨åŒ¹é…é•¿æ ‡é¢˜, ä¸”æœªæ‹†åˆ†æ ‡é¢˜é•¿åº¦ <={min_match_length}, åŒ¹é…å¤±è´¥!")
                return False
        elif len_no_split > min_match_length and len_no_split <= mid_title_length:
            required_match_length = min(min_match_length, len_short)  # å–çŸ­æ ‡é¢˜é•¿åº¦å’Œ min_match_length çš„æœ€å°å€¼
            substring = amazon_compare_title[:required_match_length]  # ä» amazon_compare_title çš„é¦–å­—ç¬¦å¼€å§‹æˆªå–
            if substring in compare_title:  # åˆ¤æ–­å­ä¸²æ˜¯å¦å‡ºç°åœ¨ compare_title ä¸­
                print(f"åŒ¹é…é•¿åº¦ >= {required_match_length}, ç¬¦åˆè¦æ±‚ , åŒ¹é…æˆåŠŸ!")
                return True
            else:
                print(f"åŒ¹é…é•¿åº¦ < {required_match_length}, åŒ¹é…å¤±è´¥!")
                return False
        else:
            required_match_length = min(math.floor(len_compare * no_split_match_ratio), len_short) # å–çŸ­æ ‡é¢˜é•¿åº¦å’Œ len_compare * no_split_match_ratio çš„æœ€å°å€¼
            substring = amazon_compare_title[:required_match_length]  # ä» amazon_compare_title çš„é¦–å­—ç¬¦å¼€å§‹æˆªå–
            if substring in compare_title:  # åˆ¤æ–­å­ä¸²æ˜¯å¦å‡ºç°åœ¨ compare_title ä¸­
                print(f"åŒ¹é…é•¿åº¦ >= {required_match_length}, ç¬¦åˆè¦æ±‚ , åŒ¹é…æˆåŠŸ!")
                return True
            else:
                print(f"åŒ¹é…é•¿åº¦ < {required_match_length}, åŒ¹é…å¤±è´¥!")
                return False
    else: # åŒ¹é…æ‹†åˆ†æ ‡é¢˜
        print(f"æ ‡é¢˜å·²æ‹†åˆ†, éµå¾ªæ—¢å®šè§„åˆ™åŒ¹é…\næ‹†åˆ†æ ‡é¢˜: {compare_title}\næœªæ‹†åˆ†æ ‡é¢˜: {no_split_compare_title}\nAmazonæ ‡é¢˜: {amazon_compare_title}")
        print(f"å…ˆä¸æœªæ‹†åˆ†æ ‡é¢˜åŒ¹é…")
        if len_amazon <= min_match_length:
            if amazon_compare_title in no_split_compare_title:
                print(f"Amazonæ ‡é¢˜å®Œå…¨åŒ¹é…æœªæ‹†åˆ†æ ‡é¢˜, ç»§ç»­åŒ¹é…")
                pass
            else:
                print(f"Amazonæ ‡é¢˜ä¸å®Œå…¨åŒ¹é…é•¿æ ‡é¢˜, ä¸”Amazonæ ‡é¢˜é•¿åº¦ <={min_match_length}, åŒ¹é…å¤±è´¥!")
                return False
        else:
            substring = amazon_compare_title[:min_match_length]  # ä» amazon_compare_title çš„é¦–å­—ç¬¦å¼€å§‹æˆªå–
            if substring in no_split_compare_title:  # åˆ¤æ–­å­ä¸²æ˜¯å¦å‡ºç°åœ¨ no_split_compare_title ä¸­
                print(f"Amazonæ ‡é¢˜ä¸æœªæ‹†åˆ†æ ‡é¢˜åŒ¹é…é•¿åº¦ >= {min_match_length}, ç»§ç»­åŒ¹é…")
                pass
            else:
                print(f"Amazonæ ‡é¢˜ä¸æœªæ‹†åˆ†æ ‡é¢˜åŒ¹é…é•¿åº¦ < {min_match_length}, åŒ¹é…å¤±è´¥!")
                return False
            if len_no_split > long_title_length:
                if min(len_no_split, len_amazon)/max(len_no_split, len_amazon) < length_ratio:
                    print(f"è¶…é•¿æ ‡é¢˜, é•¿åº¦æ¯” < {length_ratio}, åŒ¹é…å¤±è´¥!")
                    return False
                else:
                    print(f"è¶…é•¿æ ‡é¢˜, é•¿åº¦æ¯” >= {length_ratio}, ç»§ç»­åŒ¹é…")
                    pass
                required_match_length = math.floor(min(len_no_split, len_amazon) * golden_ratio)
                substring = amazon_compare_title[:required_match_length]  # ä» amazon_compare_title çš„é¦–å­—ç¬¦å¼€å§‹æˆªå–
                if substring in no_split_compare_title:  # åˆ¤æ–­å­ä¸²æ˜¯å¦å‡ºç°åœ¨ no_split_compare_title ä¸­
                    print(f"è¶…é•¿æ ‡é¢˜, åŒ¹é…ç‡ >= {golden_ratio}, ç¬¦åˆè¦æ±‚, ç»§ç»­åŒ¹é…")
                    pass
                else:
                    print(f"è¶…é•¿æ ‡é¢˜, åŒ¹é…ç‡ < {golden_ratio}, åŒ¹é…å¤±è´¥!")
                    return False
        print(f"æœªæ‹†åˆ†æ ‡é¢˜åŒ¹é…é€šè¿‡, å†ä¸æ‹†åˆ†æ ‡é¢˜åŒ¹é…")
        if len_compare <= math.ceil(1.5 * min_match_length):
            if short_title in long_title and compare_title.startswith(short_title):
                print(f"çŸ­æ ‡é¢˜å®Œå…¨åŒ¹é…é•¿æ ‡é¢˜, åŒ¹é…æˆåŠŸ!")
                return True
            else:
                print(f"æ‹†åˆ†æ ‡é¢˜ä¸å®Œå…¨åŒ¹é…Amazonæ ‡é¢˜, ä¸”æ‹†åˆ†æ ‡é¢˜é•¿åº¦ <={math.ceil(1.5 * min_match_length)}, åŒ¹é…å¤±è´¥!")
                return False
        else:
            required_match_length = min(math.ceil(len_compare * split_match_ratio), len_short) # å–çŸ­æ ‡é¢˜é•¿åº¦å’Œ len_compare * split_match_ratio çš„æœ€å°å€¼
            substring = compare_title[:required_match_length]  # ä» compare_title çš„é¦–å­—ç¬¦å¼€å§‹æˆªå–
            if substring in amazon_compare_title:  # åˆ¤æ–­å­ä¸²æ˜¯å¦å‡ºç°åœ¨ amazon_compare_title ä¸­
                print(f"åŒ¹é…é•¿åº¦ >= {required_match_length}, ç¬¦åˆè¦æ±‚ , åŒ¹é…æˆåŠŸ!")
                return True
            else:
                print(f"åŒ¹é…é•¿åº¦ < {required_match_length}, åŒ¹é…å¤±è´¥!")
                return False

def _check_realse_date(json_data, amazon_title, promotion_keywords=[], amazon_release=None):
    """
    æ¯”è¾ƒå½±ç‰‡å‘è¡Œæ—¥æœŸä¸Amazonè¯¦æƒ…é¡µçš„å‘è¡Œæ—¥æœŸæ˜¯å¦ä¸€è‡´, é¿å…åŒä¸€ä¸ªæ¼”å‘˜çš„ç›¸åŒæ ‡é¢˜å½±ç‰‡è¢«è¯¯åŒ¹é…
    1. å¦‚æœæœ‰ä»»ä½•ä¸€ä¸ªæ—¥æœŸä¸å­˜åœ¨, åˆ™è¿”å›True
    2. å¦‚æœä¸¤ä¸ªæ—¥æœŸéƒ½å­˜åœ¨, åˆ™å¼€å§‹æ¯”è¾ƒ
    3. å¦‚æœäºŒè€…é—´éš” <=30å¤©è¿”å›True, å¦åˆ™è¿”å›False, è¿™æ˜¯å› ä¸ºæœ‰æ—¶å€™å½±ç‰‡å‘è¡Œæ—¥æœŸå–çš„æ˜¯é…ä¿¡æ—¥æœŸ, ä¼šæœ‰ä¸€å®šçš„å·®å¼‚
    4. å¦‚æœäºŒè€…é—´éš” >30å¤©, ä½†å½±ç‰‡æ ‡é¢˜ä¸­åŒ…å«ä¿ƒé”€æ¨å¹¿å…³é”®å­—, åˆ™ä¸è®¤ä¸ºæ—¥æœŸä¸ä¸€è‡´
    """
    print(f"å¼€å§‹æ£€æµ‹å‘è¡Œæ—¥æœŸæ˜¯å¦åŒ¹é…")
    movie_release = json_data.get("release")
    
    if not amazon_release:
        print(f"Amazonè¯¦æƒ…é¡µæ— å‘è¡Œæ—¥æœŸ, è·³è¿‡æ­¤æ£€æµ‹")
        return "NO RELEASE DATE"
    elif not movie_release:
        print(f"æ— å½±ç‰‡å‘è¡Œæ—¥æœŸ, è·³è¿‡æ­¤æ£€æµ‹")
        return "NO RELEASE DATE"
    else:
        # å½±ç‰‡å‘è¡Œæ—¥æœŸ, æ ¼å¼ä¸º 2024-08-17
        movie_release_date = datetime.strptime(movie_release, "%Y-%m-%d")
        print(f"å½±ç‰‡å‘è¡Œæ—¥æœŸ: {movie_release}")
        # Amazonè¯¦æƒ…é¡µå‘è¡Œæ—¥æœŸ, æ ¼å¼ä¸º 2024/8/17
        amazon_release_date = datetime.strptime(amazon_release, "%Y/%m/%d")
        print(f"Amazonè¯¦æƒ…é¡µå‘è¡Œæ—¥æœŸ: {amazon_release_date.strftime('%Y-%m-%d')}")
    
    date_diff = abs((movie_release_date - amazon_release_date).days)
    if date_diff <= 30:
        return "SUCCESS"
    elif any(promotion in amazon_title for promotion in promotion_keywords):
        print(f"å½±ç‰‡å‘è¡Œæ—¥æœŸä¸Amazonè¯¦æƒ…é¡µçš„å‘è¡Œæ—¥æœŸæœ‰å·®å¼‚, ä½†å½±ç‰‡æ ‡é¢˜ä¸­åŒ…å«ä¿ƒé”€æ¨å¹¿å…³é”®å­—, è·³è¿‡æ­¤æ£€æµ‹")
        return "PROMOTION"
    return "ERROR"

def _check_detail_page(json_data, title_match_ele, actor_amazon):
    """
    è·å–amazonçš„è¯¦æƒ…é¡µ, æ£€æµ‹æ¼”å‘˜åæ˜¯å¦åŒ¹é…, å‘è¡Œæ—¥æœŸæ˜¯å¦å»åˆ
    è¿”å›:
        å¸ƒå°”å€¼
    """
    detail_url = title_match_ele[1]
    amazon_title = title_match_ele[2]
    promotion_keywords = ["ç‰¹é¸ã‚¢ã‚¦ãƒˆãƒ¬ãƒƒãƒˆ", "ãƒ™ã‚¹ãƒˆãƒ’ãƒƒãƒ„"]
    try:
        url_new = "https://www.amazon.co.jp" + re.findall(r"(/dp/[^/]+)", detail_url)[0]
    except:
        
        url_new = detail_url
    print(f"\nè¯¦æƒ…é¡µurl: {url_new}")
    result, html_detail = get_amazon_data(url_new)
    if result and html_detail:
        html = etree.fromstring(html_detail, etree.HTMLParser())
        # è·å–æ¼”å‘˜å
        detail_actor = html.xpath('//span[@class="author notFaded" and .//span[contains(text(), "(å‡ºæ¼”)")]]//a[@class="a-link-normal"]/text()')
        # å»é™¤æ‰€æœ‰ç©ºæ ¼å¹¶è¿‡æ»¤æ‰ç©ºå­—ç¬¦ä¸²
        raw_detail_actor_list = []
        for each_actor in detail_actor:
            if each_actor.strip():
                each_actor.replace(" ", "")
                raw_detail_actor_list.extend(each_actor.split("/"))
        detail_actor_list = _split_actor(raw_detail_actor_list) if raw_detail_actor_list else []
        # detail_info_1 = str(
        #     html.xpath('//ul[@class="a-unordered-list a-vertical a-spacing-mini"]//text()')
        # ).replace(" ", "")
        # detail_info_2 = str(
        #     html.xpath('//div[@id="detailBulletsWrapper_feature_div"]//text()')
        # ).replace(" ", "")
        # detail_info_3 = str(html.xpath('//div[@id="productDescription"]//text()')).replace(" ", "")
        # all_info = detail_actor + detail_info_1 + detail_info_2 + detail_info_3
        # print(f"è¯¦æƒ…é¡µä¿¡æ¯: {all_info}")
        # è·å–å‘è¡Œæ—¥æœŸ
        date_text = html.xpath("//span[contains(text(), 'ç™ºå£²æ—¥')]/following-sibling::span[1]/text()")
        amazon_release = date_text[0].strip() if date_text else ""
        check_release = _check_realse_date(json_data, amazon_title, promotion_keywords, amazon_release)
        if check_release == "SUCCESS":
            print(f"å‘è¡Œæ—¥æœŸåŒ¹é…æˆåŠŸ!")
            return "RELEASE MATCH"
        elif check_release in ["NO RELEASE DATE", "PROMOTION"]:
            if detail_actor_list:
                print(f"å¼€å§‹åŒ¹é…æ¼”å‘˜\nè¯¦æƒ…é¡µæ¼”å‘˜åˆ—è¡¨: {detail_actor_list}")
                for each_actor in actor_amazon:
                    if each_actor in detail_actor_list:
                        print(f"è¯¦æƒ…é¡µåŒ¹é…åˆ°æ¼”å‘˜: {each_actor}")
                        return "ACTOR MATCH"
                print(f"è¯¦æƒ…é¡µæ¼”å‘˜ä¸åŒ¹é…, è·³è¿‡")
                return "ACTOR MISMATCH"
            else:
                print(f"è¯¦æƒ…é¡µæœªæ‰¾åˆ°æ¼”å‘˜, è·³è¿‡")
                return "LACK PROOF"
        else:
            print(f"å‘è¡Œæ—¥æœŸä¸åŒ¹é…, è·³è¿‡")
            return "RELEASE MISMATCH"
    print(f"è¯¦æƒ…é¡µè·å–å¤±è´¥, è·³è¿‡")
    return "ERROR"

def get_big_pic_by_amazon(json_data, original_title, raw_actor_list):
    if not original_title or not raw_actor_list:
        return ""
    hd_pic_url = ""
    actor_list, best_match_actor = _get_actor_list(json_data, original_title, raw_actor_list)
    # ç§»é™¤æ ‡é¢˜ä¸­åŒ¹é…çš„pattern
    pattern = r"^\[.*?\]|ã€.*?ã€‘|DVD|ï¼ˆDODï¼‰|ï¼ˆBODï¼‰"
    # æ‹†åˆ†æ ‡é¢˜
    no_split_title_list, search_title_list = _split_title(original_title, actor_list, best_match_actor, min_length=3, pattern=pattern, separator=" ", extra_separator="ï¼,â€¦")
    # å°†æœªæ‹†åˆ†çš„æ ‡é¢˜è¿›è¡Œå¤„ç†
    no_split_compare_title, no_split_compare_title_no_actor  = _get_compare_title(no_split_title_list[-1], actor_list, pattern=pattern)
    # å›¾ç‰‡urlè¿‡æ»¤é›†åˆ, å¦‚æœåŒ¹é…ç›´æ¥è·³è¿‡
    pic_url_filtered_set = set()
    # æ ‡é¢˜è¿‡æ»¤é›†åˆ, å¦‚æœåŒ¹é…ç›´æ¥è·³è¿‡
    amazon_title_filtered_set = set()
    # æ ‡é¢˜åŒ¹é…é€šè¿‡, ä½†è¯¦æƒ…é¡µæœªæ‰¾åˆ°æœ‰æ•ˆåŒ¹é…æ•°æ®çš„å›¾ç‰‡url
    pic_legacy_list = []
    
    # æœç´¢æ ‡é¢˜
    for search_title in search_title_list:
        print(f"\n/********************å¼€å§‹æœç´¢************************/")
        print(f"æœç´¢æ ‡é¢˜æ€»åˆ—è¡¨:\nsearch_title_list = {search_title_list}")
        print(f"æœç´¢æ ‡é¢˜:\nsearch_title = {search_title}")
        print(f"å›¾ç‰‡urlè¿‡æ»¤é›†åˆ:\npic_url_filtered_set = {pic_url_filtered_set}") 
        print(f"æ ‡é¢˜è¿‡æ»¤é›†åˆ:\namazon_title_filtered_set = {amazon_title_filtered_set}")
        if (
            search_title == search_title_list[0]
            and len(search_title) <= 3
        ):
            print(f"åŸå§‹æ ‡é¢˜è¿‡çŸ­, è·³è¿‡: {search_title}")
            print(f"æœç´¢åŒ…å«æ¼”å‘˜åçš„åŸå§‹æ ‡é¢˜")
            continue

        # éœ€è¦ä¸¤æ¬¡urlencodeï¼Œnb_sb_nossè¡¨ç¤ºæ— æ¨èæ¥æº
        url_search = (
            "https://www.amazon.co.jp/black-curtain/save-eligibility/black-curtain?returnUrl=/s?k="
            + urllib.parse.quote_plus(urllib.parse.quote_plus(search_title.replace("&", " ") + " [DVD]"))
            + "&ref=nb_sb_noss"
        )
        result, html_search = get_amazon_data(url_search)

        if result and html_search:
            # é¡µé¢æ˜¾ç¤º "æ²¡æœ‰æ‰¾åˆ°ä¸æœç´¢åŒ¹é…çš„å•†å“ã€‚", æœ‰æ—¶ä¸‹æ–¹çš„æ¨èå•†å“ä¸­ä¼šæœ‰æ­£ç¡®çš„ç»“æœ, ä½†ä¸èƒ½ä¿è¯ç™¾åˆ†ç™¾å‡ºç°, å¯èƒ½å’Œcookieæœ‰å…³, æš‚æ—¶æœªæ‰¾åˆ°åŸå› , å› æ­¤ç›´æ¥è·³è¿‡
            if "æ¤œç´¢ã«ä¸€è‡´ã™ã‚‹å•†å“ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚" in html_search:
                print(f"æ— æœç´¢ç»“æœ, ç»“æŸæœ¬æ¬¡æœç´¢\n")
                continue
            html = etree.fromstring(html_search, etree.HTMLParser())
            
            # æ ‡é¢˜åŒ¹é…åˆ—è¡¨
            title_match_list = []
            # è®¡ç®—æ— æ•ˆæ ‡é¢˜çš„æ•°é‡, é¿å…åœ¨æ— æ•ˆå…³é”®è¯ä¸Šæµªè´¹è¿‡å¤šæ—¶é—´
            invalid_title_count = 0
            amazon_result = html.xpath('//div[@class="a-section a-spacing-base"]')
            print(f"æ‰¾åˆ°{len(amazon_result)}ä¸ªç»“æœ")
            
            # å¼€å§‹å¤„ç†æœç´¢ç»“æœ
            for each_result in amazon_result:
                if invalid_title_count == 5:
                    print(f"æ— æ•ˆæ ‡é¢˜æ•°é‡è¿‡å¤š, ç»“æŸæœ¬æ¬¡æœç´¢\n")
                    break
                amazon_category_list = each_result.xpath(
                    'div//a[@class="a-size-base a-link-normal s-underline-text s-underline-link-text s-link-style a-text-bold"]/text()'
                )
                amazon_title_list = each_result.xpath(
                    'div//h2[@class="a-size-base-plus a-spacing-none a-color-base a-text-normal"]/span/text()'
                )
                pic_url_list = each_result.xpath('div//div[@class="a-section aok-relative s-image-square-aspect"]/img/@src')
                detail_url_list = each_result.xpath('div//a[@class="a-link-normal s-no-outline"]/@href')
                
                if len(amazon_category_list) and len(pic_url_list) and (len(amazon_title_list) and len(detail_url_list)):
                    amazon_category = amazon_category_list[0]  # Amazonå•†å“ç±»å‹
                    amazon_title = amazon_title_list[0]  # Amazonå•†å“æ ‡é¢˜
                    pic_url = pic_url_list[0]  # Amazonå›¾ç‰‡é“¾æ¥
                    pic_trunc_url = re.sub(r"\._[_]?AC_[^\.]+\.", ".", pic_url) # å»é™¤åç¼€ä»¥è·å¾—æ›´é«˜åˆ†è¾¨ç‡çš„å›¾ç‰‡
                    detail_url = detail_url_list[0]  # Amazonè¯¦æƒ…é¡µé“¾æ¥ï¼ˆæœ‰æ—¶å¸¦æœ‰æ¼”å‘˜åï¼‰
                    # å»é™¤é DVDä¸æ— å›¾ç‰‡çš„ç»“æœ
                    if (amazon_category not in ["DVD", "Software Download"]
                        or ".jpg" not in pic_trunc_url
                    ):
                        print(f"\næ— æ•ˆæ ‡é¢˜, è·³è¿‡: {amazon_title}")
                        invalid_title_count += 1
                        print(f"æ·»åŠ åˆ°è¿‡æ»¤é›†åˆ")
                        amazon_title_filtered_set.add(amazon_title)
                        pic_url_filtered_set.add(pic_trunc_url)
                        continue
                    
                    if amazon_title in amazon_title_filtered_set:
                        invalid_title_count += 1
                        print(f"\nè·³è¿‡å·²è¿‡æ»¤çš„æ ‡é¢˜: {amazon_title}")
                        continue
                    
                    w, h = get_imgsize(pic_trunc_url)
                    if w < 700 or w >= h:
                        print(f"\nå›¾ç‰‡éé«˜æ¸…æˆ–éç«–ç‰ˆ, è·³è¿‡: {pic_trunc_url}")
                        invalid_title_count += 1
                        print(f"æ·»åŠ åˆ°è¿‡æ»¤é›†åˆ")
                        amazon_title_filtered_set.add(amazon_title)
                        pic_url_filtered_set.add(pic_trunc_url)
                        continue
                    
                    # é¿å…å•ä½“ä½œå“å–åˆ°åˆé›†ç»“æœ GVH-435
                    collection_keywords = ['BEST', 'æ™‚é–“', 'ç·é›†ç·¨', 'å®Œå…¨', 'æšçµ„']
                    skip_flag = False
                    for collection_keyword in collection_keywords:
                        contains_s1 = collection_keyword in str(search_title_list[0]).upper()
                        contains_s2 = collection_keyword in str(amazon_title).upper()
                        if contains_s1 != contains_s2:
                            skip_flag = True
                    if skip_flag:
                        print(f"\nåˆé›†æ ‡é¢˜, è·³è¿‡: {amazon_title}")
                        invalid_title_count += 1
                        print(f"æ·»åŠ åˆ°è¿‡æ»¤é›†åˆ")
                        amazon_title_filtered_set.add(amazon_title)
                        pic_url_filtered_set.add(pic_trunc_url)
                        continue
                    print(f"\n+++++++++++++++++++++++++æ£€æµ‹æœ‰æ•ˆç»“æœ+++++++++++++++++++++++++")
                    print(f"æœç´¢æ ‡é¢˜:\nsearch_title = {search_title}")
                    print(f"Amazonå•†å“ä¿¡æ¯:\namazon_category = {amazon_category}\namazon_title = {amazon_title}\npic_trunc_url = {pic_trunc_url}")
                    if pic_trunc_url in pic_url_filtered_set:
                        print(f"\nè·³è¿‡å·²è¿‡æ»¤çš„å›¾ç‰‡url: {pic_trunc_url}")
                        continue
                    compare_title, compare_title_no_actor = _get_compare_title(search_title, actor_list, pattern=pattern)
                    amazon_compare_title, amazon_compare_title_no_actor = _get_compare_title(amazon_title, actor_list, pattern=pattern)
                    print(f"å¾…æ¯”è¾ƒçš„æœç´¢æ ‡é¢˜:\ncompare_title = {compare_title}\ncompare_title_no_actor = {compare_title_no_actor}")
                    print(f"å¾…æ¯”è¾ƒçš„Amazonæ ‡é¢˜:\namazon_compare_title = {amazon_compare_title}\namazon_compare_title_no_actor = {amazon_compare_title_no_actor}")

                    # åˆ¤æ–­æ ‡é¢˜æ˜¯å¦åŒ¹é…
                    print(f"å¾…æ¯”è¾ƒçš„æœªæ‹†åˆ†æ ‡é¢˜:\nno_split_compare_title = {no_split_compare_title}\nno_split_compare_title_no_actor = {no_split_compare_title_no_actor}")
                    if (
                        _check_title_matching(compare_title, amazon_compare_title, no_split_compare_title)
                        or _check_title_matching(compare_title_no_actor, amazon_compare_title_no_actor, no_split_compare_title_no_actor)
                    ):
                        print(f"æ ‡é¢˜åŒ¹é…æˆåŠŸ")
                        detail_url = urllib.parse.unquote_plus(detail_url)
                        temp_title = re.findall(r"(.+)keywords=", detail_url)
                        temp_detail_url = (
                            temp_title[0] + amazon_compare_title if temp_title else detail_url + amazon_compare_title
                        )
                        detail_url_full = "https://www.amazon.co.jp" + detail_url

                        # åˆ¤æ–­æ¼”å‘˜æ˜¯å¦åœ¨æ ‡é¢˜é‡Œï¼Œé¿å…åŒåæ ‡é¢˜è¯¯åŒ¹é… MOPP-023
                        for each_actor in actor_list:
                            if each_actor in temp_detail_url:
                                print(f"åŒ¹é…æ¼”å‘˜: {each_actor}")
                                print(f"é‡‡ç”¨æ­¤ç»“æœ")
                                hd_pic_url = pic_trunc_url
                                return hd_pic_url
                        else:
                            # å¦‚æœæ²¡æœ‰åŒ¹é…ä»»ä½•æ¼”å‘˜ï¼Œæ·»åŠ åˆ° title_match_list
                            print(f"æ²¡æœ‰åŒ¹é…åˆ°æ¼”å‘˜, æ·»åŠ åˆ° title_match_list")
                            title_match_list.append([pic_trunc_url, detail_url_full, amazon_title])
                            print(f"title_match_list_pic_only = {[element[0] for element in title_match_list]}")
                    else:
                        print(f"æ ‡é¢˜ä¸åŒ¹é…, è·³è¿‡: {amazon_title}")
                        invalid_title_count += 1
                        print(f"æ·»åŠ åˆ°è¿‡æ»¤é›†åˆ")
                        amazon_title_filtered_set.add(amazon_title)
                        pic_url_filtered_set.add(pic_trunc_url)
                else:
                    print(f"\nè·³è¿‡ä¸åŒ…å«ç±»å‹, å›¾ç‰‡, æ ‡é¢˜, è¯¦æƒ…é¡µé¢çš„ç»“æœ")
                    invalid_title_count += 1
                    pass
                    
            # å½“æœç´¢ç»“æœåŒ¹é…åˆ°æ ‡é¢˜ï¼Œæ²¡æœ‰åŒ¹é…åˆ°æ¼”å‘˜æ—¶ï¼Œå°è¯•å»è¯¦æƒ…é¡µè·å–æ¼”å‘˜ä¿¡æ¯
            if (len(title_match_list) > 0):
                print(f"\nå°è¯•å»è¯¦æƒ…é¡µè·å–æ¼”å‘˜ä¿¡æ¯, å°è¯•æœ€å¤š4ä¸ªç»“æœ")
                for each in title_match_list[:4]:
                    detail_page_match =  _check_detail_page(json_data, each, actor_list)
                    if detail_page_match in ["RELEASE MATCH", "ACTOR MATCH"]:
                        print(f"è¯¦æƒ…é¡µæ£€æµ‹é€šè¿‡, é‡‡ç”¨æ­¤ç»“æœ!")
                        return each[0]
                    elif detail_page_match == "LACK PROOF":
                        print(f"è¯¦æƒ…é¡µæœªæ‰¾åˆ°æœ‰æ•ˆä¿¡æ¯, å°†å›¾ç‰‡urlæ·»åŠ åˆ°ä¿ç•™åˆ—è¡¨")
                        pic_legacy_list.append(each[0])
                    else:
                        # åªæ·»åŠ å›¾ç‰‡url, å› ä¸ºæ ‡é¢˜å·²ç»åŒ¹é…, å¦åˆ™æ·»åŠ æ¼”å‘˜åæœç´¢æ—¶ä¼šè¿‡æ»¤æ‰æœ‰æ•ˆæ ‡é¢˜
                        print(f"è¯¦æƒ…é¡µæ£€æµ‹æœªé€šè¿‡, å°†å›¾ç‰‡urlæ·»åŠ åˆ°è¿‡æ»¤é›†åˆ")
                        pic_url_filtered_set.add(each[0])
                    
            # # æ·»åŠ æ¼”å‘˜åé‡æ–°æœç´¢
            # actor_add_in_title = actor_list[0]
            # if (
            #     actor_add_in_title
            #     and actor_add_in_title not in search_title
            # ):
            #     title_with_actor = search_title + ' ' + actor_add_in_title
            #     if title_with_actor not in search_title_list:
            #         search_title_list.extend([title_with_actor])
            #         print(f"æ·»åŠ æ¼”å‘˜å {actor_add_in_title} è‡³å¾…æœç´¢åˆ—è¡¨")

    if pic_legacy_list:
        pic_legacy_list = list(dict.fromkeys(pic_legacy_list))
        print(f"å·²ç»å°è¯•æ‰€æœ‰å¯èƒ½æœç´¢, ä»æœªæ‰¾åˆ°ç¡®å®åŒ¹é…ç»“æœ, é€‰å–å¯èƒ½çš„ç»“æœ")
        print(f"å›¾ç‰‡ä¿ç•™åˆ—è¡¨\npic_legacy_list = {pic_legacy_list}")
        return pic_legacy_list[0]
    return hd_pic_url


def trailer_download(json_data, folder_new_path, folder_old_path, naming_rule):
    start_time = time.time()
    download_files = config.download_files
    keep_files = config.keep_files
    trailer_name = config.trailer_name
    trailer_url = json_data["trailer"]
    trailer_old_folder_path = os.path.join(folder_old_path, "trailers")
    trailer_new_folder_path = os.path.join(folder_new_path, "trailers")

    # é¢„å‘Šç‰‡åå­—ä¸å«è§†é¢‘æ–‡ä»¶åï¼ˆåªè®©ä¸€ä¸ªè§†é¢‘å»ä¸‹è½½å³å¯ï¼‰
    if trailer_name == 1:
        trailer_folder_path = os.path.join(folder_new_path, "trailers")
        trailer_file_name = "trailer.mp4"
        trailer_file_path = os.path.join(trailer_folder_path, trailer_file_name)

        # é¢„å‘Šç‰‡æ–‡ä»¶å¤¹å·²åœ¨å·²å¤„ç†åˆ—è¡¨æ—¶ï¼Œè¿”å›ï¼ˆè¿™æ—¶åªéœ€è¦ä¸‹è½½ä¸€ä¸ªï¼Œå…¶ä»–åˆ†é›†ä¸éœ€è¦ä¸‹è½½ï¼‰
        if trailer_folder_path in Flags.trailer_deal_set:
            return
        Flags.trailer_deal_set.add(trailer_folder_path)

        # ä¸ä¸‹è½½ä¸ä¿ç•™æ—¶åˆ é™¤è¿”å›
        if "trailer" not in download_files and "trailer" not in keep_files:
            # åˆ é™¤ç›®æ ‡æ–‡ä»¶å¤¹å³å¯ï¼Œå…¶ä»–æ–‡ä»¶å¤¹å’Œæ–‡ä»¶å·²ç»åˆ é™¤äº†
            if os.path.exists(trailer_folder_path):
                shutil.rmtree(trailer_folder_path, ignore_errors=True)
            return

    else:
        # é¢„å‘Šç‰‡å¸¦æ–‡ä»¶åï¼ˆæ¯ä¸ªè§†é¢‘éƒ½æœ‰æœºä¼šä¸‹è½½ï¼Œå¦‚æœå·²æœ‰ä¸‹è½½å¥½çš„ï¼Œåˆ™ä½¿ç”¨å·²ä¸‹è½½çš„ï¼‰
        trailer_file_name = naming_rule + "-trailer.mp4"
        trailer_folder_path = folder_new_path
        trailer_file_path = os.path.join(trailer_folder_path, trailer_file_name)

        # ä¸ä¸‹è½½ä¸ä¿ç•™æ—¶åˆ é™¤è¿”å›
        if "trailer" not in download_files and "trailer" not in keep_files:
            # åˆ é™¤ç›®æ ‡æ–‡ä»¶ï¼Œåˆ é™¤é¢„å‘Šç‰‡æ—§æ–‡ä»¶å¤¹ã€æ–°æ–‡ä»¶å¤¹ï¼ˆdeal old fileæ—¶æ²¡åˆ é™¤ï¼‰
            if os.path.exists(trailer_file_path):
                delete_file(trailer_file_path)
            if os.path.exists(trailer_old_folder_path):
                shutil.rmtree(trailer_old_folder_path, ignore_errors=True)
            if trailer_new_folder_path != trailer_old_folder_path and os.path.exists(trailer_new_folder_path):
                shutil.rmtree(trailer_new_folder_path, ignore_errors=True)
            return

    # é€‰æ‹©ä¿ç•™æ–‡ä»¶ï¼Œå½“å­˜åœ¨æ–‡ä»¶æ—¶ï¼Œä¸ä¸‹è½½ã€‚ï¼ˆdone trailer path æœªè®¾ç½®æ—¶ï¼ŒæŠŠå½“å‰æ–‡ä»¶è®¾ç½®ä¸º done trailer pathï¼Œä»¥ä¾¿å…¶ä»–åˆ†é›†å¤åˆ¶ï¼‰
    if "trailer" in keep_files and os.path.exists(trailer_file_path):
        if not Flags.file_done_dic.get(json_data["number"]).get("trailer"):
            Flags.file_done_dic[json_data["number"]].update({"trailer": trailer_file_path})
            # å¸¦æ–‡ä»¶åæ—¶ï¼Œåˆ é™¤æ‰æ–°ã€æ—§æ–‡ä»¶å¤¹ï¼Œç”¨ä¸åˆ°äº†ã€‚ï¼ˆå…¶ä»–åˆ†é›†å¦‚æœæ²¡æœ‰ï¼Œå¯ä»¥å¤åˆ¶ç¬¬ä¸€ä¸ªæ–‡ä»¶çš„é¢„å‘Šç‰‡ã€‚æ­¤æ—¶ä¸åˆ ï¼Œæ²¡æœºä¼šåˆ é™¤äº†ï¼‰
            if trailer_name == 0:
                if os.path.exists(trailer_old_folder_path):
                    shutil.rmtree(trailer_old_folder_path, ignore_errors=True)
                if trailer_new_folder_path != trailer_old_folder_path and os.path.exists(trailer_new_folder_path):
                    shutil.rmtree(trailer_new_folder_path, ignore_errors=True)
        json_data["logs"] += "\n ğŸ€ Trailer done! (old)(%ss) " % get_used_time(start_time)
        return True

    # å¸¦æ–‡ä»¶åæ—¶ï¼Œé€‰æ‹©ä¸‹è½½ä¸ä¿ç•™ï¼Œæˆ–è€…é€‰æ‹©ä¿ç•™ä½†æ²¡æœ‰é¢„å‘Šç‰‡ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–åˆ†é›†å·²ä¸‹è½½æˆ–æœ¬åœ°é¢„å‘Šç‰‡
    # é€‰æ‹©ä¸‹è½½ä¸ä¿ç•™ï¼Œå½“æ²¡æœ‰ä¸‹è½½æˆåŠŸæ—¶ï¼Œä¸ä¼šåˆ é™¤ä¸ä¿ç•™çš„æ–‡ä»¶
    done_trailer_path = Flags.file_done_dic.get(json_data["number"]).get("trailer")
    if trailer_name == 0 and done_trailer_path and os.path.exists(done_trailer_path):
        if os.path.exists(trailer_file_path):
            delete_file(trailer_file_path)
        copy_file(done_trailer_path, trailer_file_path)
        json_data["logs"] += "\n ğŸ€ Trailer done! (copy trailer)(%ss)" % get_used_time(start_time)
        return

    # ä¸ä¸‹è½½æ—¶è¿”å›ï¼ˆé€‰æ‹©ä¸ä¸‹è½½ä¿ç•™ï¼Œä½†æœ¬åœ°å¹¶ä¸å­˜åœ¨ï¼Œæ­¤æ—¶è¿”å›ï¼‰
    if "trailer," not in download_files:
        return

    # ä¸‹è½½é¢„å‘Šç‰‡,æ£€æµ‹é“¾æ¥æœ‰æ•ˆæ€§
    content_length = check_url(trailer_url, length=True)
    if content_length:
        # åˆ›å»ºæ–‡ä»¶å¤¹
        if trailer_name == 1 and not os.path.exists(trailer_folder_path):
            os.makedirs(trailer_folder_path)

        # å¼€å§‹ä¸‹è½½
        download_files = config.download_files
        signal.show_traceback_log(f"ğŸ” {json_data['number']} download trailer... {trailer_url}")
        trailer_file_path_temp = trailer_file_path
        if os.path.exists(trailer_file_path):
            trailer_file_path_temp = trailer_file_path + ".[DOWNLOAD].mp4"
        if download_file_with_filepath(json_data, trailer_url, trailer_file_path_temp, trailer_folder_path):
            file_size = os.path.getsize(trailer_file_path_temp)
            if file_size >= content_length or "ignore_size" in download_files:
                json_data["logs"] += "\n ğŸ€ Trailer done! ({} {}/{})({}s) ".format(
                    json_data["trailer_from"], file_size, content_length, get_used_time(start_time)
                )
                signal.show_traceback_log(f"âœ… {json_data['number']} trailer done!")
                if trailer_file_path_temp != trailer_file_path:
                    move_file(trailer_file_path_temp, trailer_file_path)
                    delete_file(trailer_file_path_temp)
                done_trailer_path = Flags.file_done_dic.get(json_data["number"]).get("trailer")
                if not done_trailer_path:
                    Flags.file_done_dic[json_data["number"]].update({"trailer": trailer_file_path})
                    if trailer_name == 0:  # å¸¦æ–‡ä»¶åï¼Œå·²ä¸‹è½½æˆåŠŸï¼Œåˆ é™¤æ‰é‚£äº›ä¸ç”¨çš„æ–‡ä»¶å¤¹å³å¯
                        if os.path.exists(trailer_old_folder_path):
                            shutil.rmtree(trailer_old_folder_path, ignore_errors=True)
                        if trailer_new_folder_path != trailer_old_folder_path and os.path.exists(
                            trailer_new_folder_path
                        ):
                            shutil.rmtree(trailer_new_folder_path, ignore_errors=True)
                return True
            else:
                json_data["logs"] += "\n ğŸŸ  Trailer size is incorrect! delete it! ({} {}/{}) ".format(
                    json_data["trailer_from"], file_size, content_length
                )
        # åˆ é™¤ä¸‹è½½å¤±è´¥çš„æ–‡ä»¶
        delete_file(trailer_file_path_temp)
        json_data["logs"] += "\n ğŸŸ  Trailer download failed! (%s) " % trailer_url

    if os.path.exists(trailer_file_path):  # ä½¿ç”¨æ—§æ–‡ä»¶
        done_trailer_path = Flags.file_done_dic.get(json_data["number"]).get("trailer")
        if not done_trailer_path:
            Flags.file_done_dic[json_data["number"]].update({"trailer": trailer_file_path})
            if trailer_name == 0:  # å¸¦æ–‡ä»¶åï¼Œå·²ä¸‹è½½æˆåŠŸï¼Œåˆ é™¤æ‰é‚£äº›ä¸ç”¨çš„æ–‡ä»¶å¤¹å³å¯
                if os.path.exists(trailer_old_folder_path):
                    shutil.rmtree(trailer_old_folder_path, ignore_errors=True)
                if trailer_new_folder_path != trailer_old_folder_path and os.path.exists(trailer_new_folder_path):
                    shutil.rmtree(trailer_new_folder_path, ignore_errors=True)
        json_data["logs"] += "\n ğŸŸ  Trailer download failed! å°†ç»§ç»­ä½¿ç”¨ä¹‹å‰çš„æœ¬åœ°æ–‡ä»¶ï¼"
        json_data["logs"] += "\n ğŸ€ Trailer done! (old)(%ss)" % get_used_time(start_time)
        return True


def _get_big_thumb(json_data):
    """
    è·å–èƒŒæ™¯å¤§å›¾ï¼š
    1ï¼Œå®˜ç½‘å›¾ç‰‡
    2ï¼ŒAmazon å›¾ç‰‡
    3ï¼ŒGoogle æœå›¾
    """
    start_time = time.time()
    if "thumb" not in config.download_hd_pics:
        return json_data
    number = json_data["number"]
    letters = json_data["letters"]
    number_lower_line = number.lower()
    number_lower_no_line = number_lower_line.replace("-", "")
    thumb_width = 0

    if json_data["cover_from"] == 'dmm':
        if json_data["cover"]:
            thumb_width, h = get_imgsize(json_data["cover"])
            print(f"dmm thumb_width = {thumb_width}")
            # å¯¹äºå­˜åœ¨ dmm 2K æ¨ªç‰ˆå°é¢çš„å½±ç‰‡, ç›´æ¥ä¸‹è½½å…¶ç«–ç‰ˆå°é¢
            if thumb_width >= 1700:
                json_data["logs"] += "\n ğŸ–¼ HD Thumb found! ({})({}s)".format(
                    json_data["cover_from"], get_used_time(start_time)
                )
                json_data["poster_big"] = True
                return json_data
    # faleno.jp ç•ªå·æ£€æŸ¥ï¼Œéƒ½æ˜¯å¤§å›¾ï¼Œè¿”å›å³å¯
    elif json_data["cover_from"] in ["faleno", "dahlia"]:
        if json_data["cover"]:
            json_data["logs"] += "\n ğŸ–¼ HD Thumb found! ({})({}s)".format(
                json_data["cover_from"], get_used_time(start_time)
            )
        json_data["poster_big"] = True
        return json_data

    # prestige å›¾ç‰‡æœ‰çš„æ˜¯å¤§å›¾ï¼Œéœ€è¦æ£€æµ‹å›¾ç‰‡åˆ†è¾¨ç‡
    elif json_data["cover_from"] in ["prestige", "mgstage"]:
        if json_data["cover"]:
            thumb_width, h = get_imgsize(json_data["cover"])

    # ç‰‡å•†å®˜ç½‘æŸ¥è¯¢
    elif "official" in config.download_hd_pics:
        # faleno.jp ç•ªå·æ£€æŸ¥
        if re.findall(r"F[A-Z]{2}SS", number):
            req_url = "https://faleno.jp/top/works/%s/" % number_lower_no_line
            result, response = get_html(req_url)
            if result:
                temp_url = re.findall(
                    r'src="((https://cdn.faleno.net/top/wp-content/uploads/[^_]+_)([^?]+))\?output-quality=', response
                )
                if temp_url:
                    json_data["cover"] = temp_url[0][0]
                    json_data["poster"] = temp_url[0][1] + "2125.jpg"
                    json_data["cover_from"] = "faleno"
                    json_data["poster_from"] = "faleno"
                    json_data["poster_big"] = True
                    trailer_temp = re.findall(r'class="btn09"><a class="pop_sample" href="([^"]+)', response)
                    if trailer_temp:
                        json_data["trailer"] = trailer_temp[0]
                        json_data["trailer_from"] = "faleno"
                    json_data["logs"] += "\n ğŸ–¼ HD Thumb found! (faleno)(%ss)" % get_used_time(start_time)
                    return json_data

        # km-produce.com ç•ªå·æ£€æŸ¥
        number_letter = letters.lower()
        kmp_key = ["vrkm", "mdtm", "mkmp", "savr", "bibivr", "scvr", "slvr", "averv", "kbvr", "cbikmv"]
        prestige_key = ["abp", "abw", "aka", "prdvr", "pvrbst", "sdvr", "docvr"]
        if number_letter in kmp_key:
            req_url = f"https://km-produce.com/img/title1/{number_lower_line}.jpg"
            real_url = check_url(req_url)
            if real_url:
                json_data["cover"] = real_url
                json_data["cover_from"] = "km-produce"
                json_data["logs"] += "\n ğŸ–¼ HD Thumb found! (km-produce)(%ss)" % (get_used_time(start_time))
                return json_data

        # www.prestige-av.com ç•ªå·æ£€æŸ¥
        elif number_letter in prestige_key:
            number_num = re.findall(r"\d+", number)[0]
            if number_letter == "abw" and int(number_num) > 280:
                pass
            else:
                req_url = f"https://www.prestige-av.com/api/media/goods/prestige/{number_letter}/{number_num}/pb_{number_lower_line}.jpg"
                if number_letter == "docvr":
                    req_url = f"https://www.prestige-av.com/api/media/goods/doc/{number_letter}/{number_num}/pb_{number_lower_line}.jpg"
                if get_imgsize(req_url)[0] >= 800:
                    json_data["cover"] = req_url
                    json_data["poster"] = req_url.replace("/pb_", "/pf_")
                    json_data["cover_from"] = "prestige"
                    json_data["poster_from"] = "prestige"
                    json_data["poster_big"] = True
                    json_data["logs"] += "\n ğŸ–¼ HD Thumb found! (prestige)(%ss)" % (get_used_time(start_time))
                    return json_data

    # ä½¿ç”¨googleä»¥å›¾æœå›¾
    pic_url = json_data.get("cover")
    if "google" in config.download_hd_pics:
        if pic_url and json_data["cover_from"] != "theporndb":
            thumb_url, cover_size = get_big_pic_by_google(pic_url)
            if thumb_url and cover_size[0] > thumb_width:
                json_data["cover_size"] = cover_size
                pic_domain = re.findall(r"://([^/]+)", thumb_url)[0]
                json_data["cover_from"] = f"Google({pic_domain})"
                json_data["cover"] = thumb_url
                json_data["logs"] += "\n ğŸ–¼ HD Thumb found! ({})({}s)".format(
                    json_data["cover_from"], get_used_time(start_time)
                )

    return json_data


def _get_big_poster(json_data):
    start_time = time.time()

    # æœªå‹¾é€‰ä¸‹è½½é«˜æ¸…å›¾posteræ—¶ï¼Œè¿”å›
    if "poster" not in config.download_hd_pics:
        return json_data

    # å¦‚æœæœ‰å¤§å›¾æ—¶ï¼Œç›´æ¥ä¸‹è½½
    if json_data.get("poster_big") and get_imgsize(json_data["poster"])[1] > 600:
        json_data["image_download"] = True
        json_data["logs"] += f"\n ğŸ–¼ HD Poster found! ({json_data['poster_from']})({get_used_time(start_time)}s)"
        return json_data

    # åˆå§‹åŒ–æ•°æ®
    number = json_data.get("number")
    poster_url = json_data.get("poster")
    hd_pic_url = ""
    poster_width = 0

    # é€šè¿‡åŸæ ‡é¢˜å» amazon æŸ¥è¯¢
    if "amazon" in config.download_hd_pics and json_data["mosaic"] in [
        "æœ‰ç ",
        "æœ‰ç¢¼",
        "æµå‡º",
        "æ— ç æµå‡º",
        "æ— ç ç ´è§£",
        "ç„¡ç¢¼ç ´è§£",
        "é‡Œç•ª",
        "è£ç•ª",
        "åŠ¨æ¼«",
        "å‹•æ¼«",
    ]:
        hd_pic_url = get_big_pic_by_amazon(json_data, json_data["originaltitle_amazon"], json_data["actor_amazon"])
        if hd_pic_url:
            json_data["poster"] = hd_pic_url
            json_data["poster_from"] = "Amazon"
        if json_data["poster_from"] == "Amazon":
            json_data["image_download"] = True

    # é€šè¿‡ç•ªå·å» å®˜ç½‘ æŸ¥è¯¢è·å–ç¨å¾®å¤§ä¸€äº›çš„å°é¢å›¾ï¼Œä»¥ä¾¿å» Google æœç´¢
    if (
        not hd_pic_url
        and "official" in config.download_hd_pics
        and "official" not in config.website_set
        and json_data["poster_from"] != "Amazon"
    ):
        letters = json_data["letters"].upper()
        official_url = config.official_websites.get(letters)
        if official_url:
            url_search = official_url + "/search/list?keyword=" + number.replace("-", "")
            result, html_search = get_html(url_search)
            if result:
                poster_url_list = re.findall(r'img class="c-main-bg lazyload" data-src="([^"]+)"', html_search)
                if poster_url_list:
                    # ä½¿ç”¨å®˜ç½‘å›¾ä½œä¸ºå°é¢å» google æœç´¢
                    poster_url = poster_url_list[0]
                    json_data["poster"] = poster_url
                    json_data["poster_from"] = official_url.split(".")[-2].replace("https://", "")
                    # vrä½œå“æˆ–è€…å®˜ç½‘å›¾ç‰‡é«˜åº¦å¤§äº500æ—¶ï¼Œä¸‹è½½å°é¢å›¾å¼€
                    if "VR" in number.upper() or get_imgsize(poster_url)[1] > 500:
                        json_data["image_download"] = True

    # ä½¿ç”¨googleä»¥å›¾æœå›¾ï¼Œæ”¾åœ¨æœ€åæ˜¯å› ä¸ºæœ‰æ—¶æœ‰é”™è¯¯ï¼Œæ¯”å¦‚ kawd-943
    poster_url = json_data.get("poster")
    if (
        not hd_pic_url
        and poster_url
        and "google" in config.download_hd_pics
        and json_data["poster_from"] != "theporndb"
    ):
        hd_pic_url, poster_size = get_big_pic_by_google(poster_url, poster=True)
        if hd_pic_url:
            if "prestige" in json_data["poster"] or json_data["poster_from"] == "Amazon":
                poster_width = get_imgsize(poster_url)[0]
            if poster_size[0] > poster_width:
                json_data["poster"] = hd_pic_url
                json_data["poster_size"] = poster_size
                pic_domain = re.findall(r"://([^/]+)", hd_pic_url)[0]
                json_data["poster_from"] = f"Google({pic_domain})"

    # å¦‚æœæ‰¾åˆ°äº†é«˜æ¸…é“¾æ¥ï¼Œåˆ™æ›¿æ¢
    if hd_pic_url:
        json_data["image_download"] = True
        json_data["logs"] += "\n ğŸ–¼ HD Poster found! ({})({}s)".format(
            json_data["poster_from"], get_used_time(start_time)
        )

    return json_data


def thumb_download(json_data, folder_new_path, thumb_final_path):
    start_time = time.time()
    poster_path = json_data["poster_path"]
    thumb_path = json_data["thumb_path"]
    fanart_path = json_data["fanart_path"]

    # æœ¬åœ°å­˜åœ¨ thumb.jpgï¼Œä¸”å‹¾é€‰ä¿ç•™æ—§æ–‡ä»¶æ—¶ï¼Œä¸ä¸‹è½½
    if thumb_path and "thumb" in config.keep_files:
        json_data["logs"] += "\n ğŸ€ Thumb done! (old)(%ss) " % get_used_time(start_time)
        return True

    # å¦‚æœthumbä¸ä¸‹è½½ï¼Œçœ‹fanartã€posterè¦ä¸è¦ä¸‹è½½ï¼Œéƒ½ä¸ä¸‹è½½åˆ™è¿”å›
    if "thumb" not in config.download_files:
        if "poster" in config.download_files and ("poster" not in config.keep_files or not poster_path):
            pass
        elif "fanart" in config.download_files and ("fanart" not in config.keep_files or not fanart_path):
            pass
        else:
            return True

    # å°è¯•å¤åˆ¶å…¶ä»–åˆ†é›†ã€‚çœ‹åˆ†é›†æœ‰æ²¡æœ‰ä¸‹è½½ï¼Œå¦‚æœä¸‹è½½å®Œæˆåˆ™å¯ä»¥å¤åˆ¶ï¼Œå¦åˆ™å°±è‡ªè¡Œä¸‹è½½
    if json_data["cd_part"]:
        done_thumb_path = Flags.file_done_dic.get(json_data["number"]).get("thumb")
        if (
            done_thumb_path
            and os.path.exists(done_thumb_path)
            and split_path(done_thumb_path)[0] == split_path(thumb_final_path)[0]
        ):
            copy_file(done_thumb_path, thumb_final_path)
            json_data["logs"] += "\n ğŸ€ Thumb done! (copy cd-thumb)(%ss) " % get_used_time(start_time)
            json_data["cover_from"] = "copy cd-thumb"
            json_data["thumb_path"] = thumb_final_path
            return True

    # è·å–é«˜æ¸…èƒŒæ™¯å›¾
    json_data = _get_big_thumb(json_data)

    # ä¸‹è½½å›¾ç‰‡
    cover_url = json_data.get("cover")
    cover_from = json_data.get("cover_from")
    if cover_url:
        cover_list = json_data["cover_list"]
        while [cover_from, cover_url] in cover_list:
            cover_list.remove([cover_from, cover_url])
        cover_list.insert(0, [cover_from, cover_url])

        thumb_final_path_temp = thumb_final_path
        if os.path.exists(thumb_final_path):
            thumb_final_path_temp = thumb_final_path + ".[DOWNLOAD].jpg"
        for each in cover_list:
            if not each[1]:
                continue
            cover_from, cover_url = each
            cover_url = check_url(cover_url)
            if not cover_url:
                json_data["logs"] += (
                    f"\n ğŸŸ  æ£€æµ‹åˆ° Thumb å›¾ç‰‡å¤±æ•ˆ! è·³è¿‡ï¼({cover_from})({get_used_time(start_time)}s) " + each[1]
                )
                continue
            json_data["cover_from"] = cover_from
            if download_file_with_filepath(json_data, cover_url, thumb_final_path_temp, folder_new_path):
                cover_size = check_pic(thumb_final_path_temp)
                if cover_size:
                    if (
                        not cover_from.startswith("Google")
                        or cover_size == json_data["cover_size"]
                        or (
                            cover_size[0] >= 800
                            and abs(
                                cover_size[0] / cover_size[1] - json_data["cover_size"][0] / json_data["cover_size"][1]
                            )
                            <= 0.1
                        )
                    ):
                        # å›¾ç‰‡ä¸‹è½½æ­£å¸¸ï¼Œæ›¿æ¢æ—§çš„ thumb.jpg
                        if thumb_final_path_temp != thumb_final_path:
                            move_file(thumb_final_path_temp, thumb_final_path)
                            delete_file(thumb_final_path_temp)
                        if json_data["cd_part"]:
                            dic = {"thumb": thumb_final_path}
                            Flags.file_done_dic[json_data["number"]].update(dic)
                        json_data["thumb_marked"] = False  # è¡¨ç¤ºè¿˜æ²¡æœ‰èµ°åŠ æ°´å°æµç¨‹
                        json_data["logs"] += "\n ğŸ€ Thumb done! ({})({}s) ".format(
                            json_data["cover_from"], get_used_time(start_time)
                        )
                        json_data["thumb_path"] = thumb_final_path
                        return True
                    else:
                        delete_file(thumb_final_path_temp)
                        json_data["logs"] += (
                            f"\n ğŸŸ  æ£€æµ‹åˆ° Thumb åˆ†è¾¨ç‡ä¸å¯¹{str(cover_size)}! å·²åˆ é™¤ ({cover_from})({get_used_time(start_time)}s)"
                        )
                        continue
                json_data["logs"] += f"\n ğŸŸ  Thumb download failed! {cover_from}: {cover_url} "
    else:
        json_data["logs"] += "\n ğŸŸ  Thumb url is empty! "

    # ä¸‹è½½å¤±è´¥ï¼Œæœ¬åœ°æœ‰å›¾
    if thumb_path:
        json_data["logs"] += "\n ğŸŸ  Thumb download failed! å°†ç»§ç»­ä½¿ç”¨ä¹‹å‰çš„å›¾ç‰‡ï¼"
        json_data["logs"] += "\n ğŸ€ Thumb done! (old)(%ss) " % get_used_time(start_time)
        return True
    else:
        if "ignore_pic_fail" in config.download_files:
            json_data["logs"] += "\n ğŸŸ  Thumb download failed! (ä½ å·²å‹¾é€‰ã€Œå›¾ç‰‡ä¸‹è½½å¤±è´¥æ—¶ï¼Œä¸è§†ä¸ºå¤±è´¥ï¼ã€) "
            json_data["logs"] += "\n ğŸ€ Thumb done! (none)(%ss)" % get_used_time(start_time)
            return True
        else:
            json_data["logs"] += (
                "\n ğŸ”´ Thumb download failed! ä½ å¯ä»¥åˆ°ã€Œè®¾ç½®ã€-ã€Œä¸‹è½½ã€ï¼Œå‹¾é€‰ã€Œå›¾ç‰‡ä¸‹è½½å¤±è´¥æ—¶ï¼Œä¸è§†ä¸ºå¤±è´¥ï¼ã€ "
            )
            json_data["error_info"] = (
                "Thumb download failed! ä½ å¯ä»¥åˆ°ã€Œè®¾ç½®ã€-ã€Œä¸‹è½½ã€ï¼Œå‹¾é€‰ã€Œå›¾ç‰‡ä¸‹è½½å¤±è´¥æ—¶ï¼Œä¸è§†ä¸ºå¤±è´¥ï¼ã€"
            )
            return False


def poster_download(json_data, folder_new_path, poster_final_path):
    start_time = time.time()
    download_files = config.download_files
    keep_files = config.keep_files
    poster_path = json_data["poster_path"]
    thumb_path = json_data["thumb_path"]
    fanart_path = json_data["fanart_path"]
    image_cut = ""

    # ä¸ä¸‹è½½posterã€ä¸ä¿ç•™posteræ—¶ï¼Œè¿”å›
    if "poster" not in download_files and "poster" not in keep_files:
        if poster_path:
            delete_file(poster_path)
        return True

    # æœ¬åœ°æœ‰posteræ—¶ï¼Œä¸”å‹¾é€‰ä¿ç•™æ—§æ–‡ä»¶æ—¶ï¼Œä¸ä¸‹è½½
    if poster_path and "poster" in keep_files:
        json_data["logs"] += "\n ğŸ€ Poster done! (old)(%ss)" % get_used_time(start_time)
        return True

    # ä¸ä¸‹è½½æ—¶è¿”å›
    if "poster" not in download_files:
        return True

    # å°è¯•å¤åˆ¶å…¶ä»–åˆ†é›†ã€‚çœ‹åˆ†é›†æœ‰æ²¡æœ‰ä¸‹è½½ï¼Œå¦‚æœä¸‹è½½å®Œæˆåˆ™å¯ä»¥å¤åˆ¶ï¼Œå¦åˆ™å°±è‡ªè¡Œä¸‹è½½
    if json_data["cd_part"]:
        done_poster_path = Flags.file_done_dic.get(json_data["number"]).get("poster")
        if (
            done_poster_path
            and os.path.exists(done_poster_path)
            and split_path(done_poster_path)[0] == split_path(poster_final_path)[0]
        ):
            copy_file(done_poster_path, poster_final_path)
            json_data["poster_from"] = "copy cd-poster"
            json_data["poster_path"] = poster_final_path
            json_data["logs"] += "\n ğŸ€ Poster done! (copy cd-poster)(%ss)" % get_used_time(start_time)
            return True

    # å‹¾é€‰å¤åˆ¶ thumbæ—¶ï¼šå›½äº§ï¼Œå¤åˆ¶thumb;æ— ç ï¼Œå‹¾é€‰ä¸è£å‰ªæ—¶ï¼Œä¹Ÿå¤åˆ¶thumb
    if thumb_path:
        mosaic = json_data["mosaic"]
        number = json_data["number"]
        copy_flag = False
        if number.startswith("FC2"):
            image_cut = "center"
            if "ignore_fc2" in download_files:
                copy_flag = True
        elif mosaic == "å›½äº§" or mosaic == "åœ‹ç”¢":
            image_cut = "right"
            if "ignore_guochan" in download_files:
                copy_flag = True
        elif mosaic == "æ— ç " or mosaic == "ç„¡ç¢¼" or mosaic == "ç„¡ä¿®æ­£":
            image_cut = "center"
            if "ignore_wuma" in download_files:
                copy_flag = True
        elif mosaic == "æœ‰ç " or mosaic == "æœ‰ç¢¼":
            if "ignore_youma" in download_files:
                copy_flag = True
        if copy_flag:
            copy_file(thumb_path, poster_final_path)
            json_data["poster_marked"] = json_data["thumb_marked"]
            json_data["poster_from"] = "copy thumb"
            json_data["poster_path"] = poster_final_path
            json_data["logs"] += "\n ğŸ€ Poster done! (copy thumb)(%ss)" % get_used_time(start_time)
            return True

    # è·å–é«˜æ¸… poster
    json_data = _get_big_poster(json_data)

    # ä¸‹è½½å›¾ç‰‡
    poster_url = json_data.get("poster")
    poster_from = json_data.get("poster_from")
    poster_final_path_temp = poster_final_path
    if os.path.exists(poster_final_path):
        poster_final_path_temp = poster_final_path + ".[DOWNLOAD].jpg"
    if json_data["image_download"]:
        start_time = time.time()
        if download_file_with_filepath(json_data, poster_url, poster_final_path_temp, folder_new_path):
            poster_size = check_pic(poster_final_path_temp)
            if poster_size:
                if (
                    not poster_from.startswith("Google")
                    or poster_size == json_data["poster_size"]
                    or "media-amazon.com" in poster_url
                ):
                    if poster_final_path_temp != poster_final_path:
                        move_file(poster_final_path_temp, poster_final_path)
                        delete_file(poster_final_path_temp)
                    if json_data["cd_part"]:
                        dic = {"poster": poster_final_path}
                        Flags.file_done_dic[json_data["number"]].update(dic)
                    json_data["poster_marked"] = False  # ä¸‹è½½çš„å›¾ï¼Œè¿˜æ²¡åŠ æ°´å°
                    json_data["poster_path"] = poster_final_path
                    json_data["logs"] += f"\n ğŸ€ Poster done! ({poster_from})({get_used_time(start_time)}s)"
                    return True
                else:
                    delete_file(poster_final_path_temp)
                    json_data["logs"] += f"\n ğŸŸ  æ£€æµ‹åˆ° Poster åˆ†è¾¨ç‡ä¸å¯¹{str(poster_size)}! å·²åˆ é™¤ ({poster_from})"

    # åˆ¤æ–­ä¹‹å‰æœ‰æ²¡æœ‰ poster å’Œ thumb
    if not poster_path and not thumb_path:
        json_data["poster_path"] = ""
        if "ignore_pic_fail" in download_files:
            json_data["logs"] += "\n ğŸŸ  Poster download failed! (ä½ å·²å‹¾é€‰ã€Œå›¾ç‰‡ä¸‹è½½å¤±è´¥æ—¶ï¼Œä¸è§†ä¸ºå¤±è´¥ï¼ã€) "
            json_data["logs"] += "\n ğŸ€ Poster done! (none)(%ss)" % get_used_time(start_time)
            return True
        else:
            json_data["logs"] += (
                "\n ğŸ”´ Poster download failed! ä½ å¯ä»¥åˆ°ã€Œè®¾ç½®ã€-ã€Œä¸‹è½½ã€ï¼Œå‹¾é€‰ã€Œå›¾ç‰‡ä¸‹è½½å¤±è´¥æ—¶ï¼Œä¸è§†ä¸ºå¤±è´¥ï¼ã€ "
            )
            json_data["error_info"] = (
                "Poster download failed! ä½ å¯ä»¥åˆ°ã€Œè®¾ç½®ã€-ã€Œä¸‹è½½ã€ï¼Œå‹¾é€‰ã€Œå›¾ç‰‡ä¸‹è½½å¤±è´¥æ—¶ï¼Œä¸è§†ä¸ºå¤±è´¥ï¼ã€"
            )
            return False

    # ä½¿ç”¨thumbè£å‰ª
    poster_final_path_temp = poster_final_path + ".[CUT].jpg"
    if fanart_path:
        thumb_path = fanart_path
    if cut_thumb_to_poster(json_data, thumb_path, poster_final_path_temp, image_cut):
        # è£å‰ªæˆåŠŸï¼Œæ›¿æ¢æ—§å›¾
        move_file(poster_final_path_temp, poster_final_path)
        if json_data["cd_part"]:
            dic = {"poster": poster_final_path}
            Flags.file_done_dic[json_data["number"]].update(dic)
        json_data["poster_path"] = poster_final_path
        json_data["poster_marked"] = False
        return True

    # è£å‰ªå¤±è´¥ï¼Œæœ¬åœ°æœ‰å›¾
    if poster_path:
        json_data["logs"] += "\n ğŸŸ  Poster cut failed! å°†ç»§ç»­ä½¿ç”¨ä¹‹å‰çš„å›¾ç‰‡ï¼"
        json_data["logs"] += "\n ğŸ€ Poster done! (old)(%ss) " % get_used_time(start_time)
        return True
    else:
        if "ignore_pic_fail" in download_files:
            json_data["logs"] += "\n ğŸŸ  Poster cut failed! (ä½ å·²å‹¾é€‰ã€Œå›¾ç‰‡ä¸‹è½½å¤±è´¥æ—¶ï¼Œä¸è§†ä¸ºå¤±è´¥ï¼ã€) "
            json_data["logs"] += "\n ğŸ€ Poster done! (none)(%ss)" % get_used_time(start_time)
            return True
        else:
            json_data["logs"] += (
                "\n ğŸ”´ Poster cut failed! ä½ å¯ä»¥åˆ°ã€Œè®¾ç½®ã€-ã€Œä¸‹è½½ã€ï¼Œå‹¾é€‰ã€Œå›¾ç‰‡ä¸‹è½½å¤±è´¥æ—¶ï¼Œä¸è§†ä¸ºå¤±è´¥ï¼ã€ "
            )
            json_data["error_info"] = "Poster failedï¼ä½ å¯ä»¥åˆ°ã€Œè®¾ç½®ã€-ã€Œä¸‹è½½ã€ï¼Œå‹¾é€‰ã€Œå›¾ç‰‡ä¸‹è½½å¤±è´¥æ—¶ï¼Œä¸è§†ä¸ºå¤±è´¥ï¼ã€"
            return False


def fanart_download(json_data, fanart_final_path):
    """
    å¤åˆ¶thumbä¸ºfanart
    """
    start_time = time.time()
    thumb_path = json_data["thumb_path"]
    fanart_path = json_data["fanart_path"]
    download_files = config.download_files
    keep_files = config.keep_files

    # ä¸ä¿ç•™ä¸ä¸‹è½½æ—¶åˆ é™¤è¿”å›
    if ",fanart" not in keep_files and ",fanart" not in download_files:
        if fanart_path and os.path.exists(fanart_path):
            delete_file(fanart_path)
        return True

    # ä¿ç•™ï¼Œå¹¶ä¸”æœ¬åœ°å­˜åœ¨ fanart.jpgï¼Œä¸ä¸‹è½½è¿”å›
    if ",fanart" in keep_files and fanart_path:
        json_data["logs"] += "\n ğŸ€ Fanart done! (old)(%ss)" % get_used_time(start_time)
        return True

    # ä¸ä¸‹è½½æ—¶ï¼Œè¿”å›
    if ",fanart" not in download_files:
        return True

    # å°è¯•å¤åˆ¶å…¶ä»–åˆ†é›†ã€‚çœ‹åˆ†é›†æœ‰æ²¡æœ‰ä¸‹è½½ï¼Œå¦‚æœä¸‹è½½å®Œæˆåˆ™å¯ä»¥å¤åˆ¶ï¼Œå¦åˆ™å°±è‡ªè¡Œä¸‹è½½
    if json_data["cd_part"]:
        done_fanart_path = Flags.file_done_dic.get(json_data["number"]).get("fanart")
        if (
            done_fanart_path
            and os.path.exists(done_fanart_path)
            and split_path(done_fanart_path)[0] == split_path(fanart_final_path)[0]
        ):
            if fanart_path:
                delete_file(fanart_path)
            copy_file(done_fanart_path, fanart_final_path)
            json_data["fanart_from"] = "copy cd-fanart"
            json_data["fanart_path"] = fanart_final_path
            json_data["logs"] += "\n ğŸ€ Fanart done! (copy cd-fanart)(%ss)" % get_used_time(start_time)
            return True

    # å¤åˆ¶thumb
    if thumb_path:
        if fanart_path:
            delete_file(fanart_path)
        copy_file(thumb_path, fanart_final_path)
        json_data["fanart_from"] = "copy thumb"
        json_data["fanart_path"] = fanart_final_path
        json_data["fanart_marked"] = json_data["thumb_marked"]
        json_data["logs"] += "\n ğŸ€ Fanart done! (copy thumb)(%ss)" % get_used_time(start_time)
        if json_data["cd_part"]:
            dic = {"fanart": fanart_final_path}
            Flags.file_done_dic[json_data["number"]].update(dic)
        return True
    else:
        # æœ¬åœ°æœ‰ fanart æ—¶ï¼Œä¸ä¸‹è½½
        if fanart_path:
            json_data["logs"] += "\n ğŸŸ  Fanart copy failed! æœªæ‰¾åˆ° thumb å›¾ç‰‡ï¼Œå°†ç»§ç»­ä½¿ç”¨ä¹‹å‰çš„å›¾ç‰‡ï¼"
            json_data["logs"] += "\n ğŸ€ Fanart done! (old)(%ss)" % get_used_time(start_time)
            return True

        else:
            if "ignore_pic_fail" in download_files:
                json_data["logs"] += "\n ğŸŸ  Fanart failed! (ä½ å·²å‹¾é€‰ã€Œå›¾ç‰‡ä¸‹è½½å¤±è´¥æ—¶ï¼Œä¸è§†ä¸ºå¤±è´¥ï¼ã€) "
                json_data["logs"] += "\n ğŸ€ Fanart done! (none)(%ss)" % get_used_time(start_time)
                return True
            else:
                json_data["logs"] += (
                    "\n ğŸ”´ Fanart failed! ä½ å¯ä»¥åˆ°ã€Œè®¾ç½®ã€-ã€Œä¸‹è½½ã€ï¼Œå‹¾é€‰ã€Œå›¾ç‰‡ä¸‹è½½å¤±è´¥æ—¶ï¼Œä¸è§†ä¸ºå¤±è´¥ï¼ã€ "
                )
                json_data["error_info"] = (
                    "Fanart ä¸‹è½½å¤±è´¥ï¼ä½ å¯ä»¥åˆ°ã€Œè®¾ç½®ã€-ã€Œä¸‹è½½ã€ï¼Œå‹¾é€‰ã€Œå›¾ç‰‡ä¸‹è½½å¤±è´¥æ—¶ï¼Œä¸è§†ä¸ºå¤±è´¥ï¼ã€"
                )
                return False


def extrafanart_download(json_data, folder_new_path):
    start_time = time.time()
    download_files = config.download_files
    keep_files = config.keep_files
    extrafanart_list = json_data.get("extrafanart")
    extrafanart_folder_path = os.path.join(folder_new_path, "extrafanart")

    # ä¸ä¸‹è½½ä¸ä¿ç•™æ—¶åˆ é™¤è¿”å›
    if "extrafanart" not in download_files and "extrafanart" not in keep_files:
        if os.path.exists(extrafanart_folder_path):
            shutil.rmtree(extrafanart_folder_path, ignore_errors=True)
        return

    # æœ¬åœ°å­˜åœ¨ extrafanart_folderï¼Œä¸”å‹¾é€‰ä¿ç•™æ—§æ–‡ä»¶æ—¶ï¼Œä¸ä¸‹è½½
    if "extrafanart" in keep_files and os.path.exists(extrafanart_folder_path):
        json_data["logs"] += "\n ğŸ€ Extrafanart done! (old)(%ss) " % get_used_time(start_time)
        return True

    # å¦‚æœ extrafanart ä¸ä¸‹è½½
    if "extrafanart" not in download_files:
        return True

    # æ£€æµ‹é“¾æ¥æœ‰æ•ˆæ€§
    if extrafanart_list and check_url(extrafanart_list[0]):
        extrafanart_folder_path_temp = extrafanart_folder_path
        if os.path.exists(extrafanart_folder_path_temp):
            extrafanart_folder_path_temp = extrafanart_folder_path + "[DOWNLOAD]"
            if not os.path.exists(extrafanart_folder_path_temp):
                os.makedirs(extrafanart_folder_path_temp)
        else:
            os.makedirs(extrafanart_folder_path_temp)

        extrafanart_count = 0
        extrafanart_count_succ = 0
        task_list = []
        for extrafanart_url in extrafanart_list:
            extrafanart_count += 1
            extrafanart_name = "fanart" + str(extrafanart_count) + ".jpg"
            extrafanart_file_path = os.path.join(extrafanart_folder_path_temp, extrafanart_name)
            task_list.append(
                [json_data, extrafanart_url, extrafanart_file_path, extrafanart_folder_path_temp, extrafanart_name]
            )
        extrafanart_pool = Pool(20)  # å‰§ç…§ä¸‹è½½çº¿ç¨‹æ± 
        result = extrafanart_pool.map(_mutil_extrafanart_download_thread, task_list)
        for res in result:
            if res:
                extrafanart_count_succ += 1
        if extrafanart_count_succ == extrafanart_count:
            if extrafanart_folder_path_temp != extrafanart_folder_path:
                shutil.rmtree(extrafanart_folder_path)
                os.rename(extrafanart_folder_path_temp, extrafanart_folder_path)
            json_data["logs"] += "\n ğŸ€ ExtraFanart done! ({} {}/{})({}s)".format(
                json_data["extrafanart_from"], extrafanart_count_succ, extrafanart_count, get_used_time(start_time)
            )
            return True
        else:
            json_data["logs"] += "\n ğŸŸ   ExtraFanart download failed! ({} {}/{})({}s)".format(
                json_data["extrafanart_from"], extrafanart_count_succ, extrafanart_count, get_used_time(start_time)
            )
            if extrafanart_folder_path_temp != extrafanart_folder_path:
                shutil.rmtree(extrafanart_folder_path_temp)
            else:
                json_data["logs"] += "\n ğŸ€ ExtraFanart done! (incomplete)(%ss)" % get_used_time(start_time)
                return False
        json_data["logs"] += "\n ğŸŸ  ExtraFanart download failed! å°†ç»§ç»­ä½¿ç”¨ä¹‹å‰çš„æœ¬åœ°æ–‡ä»¶ï¼"
    if os.path.exists(extrafanart_folder_path):  # ä½¿ç”¨æ—§æ–‡ä»¶
        json_data["logs"] += "\n ğŸ€ ExtraFanart done! (old)(%ss)" % get_used_time(start_time)
        return True


def show_netstatus():
    signal.show_net_info(time.strftime("%Y-%m-%d %H:%M:%S").center(80, "="))
    proxy_type = ""
    retry_count = 0
    proxy = ""
    timeout = 0
    try:
        proxy_type, proxy, timeout, retry_count = config.type, config.proxy, config.timeout, config.retry
    except:
        signal.show_traceback_log(traceback.format_exc())
        signal.show_net_info(traceback.format_exc())
    if proxy == "" or proxy_type == "" or proxy_type == "no":
        signal.show_net_info(
            " å½“å‰ç½‘ç»œçŠ¶æ€ï¼šâŒ æœªå¯ç”¨ä»£ç†\n   ç±»å‹ï¼š "
            + str(proxy_type)
            + "    åœ°å€ï¼š"
            + str(proxy)
            + "    è¶…æ—¶æ—¶é—´ï¼š"
            + str(timeout)
            + "    é‡è¯•æ¬¡æ•°ï¼š"
            + str(retry_count)
        )
    else:
        signal.show_net_info(
            " å½“å‰ç½‘ç»œçŠ¶æ€ï¼šâœ… å·²å¯ç”¨ä»£ç†\n   ç±»å‹ï¼š "
            + proxy_type
            + "    åœ°å€ï¼š"
            + proxy
            + "    è¶…æ—¶æ—¶é—´ï¼š"
            + str(timeout)
            + "    é‡è¯•æ¬¡æ•°ï¼š"
            + str(retry_count)
        )
    signal.show_net_info("=" * 80)


def check_proxyChange():
    new_proxy = (config.type, config.proxy, config.timeout, config.retry)
    if Flags.current_proxy:
        if new_proxy != Flags.current_proxy:
            signal.show_net_info("\nğŸŒˆ ä»£ç†è®¾ç½®å·²æ”¹å˜ï¼š")
            show_netstatus()
    Flags.current_proxy = new_proxy
