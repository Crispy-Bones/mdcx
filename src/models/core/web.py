"""
åˆ®å‰Šè¿‡ç¨‹çš„ç½‘ç»œæ“ä½œ
"""

import os
import re
import shutil
import time
import traceback
import urllib

from lxml import etree

from models.base.file import copy_file, delete_file, move_file, split_path
from models.base.image import check_pic, cut_thumb_to_poster
from models.base.pool import Pool
from models.base.utils import get_used_time
from models.base.web import check_url, get_amazon_data, get_big_pic_by_google, get_html, get_imgsize, multi_download
from models.config.config import config
from models.core.flags import Flags
from models.core.utils import convert_half
from models.signals import signal
from datetime import datetime

def get_actorname(number):
    # è·å–çœŸå®æ¼”å‘˜åå­—
    url = f"https://av-wiki.net/?s={number}"
    result, res = get_html(url)
    if not result:
        return False, f"Error: {res}"
    html_detail = etree.fromstring(res, etree.HTMLParser(encoding="utf-8"))
    actor_box = html_detail.xpath('//ul[@class="post-meta clearfix"]')
    for each in actor_box:
        actor_name = each.xpath('li[@class="actress-name"]/a/text()')
        actor_number = each.xpath('li[@class="actress-name"]/following-sibling::li[last()]/text()')
        if actor_number:
            if actor_number[0].upper().endswith(number.upper()) or number.upper().endswith(actor_number[0].upper()):
                return True, ",".join(actor_name)
    return False, "No Result!"


def get_yesjav_title(json_data, movie_number):
    yesjav_url = "http://www.yesjav.info/search.asp?q=%s&" % movie_number
    movie_title = ""
    result, response = get_html(yesjav_url)
    if result and response:
        parser = etree.HTMLParser(encoding="utf-8")
        html = etree.HTML(response, parser)
        movie_title = html.xpath(
            '//dl[@id="zi"]/p/font/a/b[contains(text(), $number)]/../../a[contains(text(), "ä¸­æ–‡å­—å¹•")]/text()',
            number=movie_number,
        )
        if movie_title:
            movie_title = movie_title[0]
            for each in config.char_list:
                movie_title = movie_title.replace(each, "")
            movie_title = movie_title.strip()
    return movie_title


def google_translate(title, outline):
    e1 = None
    e2 = None
    if title:
        title, e1 = _google_translate(title)
    if outline:
        outline, e2 = _google_translate(outline)
    return title, outline, e1 or e2


def _google_translate(msg: str) -> (str, str):
    try:
        msg_unquote = urllib.parse.unquote(msg)
        url = f"https://translate.google.com/translate_a/single?client=gtx&sl=auto&tl=zh-CN&dt=t&q={msg_unquote}"
        result, response = get_html(url, json_data=True)
        if not result:
            return msg, f"è¯·æ±‚å¤±è´¥ï¼å¯èƒ½æ˜¯è¢«å°äº†ï¼Œå¯å°è¯•æ›´æ¢ä»£ç†ï¼é”™è¯¯ï¼š{response}"
        return "".join([sen[0] for sen in response[0]]), ""
    except Exception as e:
        return msg, str(e)


def download_file_with_filepath(json_data, url, file_path, folder_new_path):
    if not url:
        return False

    if not os.path.exists(folder_new_path):
        os.makedirs(folder_new_path)
    try:
        if multi_download(url, file_path):
            return True
    except:
        pass
    json_data["logs"] += f"\n ğŸ¥º Download failed! {url}"
    return False


def _mutil_extrafanart_download_thread(task):
    json_data, extrafanart_url, extrafanart_file_path, extrafanart_folder_path, extrafanart_name = task
    if download_file_with_filepath(json_data, extrafanart_url, extrafanart_file_path, extrafanart_folder_path):
        if check_pic(extrafanart_file_path):
            return True
    else:
        json_data["logs"] += f"\n ğŸ’¡ {extrafanart_name} download failed! ( {extrafanart_url} )"
        return False


def get_actor_list(json_data, title, raw_actor_list):
    """
    å¯¹å«æœ‰ ï¼ˆï¼‰çš„æ¼”å‘˜åè¿›è¡Œæ‹†åˆ†æ•´åˆ, è¿”å›å»é‡åˆ—è¡¨å’Œæ ‡é¢˜ä¸­çš„æ¼”å‘˜å 
    """
    print(f"åŸå§‹æ ‡é¢˜: {title}\nåŸå§‹æ¼”å‘˜åˆ—è¡¨: {raw_actor_list}")
    def split_actor(raw_actor_list):
        # åˆ›å»ºä¸€ä¸ªç©ºé›†åˆç”¨äºå­˜å‚¨ç»“æœ
        actor_set = set()
        # éå†åˆ—è¡¨
        for item in raw_actor_list:
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…æ‹¬å·å†…å®¹
            match = re.match(r"(.+?)[ï¼ˆ\(](.+?)[ï¼‰\)]", item)
            if match:  # å¦‚æœåŒ¹é…æˆåŠŸ
                name_before_bracket = match.group(1).strip()  # æ‹¬å·å‰çš„å†…å®¹
                name_in_bracket = match.group(2).strip()      # æ‹¬å·å†…çš„å†…å®¹
                
                # å°†æ‹†åˆ†åçš„ä¸¤ä¸ªéƒ¨åˆ†æ·»åŠ åˆ°é›†åˆä¸­
                actor_set.add(name_before_bracket)
                actor_set.add(name_in_bracket)
            else:
                # å¦‚æœæ²¡æœ‰æ‹¬å·ï¼Œç›´æ¥æ·»åŠ åˆ°é›†åˆä¸­
                actor_set.add(item.strip())

        # å°†é›†åˆè½¬æ¢ä¸ºåˆ—è¡¨
        return list(actor_set)
    
    actor_in_title_list = []
    actor_list = split_actor(raw_actor_list)
    for item in actor_list:
        if str(item).upper() in str(title).upper():
            actor_in_title_list.append(item)
            break
    if not actor_in_title_list:
        amazon_orginaltitle_actor = json_data.get("amazon_orginaltitle_actor")
        actor_in_title_list = split_actor = ([amazon_orginaltitle_actor])
    print(f"æ•´åˆåçš„æ¼”å‘˜åˆ—è¡¨: {actor_list}\næ ‡é¢˜ä¸­çš„æ¼”å‘˜åˆ—è¡¨: {actor_in_title_list}")
    return actor_list, actor_in_title_list

def get_halfwidth_no_actor_title(title, actor_list, operation_flags=0b111):
    halfwidth_title = convert_half(title, operation_flags)
    for actor in actor_list:
        halfwidth_title = halfwidth_title.replace(actor, "")
    halfwidth_no_actor_title = halfwidth_title.strip()
    return halfwidth_title, halfwidth_no_actor_title

def split_title(original_title, actor_list, separator=" ", extra_separator=None):
    """
    1. ç§»é™¤æ ‡é¢˜ä¸­ã€ã€‘åŠå…¶å†…å®¹ï¼›
    2. å¯¹æ ‡é¢˜è¿›è¡Œæ•æ„Ÿè¯è½¬æ¢ï¼Œè‹¥è½¬æ¢åç»“æœä¸åŒåˆ™åŠ å…¥ç»“æœåˆ—è¡¨ï¼›
    3. æ„å»ºåˆ†éš”ç¬¦æ­£åˆ™è¡¨è¾¾å¼ï¼›
    4. è‹¥åŸå§‹æ ‡é¢˜ä¸åŒ…å«ä»»ä½•åˆ†éš”ç¬¦ï¼Œåˆ™ç›´æ¥è¿”å›åŸºç¡€æ ‡é¢˜åˆ—è¡¨ï¼›
    5. å¦åˆ™æŒ‰ä¸»åˆ†éš”ç¬¦æ‹†åˆ†å¹¶è¿‡æ»¤æ— æ•ˆå­ä¸²ï¼›
    6. è‹¥æœ‰é¢å¤–åˆ†éš”ç¬¦ï¼Œç»§ç»­æŒ‰å…¶æ‹†åˆ†ï¼›
    7. æœ€ç»ˆè¿”å›åŸå§‹æ ‡é¢˜ä¸æ•æ„Ÿè¯è½¬æ¢åçš„æ ‡é¢˜åˆ—è¡¨ï¼Œä»¥åŠæ‰€æœ‰æœ‰æ•ˆæ ‡é¢˜ç‰‡æ®µçš„å»é‡åˆ—è¡¨ã€‚
    """
    
    # å»é™¤ã€ã€‘å†…çš„å†…å®¹
    original_title = re.sub(r"ã€.*?ã€‘", "", original_title).strip()
    original_title = original_title.replace("ï¼ˆDODï¼‰", "").strip()
    
    # åˆå§‹åŒ–åŸºç¡€æ ‡é¢˜åˆ—è¡¨
    original_title_base_list = [original_title]
    
    # æ•æ„Ÿè¯æ›¿æ¢ APAK-162
    original_title_special = convert_half(original_title, operation_flags=0b001)
    if original_title_special != original_title:
        original_title_base_list.append(original_title_special)
    
    # å»é‡å¹¶ä¿æŒé¡ºåº
    original_title_base_list = list(dict.fromkeys(original_title_base_list))
    
    # æ„é€ åˆ†éš”ç¬¦çš„æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
    pattern_parts = []
    separator_list = [separator]
    if extra_separator:
        separator_list.extend(extra_separator.split(","))
    for each_sep in separator_list:
        pattern_parts.append(re.escape(each_sep))
    pattern = "|".join(pattern_parts)
    
    # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°åˆ†éš”ç¬¦ï¼Œç›´æ¥è¿”å›åŸºç¡€æ ‡é¢˜åˆ—è¡¨
    if not re.search(pattern, original_title):
        return original_title_base_list, original_title_base_list
    
    def is_valid_part(part, actor_list):
        """åˆ¤æ–­ä¸€ä¸ªç‰‡æ®µæ˜¯å¦æœ‰æ•ˆ"""
        part = part.strip()
        if not part:
            return False
        if part in actor_list:
            return False
        if len(part) > 8:
            return True
        if len(part) > 4 and not re.search(r"(^[a-zA-Z]+-\d+$)|(^[a-zA-Z0-9]+$)", part):
            return True
        return False
    
    def split_and_filter(title, sep):
        """è¾…åŠ©å‡½æ•°ï¼šæŒ‰åˆ†éš”ç¬¦æ‹†åˆ†æ ‡é¢˜å¹¶è¿‡æ»¤æ— æ•ˆå­ä¸²"""
        parts = title.split(sep)
        return [part for part in parts if is_valid_part(part, actor_list)]
    
    # å…ˆä»¥ç©ºæ ¼æ‹†åˆ†
    split_title_with_space = []
    for title in original_title_base_list:
        split_title_with_space.extend(split_and_filter(title, separator))
    
    # å†ä»¥é¢å¤–åˆ†éš”ç¬¦æ‹†åˆ†
    if extra_separator:
        for extra in extra_separator.split(","):
            split_title_with_extra = []
            base_titles = split_title_with_space or original_title_base_list
            for title in base_titles:
                split_title_with_extra.extend(split_and_filter(title, extra))
            split_title_with_space.extend(split_title_with_extra)
    
    # åˆå¹¶æ‰€æœ‰æœ‰æ•ˆæ ‡é¢˜ç‰‡æ®µå¹¶å»é‡
    all_titles = original_title_base_list + split_title_with_space
    original_title_list = list(dict.fromkeys(all_titles))
    
    # è¿”å›ä¸¤ä¸ªåˆ—è¡¨
    return original_title_base_list, original_title_list

def has_common_substring(title1, title2, length=5):
    # æ£€æŸ¥ä¸¤ä¸ªå­—ç¬¦ä¸²çš„é•¿åº¦æ˜¯å¦è‡³å°‘ä¸ºlength
    print(f"æ£€æŸ¥å­—ç¬¦ä¸²åŒ¹é…æƒ…å†µ:\ntitle1: {title1}\ntitle2: {title2}\nlength: {length}")
    if len(title1) < length or len(title2) < length:
        return False
    # ç”Ÿæˆaçš„æ‰€æœ‰é•¿åº¦ä¸ºlengthçš„è¿ç»­å­ä¸²ï¼Œå¹¶æ£€æŸ¥æ˜¯å¦å­˜åœ¨äºbä¸­
    for i in range(len(title1) - length + 1):
        substring = title1[i:i+length]
        if substring in title2:
            return True
    return False

def check_detail_page(json_data, original_title_list, title_match_ele, actor_amazon):
    """
    è·å–amazonçš„è¯¦æƒ…é¡µ, æ£€æµ‹æ¼”å‘˜åæ˜¯å¦åŒ¹é…, å‘è¡Œæ—¥æœŸæ˜¯å¦å»åˆ
    """
    detail_url = title_match_ele[1]
    try:
        url_new = "https://www.amazon.co.jp" + re.findall(r"(/dp/[^/]+)", detail_url)[0]
    except:
        
        url_new = detail_url
    print(f"è¯¦æƒ…é¡µurl: {url_new}")
    result, html_detail = get_amazon_data(url_new)
    if result and html_detail:
        html = etree.fromstring(html_detail, etree.HTMLParser())
        # è·å–æ¼”å‘˜å
        detail_actor = str(html.xpath('//span[@class="author notFaded"]/a/text()')).replace(" ", "")
        detail_info_1 = str(
            html.xpath('//ul[@class="a-unordered-list a-vertical a-spacing-mini"]//text()')
        ).replace(" ", "")
        detail_info_2 = str(
            html.xpath('//div[@id="detailBulletsWrapper_feature_div"]//text()')
        ).replace(" ", "")
        detail_info_3 = str(html.xpath('//div[@id="productDescription"]//text()')).replace(" ", "")
        all_info = detail_actor + detail_info_1 + detail_info_2 + detail_info_3
        # è·å–å‘è¡Œæ—¥æœŸ
        date_text = html.xpath("//span[contains(text(), 'ç™ºå£²æ—¥')]/following-sibling::span[1]/text()")
        release_amazon = date_text[0].strip() if date_text else "1970/1/1"
        release_date_amazon = datetime.strptime(release_amazon, "%Y/%m/%d")
        release_date = datetime.strptime(json_data.get('release'), "%Y-%m-%d")
        print(f"è¯¦æƒ…é¡µæ—¥æœŸï¼š{release_date_amazon.strftime('%Y-%m-%d')}")
        print(f"release date: {json_data.get('release')}'")
        if (abs((release_date_amazon - release_date).days) < 30
            or release_amazon == "1970/1/1"
            or not release_date
        ):
            for each_actor in actor_amazon:
                if each_actor in all_info:
                    print(f"è¯¦æƒ…é¡µåŒ¹é…åˆ°æ¼”å‘˜: {each_actor}")
                    return True, True
            print(f"è¯¦æƒ…é¡µæœªåŒ¹é…åˆ°æ¼”å‘˜")
            
            for each_title in original_title_list:
                detail_page_title = title_match_ele[2]
                if has_common_substring(each_title, detail_page_title, 20): # æ ‡é¢˜åŒ¹é…è¾¾åˆ°20ä¸ªå­—ç¬¦
                    print(f"æ ‡é¢˜åŒ¹é…åº¦é«˜")
                    return False, True
            print(f"æ ‡é¢˜åŒ¹é…åº¦ä½")
        else:
            print(f"å‘è¡Œæ—¥æœŸä¸åŒ¹é…")
            return False, False
    return False, False

def get_big_pic_by_amazon(json_data, originaltitle_amazon, actor_amazon):
    if not originaltitle_amazon or not actor_amazon:
        return ""
    hd_pic_url = ""
    actor_amazon, amazon_orginaltitle_actor = get_actor_list(json_data, originaltitle_amazon, actor_amazon)

    # æ‹†åˆ†æ ‡é¢˜
    originaltitle_amazon_base_list, originaltitle_amazon_list = split_title(originaltitle_amazon, actor_amazon, " ", "â€¦")
    # å›¾ç‰‡urlè¿‡æ»¤é›†åˆ, å‘½ä¸­ç›´æ¥è·³è¿‡
    pic_url_filtered_set = set()
    # æ ‡é¢˜è¿‡æ»¤é›†åˆ, å‘½ä¸­ç›´æ¥è·³è¿‡
    pic_title_filtered_set = set()
    # å»é™¤æ ‡ç‚¹ç©ºæ ¼, å…¨è§’è½¬åŠè§’åçš„æ ‡é¢˜åˆ—è¡¨
    originaltitle_amazon_halfwidth_no_actor_base_list = []
    for each_title in originaltitle_amazon_base_list:
        originaltitle_amazon_halfwidth_no_actor_base_list.append(get_halfwidth_no_actor_title(each_title, actor_amazon, operation_flags=0b110)[1])
    print(f"å»é™¤æ ‡ç‚¹ç©ºæ ¼å’Œæ¼”å‘˜å, å…¨è§’è½¬åŠè§’åçš„æ ‡é¢˜åˆ—è¡¨: {originaltitle_amazon_halfwidth_no_actor_base_list}")
    # æœç´¢æ ‡é¢˜
    for originaltitle_amazon in originaltitle_amazon_list:
        print(f"\n/********************å¼€å§‹æœç´¢************************/")
        print(f"æ ‡é¢˜åˆ—è¡¨: {originaltitle_amazon_list}")
        print(f"æœç´¢æ ‡é¢˜: {originaltitle_amazon}")
        print(f"å›¾ç‰‡urlè¿‡æ»¤é›†åˆ: pic_url_filtered_set = {pic_url_filtered_set}") 
        print(f"æ ‡é¢˜è¿‡æ»¤é›†åˆ: pic_title_filtered_set = {pic_title_filtered_set}")
        # éœ€è¦ä¸¤æ¬¡urlencodeï¼Œnb_sb_nossè¡¨ç¤ºæ— æ¨èæ¥æº
        url_search = (
            "https://www.amazon.co.jp/black-curtain/save-eligibility/black-curtain?returnUrl=/s?k="
            + urllib.parse.quote_plus(urllib.parse.quote_plus(originaltitle_amazon.replace("&", " ") + " [DVD]"))
            + "&ref=nb_sb_noss"
        )
        result, html_search = get_amazon_data(url_search)

        if result and html_search:
            # æ— ç»“æœç›´æ¥è·³è¿‡
            if "æ¤œç´¢ã«ä¸€è‡´ã™ã‚‹å•†å“ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚" in html_search:
                print(f"æ— æœç´¢ç»“æœ, ç»“æŸæœ¬æ¬¡æœç´¢\n")
                continue
            html = etree.fromstring(html_search, etree.HTMLParser())
            originaltitle_amazon_halfwidth, originaltitle_amazon_halfwidth_no_actor = get_halfwidth_no_actor_title(originaltitle_amazon, actor_amazon)
            # æ£€æŸ¥æœç´¢ç»“æœ
            title_match_list = []
            # s-card-container s-overflow-hidden aok-relative puis-wide-grid-style puis-wide-grid-style-t2 puis-expand-height puis-include-content-margin puis s-latency-cf-section s-card-border
            pic_card = html.xpath('//div[@class="a-section a-spacing-base"]')
            print(f"æ‰¾åˆ°{len(pic_card)}ä¸ªç»“æœ")
            
            # å¼€å§‹å¤„ç†æœç´¢ç»“æœ, å¦‚æœç»“æœè¿‡å¤š, åªå¤„ç†å‰20ä¸ªç»“æœ
            for each in pic_card[:20]:  # tek-077
                pic_ver_list = each.xpath(
                    'div//a[@class="a-size-base a-link-normal s-underline-text s-underline-link-text s-link-style a-text-bold"]/text()'
                )
                pic_title_list = each.xpath(
                    'div//h2[@class="a-size-base-plus a-spacing-none a-color-base a-text-normal"]/span/text()'
                )
                pic_url_list = each.xpath('div//div[@class="a-section aok-relative s-image-square-aspect"]/img/@src')
                detail_url_list = each.xpath('div//a[@class="a-link-normal s-no-outline"]/@href')
                
                if len(pic_ver_list) and len(pic_url_list) and (len(pic_title_list) and len(detail_url_list)):
                    pic_ver = pic_ver_list[0]  # å›¾ç‰‡ç‰ˆæœ¬
                    pic_title = pic_title_list[0]  # å›¾ç‰‡æ ‡é¢˜
                    pic_url = pic_url_list[0]  # å›¾ç‰‡é“¾æ¥
                    pic_trunc_url = re.sub(r"\._[_]?AC_[^\.]+\.", ".", pic_url) # å»é™¤åç¼€ä»¥è·å¾—æ›´é«˜åˆ†è¾¨ç‡çš„å›¾ç‰‡
                    detail_url = detail_url_list[0]  # è¯¦æƒ…é¡µé“¾æ¥ï¼ˆæœ‰æ—¶å¸¦æœ‰æ¼”å‘˜åï¼‰

                    # å»é™¤é DVDä¸æ— å›¾ç‰‡çš„ç»“æœ
                    if (pic_ver not in ["DVD", "Software Download"]
                        or ".jpg" not in pic_trunc_url
                    ):
                        print(f"\næ— æ•ˆæ ‡é¢˜, è·³è¿‡: {pic_title}")
                        print(f"æ·»åŠ åˆ°è¿‡æ»¤é›†åˆ")
                        pic_title_filtered_set.add(pic_title)
                        pic_url_filtered_set.add(pic_trunc_url)
                        continue
                    
                    if pic_title in pic_title_filtered_set:
                        print(f"\nè·³è¿‡å·²è¿‡æ»¤çš„æ ‡é¢˜: {pic_title}")
                        continue
                    
                    w, h = get_imgsize(pic_trunc_url)
                    if w < 700 or w >= h:
                        print(f"\nå›¾ç‰‡éé«˜æ¸…æˆ–éç«–ç‰ˆ, è·³è¿‡: {pic_trunc_url}")
                        print(f"æ·»åŠ åˆ°è¿‡æ»¤é›†åˆ")
                        pic_title_filtered_set.add(pic_title)
                        pic_url_filtered_set.add(pic_trunc_url)
                        continue
                    
                    # é¿å…å•ä½“ä½œå“å–åˆ°åˆé›†ç»“æœ GVH-435
                    collection_keywords = ['BEST', 'æ™‚é–“', 'ç·é›†ç·¨', 'å®Œå…¨', 'æšçµ„']
                    skip_flag = False
                    for collection_keyword in collection_keywords:
                        contains_s1 = collection_keyword in str(originaltitle_amazon_halfwidth_no_actor_base_list[0]).upper()
                        contains_s2 = collection_keyword in str(pic_title).upper()
                        if contains_s1 != contains_s2:
                            skip_flag = True
                    if skip_flag:
                        print(f"\nåˆé›†æ ‡é¢˜, è·³è¿‡: {pic_title}")
                        print(f"æ·»åŠ åˆ°è¿‡æ»¤é›†åˆ")
                        pic_title_filtered_set.add(pic_title)
                        pic_url_filtered_set.add(pic_trunc_url)
                        continue
                    print(f"\n+++++++++++++++++++++++++æ£€æµ‹æœ‰æ•ˆé“¾æ¥+++++++++++++++++++++++++")
                    print(f"æœç´¢æ ‡é¢˜: {originaltitle_amazon}")
                    print(f"é“¾æ¥å†…å®¹:\npic_ver = {pic_ver}\npic_title = {pic_title}\npic_trunc_url = {pic_trunc_url}")
                    pic_title_halfwidth = convert_half(re.sub(r"ã€.*ã€‘", "", pic_title))
                    if pic_trunc_url in pic_url_filtered_set:
                        print(f"\nè·³è¿‡å·²è¿‡æ»¤çš„å›¾ç‰‡url: {pic_trunc_url}")
                        continue
                    print(f"pic_titleå»é™¤æ¼”å‘˜å:")
                    print(f"pic_title_halfwidth = {pic_title_halfwidth}")
                    for each_actor in actor_amazon:
                        pic_title_halfwidth = pic_title_halfwidth.replace(each_actor, "")
                    pic_title_halfwidth_no_actor = pic_title_halfwidth.strip()
                    print(f"pic_title_halfwidth_no_actor = {pic_title_halfwidth_no_actor}\n")

                    # åˆ¤æ–­æ ‡é¢˜æ˜¯å¦å‘½ä¸­
                    if (
                        originaltitle_amazon_halfwidth[:15] in pic_title_halfwidth
                        or originaltitle_amazon_halfwidth_no_actor[:15] in pic_title_halfwidth_no_actor
                        or has_common_substring(originaltitle_amazon_halfwidth,pic_title_halfwidth)
                        or has_common_substring(originaltitle_amazon_halfwidth_no_actor,pic_title_halfwidth_no_actor)
                    ):
                        print(f"å‘½ä¸­æ ‡é¢˜:\noriginaltitle_amazon_halfwidth = {originaltitle_amazon_halfwidth}\npic_title_halfwidth = {pic_title_halfwidth}")
                        print(f"originaltitle_amazon_halfwidth_no_actor = {originaltitle_amazon_halfwidth_no_actor}\npic_title_halfwidth_no_actor = {pic_title_halfwidth_no_actor}\n")
                        detail_url = urllib.parse.unquote_plus(detail_url)
                        temp_title = re.findall(r"(.+)keywords=", detail_url)
                        temp_detail_url = (
                            temp_title[0] + pic_title_halfwidth if temp_title else detail_url + pic_title_halfwidth
                        )
                        detail_url_full = "https://www.amazon.co.jp" + detail_url

                        # åˆ¤æ–­æ¼”å‘˜æ˜¯å¦åœ¨æ ‡é¢˜é‡Œï¼Œé¿å…åŒåæ ‡é¢˜è¯¯åŒ¹é… MOPP-023
                        for each_actor in actor_amazon:
                            if each_actor in temp_detail_url:
                                print(f"å‘½ä¸­æ¼”å‘˜: {each_actor}")
                                print(f"é‡‡ç”¨æ­¤ç»“æœ")
                                hd_pic_url = pic_trunc_url
                                return hd_pic_url
                        else:
                            # å¦‚æœæ²¡æœ‰å‘½ä¸­ä»»ä½•æ¼”å‘˜ï¼Œæ·»åŠ åˆ° title_match_list
                            print(f"æ²¡æœ‰å‘½ä¸­æ¼”å‘˜, æ·»åŠ åˆ° title_match_list")
                            title_match_list.append([pic_trunc_url, detail_url_full, pic_title_halfwidth_no_actor])
                            print(f"title_match_list_pic_only = {[element[0] for element in title_match_list]}")
                    else:
                        print(f"æ ‡é¢˜æœªå‘½ä¸­, è·³è¿‡: {pic_title}")
                        print(f"æ·»åŠ åˆ°è¿‡æ»¤é›†åˆ")
                        pic_title_filtered_set.add(pic_title)
                        pic_url_filtered_set.add(pic_trunc_url)
                else:
                    print(f"\nè·³è¿‡ä¸åŒ…å«ç±»å‹, å›¾ç‰‡, æ ‡é¢˜, è¯¦æƒ…é¡µé¢çš„ç»“æœ")
                    pass
                    
            # å½“æœç´¢ç»“æœå‘½ä¸­äº†æ ‡é¢˜ï¼Œæ²¡æœ‰å‘½ä¸­æ¼”å‘˜æ—¶ï¼Œå°è¯•å»è¯¦æƒ…é¡µè·å–æ¼”å‘˜ä¿¡æ¯
            if (
                len(title_match_list) > 0
                and len(title_match_list) <= 20
                and "s-pagination-item s-pagination-next s-pagination-button s-pagination-button-accessibility s-pagination-separator" not in html_search
            ):
                print(f"å°è¯•å»è¯¦æƒ…é¡µè·å–æ¼”å‘˜ä¿¡æ¯")
                # æ£€æµ‹å‰4ä¸ªç»“æœ
                title_match_pic_list = []
                for each in title_match_list[:4]:
                    actor_match, title_match =  check_detail_page(json_data, originaltitle_amazon_halfwidth_no_actor_base_list, each, actor_amazon)
                    if actor_match:
                        print(f"è¯¦æƒ…é¡µæ£€æµ‹é€šè¿‡, é‡‡ç”¨æ­¤ç»“æœ")
                        return each[0]
                    elif title_match:
                        title_match_pic_list.append(each[0])
                if len(title_match_pic_list) > 0:
                    print(f"é‡‡ç”¨ç¬¬ä¸€ä¸ªæ ‡é¢˜åŒ¹é…åº¦é«˜çš„ç»“æœ")
                    return title_match_pic_list[0]
                else:
                    print(f"è¯¦æƒ…é¡µæ£€æµ‹æœªé€šè¿‡, æ·»åŠ åˆ°è¿‡æ»¤é›†åˆ")
                    print(f"è¯¦æƒ…é¡µæ ‡é¢˜: pic_title = {pic_title}")
                    pic_url_filtered_set.add(each[0])
                    pic_title_filtered_set.add(pic_title)
                    
            # æœ‰å¾ˆå¤šç»“æœæ—¶ï¼ˆæœ‰ä¸‹ä¸€é¡µæŒ‰é’®ï¼‰ï¼ŒåŠ æ¼”å‘˜åå­—é‡æ–°æœç´¢
            if (
                "s-pagination-item s-pagination-next s-pagination-button s-pagination-button-accessibility s-pagination-separator" in html_search
                or len(title_match_list) > 5
            ):
                if amazon_orginaltitle_actor:
                    for actor in amazon_orginaltitle_actor:
                        if not (actor in originaltitle_amazon):
                            originaltitle_amazon_list.extend([originaltitle_amazon + ' ' + actor])
                    print(f"\næ·»åŠ æ¼”å‘˜åå†æ¬¡æœç´¢")

    return hd_pic_url


def trailer_download(json_data, folder_new_path, folder_old_path, naming_rule):
    start_time = time.time()
    download_files = config.download_files
    keep_files = config.keep_files
    trailer_name = config.trailer_name
    trailer_url = json_data["trailer"]
    trailer_old_folder_path = os.path.join(folder_old_path, "trailers")
    trailer_new_folder_path = os.path.join(folder_new_path, "trailers")

    # é¢„å‘Šç‰‡åå­—ä¸å«è§†é¢‘æ–‡ä»¶åï¼ˆåªè®©ä¸€ä¸ªè§†é¢‘å»ä¸‹è½½å³å¯ï¼‰
    if trailer_name == 1:
        trailer_folder_path = os.path.join(folder_new_path, "trailers")
        trailer_file_name = "trailer.mp4"
        trailer_file_path = os.path.join(trailer_folder_path, trailer_file_name)

        # é¢„å‘Šç‰‡æ–‡ä»¶å¤¹å·²åœ¨å·²å¤„ç†åˆ—è¡¨æ—¶ï¼Œè¿”å›ï¼ˆè¿™æ—¶åªéœ€è¦ä¸‹è½½ä¸€ä¸ªï¼Œå…¶ä»–åˆ†é›†ä¸éœ€è¦ä¸‹è½½ï¼‰
        if trailer_folder_path in Flags.trailer_deal_set:
            return
        Flags.trailer_deal_set.add(trailer_folder_path)

        # ä¸ä¸‹è½½ä¸ä¿ç•™æ—¶åˆ é™¤è¿”å›
        if "trailer" not in download_files and "trailer" not in keep_files:
            # åˆ é™¤ç›®æ ‡æ–‡ä»¶å¤¹å³å¯ï¼Œå…¶ä»–æ–‡ä»¶å¤¹å’Œæ–‡ä»¶å·²ç»åˆ é™¤äº†
            if os.path.exists(trailer_folder_path):
                shutil.rmtree(trailer_folder_path, ignore_errors=True)
            return

    else:
        # é¢„å‘Šç‰‡å¸¦æ–‡ä»¶åï¼ˆæ¯ä¸ªè§†é¢‘éƒ½æœ‰æœºä¼šä¸‹è½½ï¼Œå¦‚æœå·²æœ‰ä¸‹è½½å¥½çš„ï¼Œåˆ™ä½¿ç”¨å·²ä¸‹è½½çš„ï¼‰
        trailer_file_name = naming_rule + "-trailer.mp4"
        trailer_folder_path = folder_new_path
        trailer_file_path = os.path.join(trailer_folder_path, trailer_file_name)

        # ä¸ä¸‹è½½ä¸ä¿ç•™æ—¶åˆ é™¤è¿”å›
        if "trailer" not in download_files and "trailer" not in keep_files:
            # åˆ é™¤ç›®æ ‡æ–‡ä»¶ï¼Œåˆ é™¤é¢„å‘Šç‰‡æ—§æ–‡ä»¶å¤¹ã€æ–°æ–‡ä»¶å¤¹ï¼ˆdeal old fileæ—¶æ²¡åˆ é™¤ï¼‰
            if os.path.exists(trailer_file_path):
                delete_file(trailer_file_path)
            if os.path.exists(trailer_old_folder_path):
                shutil.rmtree(trailer_old_folder_path, ignore_errors=True)
            if trailer_new_folder_path != trailer_old_folder_path and os.path.exists(trailer_new_folder_path):
                shutil.rmtree(trailer_new_folder_path, ignore_errors=True)
            return

    # é€‰æ‹©ä¿ç•™æ–‡ä»¶ï¼Œå½“å­˜åœ¨æ–‡ä»¶æ—¶ï¼Œä¸ä¸‹è½½ã€‚ï¼ˆdone trailer path æœªè®¾ç½®æ—¶ï¼ŒæŠŠå½“å‰æ–‡ä»¶è®¾ç½®ä¸º done trailer pathï¼Œä»¥ä¾¿å…¶ä»–åˆ†é›†å¤åˆ¶ï¼‰
    if "trailer" in keep_files and os.path.exists(trailer_file_path):
        if not Flags.file_done_dic.get(json_data["number"]).get("trailer"):
            Flags.file_done_dic[json_data["number"]].update({"trailer": trailer_file_path})
            # å¸¦æ–‡ä»¶åæ—¶ï¼Œåˆ é™¤æ‰æ–°ã€æ—§æ–‡ä»¶å¤¹ï¼Œç”¨ä¸åˆ°äº†ã€‚ï¼ˆå…¶ä»–åˆ†é›†å¦‚æœæ²¡æœ‰ï¼Œå¯ä»¥å¤åˆ¶ç¬¬ä¸€ä¸ªæ–‡ä»¶çš„é¢„å‘Šç‰‡ã€‚æ­¤æ—¶ä¸åˆ ï¼Œæ²¡æœºä¼šåˆ é™¤äº†ï¼‰
            if trailer_name == 0:
                if os.path.exists(trailer_old_folder_path):
                    shutil.rmtree(trailer_old_folder_path, ignore_errors=True)
                if trailer_new_folder_path != trailer_old_folder_path and os.path.exists(trailer_new_folder_path):
                    shutil.rmtree(trailer_new_folder_path, ignore_errors=True)
        json_data["logs"] += "\n ğŸ€ Trailer done! (old)(%ss) " % get_used_time(start_time)
        return True

    # å¸¦æ–‡ä»¶åæ—¶ï¼Œé€‰æ‹©ä¸‹è½½ä¸ä¿ç•™ï¼Œæˆ–è€…é€‰æ‹©ä¿ç•™ä½†æ²¡æœ‰é¢„å‘Šç‰‡ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–åˆ†é›†å·²ä¸‹è½½æˆ–æœ¬åœ°é¢„å‘Šç‰‡
    # é€‰æ‹©ä¸‹è½½ä¸ä¿ç•™ï¼Œå½“æ²¡æœ‰ä¸‹è½½æˆåŠŸæ—¶ï¼Œä¸ä¼šåˆ é™¤ä¸ä¿ç•™çš„æ–‡ä»¶
    done_trailer_path = Flags.file_done_dic.get(json_data["number"]).get("trailer")
    if trailer_name == 0 and done_trailer_path and os.path.exists(done_trailer_path):
        if os.path.exists(trailer_file_path):
            delete_file(trailer_file_path)
        copy_file(done_trailer_path, trailer_file_path)
        json_data["logs"] += "\n ğŸ€ Trailer done! (copy trailer)(%ss)" % get_used_time(start_time)
        return

    # ä¸ä¸‹è½½æ—¶è¿”å›ï¼ˆé€‰æ‹©ä¸ä¸‹è½½ä¿ç•™ï¼Œä½†æœ¬åœ°å¹¶ä¸å­˜åœ¨ï¼Œæ­¤æ—¶è¿”å›ï¼‰
    if "trailer," not in download_files:
        return

    # ä¸‹è½½é¢„å‘Šç‰‡,æ£€æµ‹é“¾æ¥æœ‰æ•ˆæ€§
    content_length = check_url(trailer_url, length=True)
    if content_length:
        # åˆ›å»ºæ–‡ä»¶å¤¹
        if trailer_name == 1 and not os.path.exists(trailer_folder_path):
            os.makedirs(trailer_folder_path)

        # å¼€å§‹ä¸‹è½½
        download_files = config.download_files
        signal.show_traceback_log(f"ğŸ” {json_data['number']} download trailer... {trailer_url}")
        trailer_file_path_temp = trailer_file_path
        if os.path.exists(trailer_file_path):
            trailer_file_path_temp = trailer_file_path + ".[DOWNLOAD].mp4"
        if download_file_with_filepath(json_data, trailer_url, trailer_file_path_temp, trailer_folder_path):
            file_size = os.path.getsize(trailer_file_path_temp)
            if file_size >= content_length or "ignore_size" in download_files:
                json_data["logs"] += "\n ğŸ€ Trailer done! ({} {}/{})({}s) ".format(
                    json_data["trailer_from"], file_size, content_length, get_used_time(start_time)
                )
                signal.show_traceback_log(f"âœ… {json_data['number']} trailer done!")
                if trailer_file_path_temp != trailer_file_path:
                    move_file(trailer_file_path_temp, trailer_file_path)
                    delete_file(trailer_file_path_temp)
                done_trailer_path = Flags.file_done_dic.get(json_data["number"]).get("trailer")
                if not done_trailer_path:
                    Flags.file_done_dic[json_data["number"]].update({"trailer": trailer_file_path})
                    if trailer_name == 0:  # å¸¦æ–‡ä»¶åï¼Œå·²ä¸‹è½½æˆåŠŸï¼Œåˆ é™¤æ‰é‚£äº›ä¸ç”¨çš„æ–‡ä»¶å¤¹å³å¯
                        if os.path.exists(trailer_old_folder_path):
                            shutil.rmtree(trailer_old_folder_path, ignore_errors=True)
                        if trailer_new_folder_path != trailer_old_folder_path and os.path.exists(
                            trailer_new_folder_path
                        ):
                            shutil.rmtree(trailer_new_folder_path, ignore_errors=True)
                return True
            else:
                json_data["logs"] += "\n ğŸŸ  Trailer size is incorrect! delete it! ({} {}/{}) ".format(
                    json_data["trailer_from"], file_size, content_length
                )
        # åˆ é™¤ä¸‹è½½å¤±è´¥çš„æ–‡ä»¶
        delete_file(trailer_file_path_temp)
        json_data["logs"] += "\n ğŸŸ  Trailer download failed! (%s) " % trailer_url

    if os.path.exists(trailer_file_path):  # ä½¿ç”¨æ—§æ–‡ä»¶
        done_trailer_path = Flags.file_done_dic.get(json_data["number"]).get("trailer")
        if not done_trailer_path:
            Flags.file_done_dic[json_data["number"]].update({"trailer": trailer_file_path})
            if trailer_name == 0:  # å¸¦æ–‡ä»¶åï¼Œå·²ä¸‹è½½æˆåŠŸï¼Œåˆ é™¤æ‰é‚£äº›ä¸ç”¨çš„æ–‡ä»¶å¤¹å³å¯
                if os.path.exists(trailer_old_folder_path):
                    shutil.rmtree(trailer_old_folder_path, ignore_errors=True)
                if trailer_new_folder_path != trailer_old_folder_path and os.path.exists(trailer_new_folder_path):
                    shutil.rmtree(trailer_new_folder_path, ignore_errors=True)
        json_data["logs"] += "\n ğŸŸ  Trailer download failed! å°†ç»§ç»­ä½¿ç”¨ä¹‹å‰çš„æœ¬åœ°æ–‡ä»¶ï¼"
        json_data["logs"] += "\n ğŸ€ Trailer done! (old)(%ss)" % get_used_time(start_time)
        return True


def _get_big_thumb(json_data):
    """
    è·å–èƒŒæ™¯å¤§å›¾ï¼š
    1ï¼Œå®˜ç½‘å›¾ç‰‡
    2ï¼ŒAmazon å›¾ç‰‡
    3ï¼ŒGoogle æœå›¾
    """
    start_time = time.time()
    if "thumb" not in config.download_hd_pics:
        return json_data
    number = json_data["number"]
    letters = json_data["letters"]
    number_lower_line = number.lower()
    number_lower_no_line = number_lower_line.replace("-", "")
    thumb_width = 0

    if json_data["cover_from"] == 'dmm':
        if json_data["cover"]:
            thumb_width, h = get_imgsize(json_data["cover"])
            # å¯¹äºå­˜åœ¨ dmm 2K æ¨ªç‰ˆå°é¢çš„å½±ç‰‡, ç›´æ¥ä¸‹è½½å…¶ç«–ç‰ˆå°é¢
            if thumb_width >= 1700:
                json_data["logs"] += "\n ğŸ–¼ HD Thumb found! ({})({}s)".format(
                    json_data["cover_from"], get_used_time(start_time)
                )
                json_data["poster_big"] = True
                return json_data
    # faleno.jp ç•ªå·æ£€æŸ¥ï¼Œéƒ½æ˜¯å¤§å›¾ï¼Œè¿”å›å³å¯
    elif json_data["cover_from"] in ["faleno", "dahlia"]:
        if json_data["cover"]:
            json_data["logs"] += "\n ğŸ–¼ HD Thumb found! ({})({}s)".format(
                json_data["cover_from"], get_used_time(start_time)
            )
        json_data["poster_big"] = True
        return json_data

    # prestige å›¾ç‰‡æœ‰çš„æ˜¯å¤§å›¾ï¼Œéœ€è¦æ£€æµ‹å›¾ç‰‡åˆ†è¾¨ç‡
    elif json_data["cover_from"] in ["prestige", "mgstage"]:
        if json_data["cover"]:
            thumb_width, h = get_imgsize(json_data["cover"])

    # ç‰‡å•†å®˜ç½‘æŸ¥è¯¢
    elif "official" in config.download_hd_pics:
        # faleno.jp ç•ªå·æ£€æŸ¥
        if re.findall(r"F[A-Z]{2}SS", number):
            req_url = "https://faleno.jp/top/works/%s/" % number_lower_no_line
            result, response = get_html(req_url)
            if result:
                temp_url = re.findall(
                    r'src="((https://cdn.faleno.net/top/wp-content/uploads/[^_]+_)([^?]+))\?output-quality=', response
                )
                if temp_url:
                    json_data["cover"] = temp_url[0][0]
                    json_data["poster"] = temp_url[0][1] + "2125.jpg"
                    json_data["cover_from"] = "faleno"
                    json_data["poster_from"] = "faleno"
                    json_data["poster_big"] = True
                    trailer_temp = re.findall(r'class="btn09"><a class="pop_sample" href="([^"]+)', response)
                    if trailer_temp:
                        json_data["trailer"] = trailer_temp[0]
                        json_data["trailer_from"] = "faleno"
                    json_data["logs"] += "\n ğŸ–¼ HD Thumb found! (faleno)(%ss)" % get_used_time(start_time)
                    return json_data

        # km-produce.com ç•ªå·æ£€æŸ¥
        number_letter = letters.lower()
        kmp_key = ["vrkm", "mdtm", "mkmp", "savr", "bibivr", "scvr", "slvr", "averv", "kbvr", "cbikmv"]
        prestige_key = ["abp", "abw", "aka", "prdvr", "pvrbst", "sdvr", "docvr"]
        if number_letter in kmp_key:
            req_url = f"https://km-produce.com/img/title1/{number_lower_line}.jpg"
            real_url = check_url(req_url)
            if real_url:
                json_data["cover"] = real_url
                json_data["cover_from"] = "km-produce"
                json_data["logs"] += "\n ğŸ–¼ HD Thumb found! (km-produce)(%ss)" % (get_used_time(start_time))
                return json_data

        # www.prestige-av.com ç•ªå·æ£€æŸ¥
        elif number_letter in prestige_key:
            number_num = re.findall(r"\d+", number)[0]
            if number_letter == "abw" and int(number_num) > 280:
                pass
            else:
                req_url = f"https://www.prestige-av.com/api/media/goods/prestige/{number_letter}/{number_num}/pb_{number_lower_line}.jpg"
                if number_letter == "docvr":
                    req_url = f"https://www.prestige-av.com/api/media/goods/doc/{number_letter}/{number_num}/pb_{number_lower_line}.jpg"
                if get_imgsize(req_url)[0] >= 800:
                    json_data["cover"] = req_url
                    json_data["poster"] = req_url.replace("/pb_", "/pf_")
                    json_data["cover_from"] = "prestige"
                    json_data["poster_from"] = "prestige"
                    json_data["poster_big"] = True
                    json_data["logs"] += "\n ğŸ–¼ HD Thumb found! (prestige)(%ss)" % (get_used_time(start_time))
                    return json_data

    # ä½¿ç”¨googleä»¥å›¾æœå›¾
    pic_url = json_data.get("cover")
    if "google" in config.download_hd_pics:
        if pic_url and json_data["cover_from"] != "theporndb":
            thumb_url, cover_size = get_big_pic_by_google(pic_url)
            if thumb_url and cover_size[0] > thumb_width:
                json_data["cover_size"] = cover_size
                pic_domain = re.findall(r"://([^/]+)", thumb_url)[0]
                json_data["cover_from"] = f"Google({pic_domain})"
                json_data["cover"] = thumb_url
                json_data["logs"] += "\n ğŸ–¼ HD Thumb found! ({})({}s)".format(
                    json_data["cover_from"], get_used_time(start_time)
                )

    return json_data


def _get_big_poster(json_data):
    start_time = time.time()

    # æœªå‹¾é€‰ä¸‹è½½é«˜æ¸…å›¾posteræ—¶ï¼Œè¿”å›
    if "poster" not in config.download_hd_pics:
        return json_data

    # å¦‚æœæœ‰å¤§å›¾æ—¶ï¼Œç›´æ¥ä¸‹è½½
    if json_data.get("poster_big") and get_imgsize(json_data["poster"])[1] > 600:
        json_data["image_download"] = True
        json_data["logs"] += f"\n ğŸ–¼ HD Poster found! ({json_data['poster_from']})({get_used_time(start_time)}s)"
        return json_data

    # åˆå§‹åŒ–æ•°æ®
    number = json_data.get("number")
    poster_url = json_data.get("poster")
    hd_pic_url = ""
    poster_width = 0

    # é€šè¿‡åŸæ ‡é¢˜å» amazon æŸ¥è¯¢
    if "amazon" in config.download_hd_pics and json_data["mosaic"] in [
        "æœ‰ç ",
        "æœ‰ç¢¼",
        "æµå‡º",
        "æ— ç ç ´è§£",
        "ç„¡ç¢¼ç ´è§£",
        "é‡Œç•ª",
        "è£ç•ª",
        "åŠ¨æ¼«",
        "å‹•æ¼«",
    ]:
        hd_pic_url = get_big_pic_by_amazon(json_data, json_data["originaltitle_amazon"], json_data["actor_amazon"])
        if hd_pic_url:
            json_data["poster"] = hd_pic_url
            json_data["poster_from"] = "Amazon"
        if json_data["poster_from"] == "Amazon":
            json_data["image_download"] = True

    # é€šè¿‡ç•ªå·å» å®˜ç½‘ æŸ¥è¯¢è·å–ç¨å¾®å¤§ä¸€äº›çš„å°é¢å›¾ï¼Œä»¥ä¾¿å» Google æœç´¢
    if (
        not hd_pic_url
        and "official" in config.download_hd_pics
        and "official" not in config.website_set
        and json_data["poster_from"] != "Amazon"
    ):
        letters = json_data["letters"].upper()
        official_url = config.official_websites.get(letters)
        if official_url:
            url_search = official_url + "/search/list?keyword=" + number.replace("-", "")
            result, html_search = get_html(url_search)
            if result:
                poster_url_list = re.findall(r'img class="c-main-bg lazyload" data-src="([^"]+)"', html_search)
                if poster_url_list:
                    # ä½¿ç”¨å®˜ç½‘å›¾ä½œä¸ºå°é¢å» google æœç´¢
                    poster_url = poster_url_list[0]
                    json_data["poster"] = poster_url
                    json_data["poster_from"] = official_url.split(".")[-2].replace("https://", "")
                    # vrä½œå“æˆ–è€…å®˜ç½‘å›¾ç‰‡é«˜åº¦å¤§äº500æ—¶ï¼Œä¸‹è½½å°é¢å›¾å¼€
                    if "VR" in number.upper() or get_imgsize(poster_url)[1] > 500:
                        json_data["image_download"] = True

    # ä½¿ç”¨googleä»¥å›¾æœå›¾ï¼Œæ”¾åœ¨æœ€åæ˜¯å› ä¸ºæœ‰æ—¶æœ‰é”™è¯¯ï¼Œæ¯”å¦‚ kawd-943
    poster_url = json_data.get("poster")
    if (
        not hd_pic_url
        and poster_url
        and "google" in config.download_hd_pics
        and json_data["poster_from"] != "theporndb"
    ):
        hd_pic_url, poster_size = get_big_pic_by_google(poster_url, poster=True)
        if hd_pic_url:
            if "prestige" in json_data["poster"] or json_data["poster_from"] == "Amazon":
                poster_width = get_imgsize(poster_url)[0]
            if poster_size[0] > poster_width:
                json_data["poster"] = hd_pic_url
                json_data["poster_size"] = poster_size
                pic_domain = re.findall(r"://([^/]+)", hd_pic_url)[0]
                json_data["poster_from"] = f"Google({pic_domain})"

    # å¦‚æœæ‰¾åˆ°äº†é«˜æ¸…é“¾æ¥ï¼Œåˆ™æ›¿æ¢
    if hd_pic_url:
        json_data["image_download"] = True
        json_data["logs"] += "\n ğŸ–¼ HD Poster found! ({})({}s)".format(
            json_data["poster_from"], get_used_time(start_time)
        )

    return json_data


def thumb_download(json_data, folder_new_path, thumb_final_path):
    start_time = time.time()
    poster_path = json_data["poster_path"]
    thumb_path = json_data["thumb_path"]
    fanart_path = json_data["fanart_path"]

    # æœ¬åœ°å­˜åœ¨ thumb.jpgï¼Œä¸”å‹¾é€‰ä¿ç•™æ—§æ–‡ä»¶æ—¶ï¼Œä¸ä¸‹è½½
    if thumb_path and "thumb" in config.keep_files:
        json_data["logs"] += "\n ğŸ€ Thumb done! (old)(%ss) " % get_used_time(start_time)
        return True

    # å¦‚æœthumbä¸ä¸‹è½½ï¼Œçœ‹fanartã€posterè¦ä¸è¦ä¸‹è½½ï¼Œéƒ½ä¸ä¸‹è½½åˆ™è¿”å›
    if "thumb" not in config.download_files:
        if "poster" in config.download_files and ("poster" not in config.keep_files or not poster_path):
            pass
        elif "fanart" in config.download_files and ("fanart" not in config.keep_files or not fanart_path):
            pass
        else:
            return True

    # å°è¯•å¤åˆ¶å…¶ä»–åˆ†é›†ã€‚çœ‹åˆ†é›†æœ‰æ²¡æœ‰ä¸‹è½½ï¼Œå¦‚æœä¸‹è½½å®Œæˆåˆ™å¯ä»¥å¤åˆ¶ï¼Œå¦åˆ™å°±è‡ªè¡Œä¸‹è½½
    if json_data["cd_part"]:
        done_thumb_path = Flags.file_done_dic.get(json_data["number"]).get("thumb")
        if (
            done_thumb_path
            and os.path.exists(done_thumb_path)
            and split_path(done_thumb_path)[0] == split_path(thumb_final_path)[0]
        ):
            copy_file(done_thumb_path, thumb_final_path)
            json_data["logs"] += "\n ğŸ€ Thumb done! (copy cd-thumb)(%ss) " % get_used_time(start_time)
            json_data["cover_from"] = "copy cd-thumb"
            json_data["thumb_path"] = thumb_final_path
            return True

    # è·å–é«˜æ¸…èƒŒæ™¯å›¾
    json_data = _get_big_thumb(json_data)

    # ä¸‹è½½å›¾ç‰‡
    cover_url = json_data.get("cover")
    cover_from = json_data.get("cover_from")
    if cover_url:
        cover_list = json_data["cover_list"]
        while [cover_from, cover_url] in cover_list:
            cover_list.remove([cover_from, cover_url])
        cover_list.insert(0, [cover_from, cover_url])

        thumb_final_path_temp = thumb_final_path
        if os.path.exists(thumb_final_path):
            thumb_final_path_temp = thumb_final_path + ".[DOWNLOAD].jpg"
        for each in cover_list:
            if not each[1]:
                continue
            cover_from, cover_url = each
            cover_url = check_url(cover_url)
            if not cover_url:
                json_data["logs"] += (
                    f"\n ğŸŸ  æ£€æµ‹åˆ° Thumb å›¾ç‰‡å¤±æ•ˆ! è·³è¿‡ï¼({cover_from})({get_used_time(start_time)}s) " + each[1]
                )
                continue
            json_data["cover_from"] = cover_from
            if download_file_with_filepath(json_data, cover_url, thumb_final_path_temp, folder_new_path):
                cover_size = check_pic(thumb_final_path_temp)
                if cover_size:
                    if (
                        not cover_from.startswith("Google")
                        or cover_size == json_data["cover_size"]
                        or (
                            cover_size[0] >= 800
                            and abs(
                                cover_size[0] / cover_size[1] - json_data["cover_size"][0] / json_data["cover_size"][1]
                            )
                            <= 0.1
                        )
                    ):
                        # å›¾ç‰‡ä¸‹è½½æ­£å¸¸ï¼Œæ›¿æ¢æ—§çš„ thumb.jpg
                        if thumb_final_path_temp != thumb_final_path:
                            move_file(thumb_final_path_temp, thumb_final_path)
                            delete_file(thumb_final_path_temp)
                        if json_data["cd_part"]:
                            dic = {"thumb": thumb_final_path}
                            Flags.file_done_dic[json_data["number"]].update(dic)
                        json_data["thumb_marked"] = False  # è¡¨ç¤ºè¿˜æ²¡æœ‰èµ°åŠ æ°´å°æµç¨‹
                        json_data["logs"] += "\n ğŸ€ Thumb done! ({})({}s) ".format(
                            json_data["cover_from"], get_used_time(start_time)
                        )
                        json_data["thumb_path"] = thumb_final_path
                        return True
                    else:
                        delete_file(thumb_final_path_temp)
                        json_data["logs"] += (
                            f"\n ğŸŸ  æ£€æµ‹åˆ° Thumb åˆ†è¾¨ç‡ä¸å¯¹{str(cover_size)}! å·²åˆ é™¤ ({cover_from})({get_used_time(start_time)}s)"
                        )
                        continue
                json_data["logs"] += f"\n ğŸŸ  Thumb download failed! {cover_from}: {cover_url} "
    else:
        json_data["logs"] += "\n ğŸŸ  Thumb url is empty! "

    # ä¸‹è½½å¤±è´¥ï¼Œæœ¬åœ°æœ‰å›¾
    if thumb_path:
        json_data["logs"] += "\n ğŸŸ  Thumb download failed! å°†ç»§ç»­ä½¿ç”¨ä¹‹å‰çš„å›¾ç‰‡ï¼"
        json_data["logs"] += "\n ğŸ€ Thumb done! (old)(%ss) " % get_used_time(start_time)
        return True
    else:
        if "ignore_pic_fail" in config.download_files:
            json_data["logs"] += "\n ğŸŸ  Thumb download failed! (ä½ å·²å‹¾é€‰ã€Œå›¾ç‰‡ä¸‹è½½å¤±è´¥æ—¶ï¼Œä¸è§†ä¸ºå¤±è´¥ï¼ã€) "
            json_data["logs"] += "\n ğŸ€ Thumb done! (none)(%ss)" % get_used_time(start_time)
            return True
        else:
            json_data["logs"] += (
                "\n ğŸ”´ Thumb download failed! ä½ å¯ä»¥åˆ°ã€Œè®¾ç½®ã€-ã€Œä¸‹è½½ã€ï¼Œå‹¾é€‰ã€Œå›¾ç‰‡ä¸‹è½½å¤±è´¥æ—¶ï¼Œä¸è§†ä¸ºå¤±è´¥ï¼ã€ "
            )
            json_data["error_info"] = (
                "Thumb download failed! ä½ å¯ä»¥åˆ°ã€Œè®¾ç½®ã€-ã€Œä¸‹è½½ã€ï¼Œå‹¾é€‰ã€Œå›¾ç‰‡ä¸‹è½½å¤±è´¥æ—¶ï¼Œä¸è§†ä¸ºå¤±è´¥ï¼ã€"
            )
            return False


def poster_download(json_data, folder_new_path, poster_final_path):
    start_time = time.time()
    download_files = config.download_files
    keep_files = config.keep_files
    poster_path = json_data["poster_path"]
    thumb_path = json_data["thumb_path"]
    fanart_path = json_data["fanart_path"]
    image_cut = ""

    # ä¸ä¸‹è½½posterã€ä¸ä¿ç•™posteræ—¶ï¼Œè¿”å›
    if "poster" not in download_files and "poster" not in keep_files:
        if poster_path:
            delete_file(poster_path)
        return True

    # æœ¬åœ°æœ‰posteræ—¶ï¼Œä¸”å‹¾é€‰ä¿ç•™æ—§æ–‡ä»¶æ—¶ï¼Œä¸ä¸‹è½½
    if poster_path and "poster" in keep_files:
        json_data["logs"] += "\n ğŸ€ Poster done! (old)(%ss)" % get_used_time(start_time)
        return True

    # ä¸ä¸‹è½½æ—¶è¿”å›
    if "poster" not in download_files:
        return True

    # å°è¯•å¤åˆ¶å…¶ä»–åˆ†é›†ã€‚çœ‹åˆ†é›†æœ‰æ²¡æœ‰ä¸‹è½½ï¼Œå¦‚æœä¸‹è½½å®Œæˆåˆ™å¯ä»¥å¤åˆ¶ï¼Œå¦åˆ™å°±è‡ªè¡Œä¸‹è½½
    if json_data["cd_part"]:
        done_poster_path = Flags.file_done_dic.get(json_data["number"]).get("poster")
        if (
            done_poster_path
            and os.path.exists(done_poster_path)
            and split_path(done_poster_path)[0] == split_path(poster_final_path)[0]
        ):
            copy_file(done_poster_path, poster_final_path)
            json_data["poster_from"] = "copy cd-poster"
            json_data["poster_path"] = poster_final_path
            json_data["logs"] += "\n ğŸ€ Poster done! (copy cd-poster)(%ss)" % get_used_time(start_time)
            return True

    # å‹¾é€‰å¤åˆ¶ thumbæ—¶ï¼šå›½äº§ï¼Œå¤åˆ¶thumbï¼›æ— ç ï¼Œå‹¾é€‰ä¸è£å‰ªæ—¶ï¼Œä¹Ÿå¤åˆ¶thumb
    if thumb_path:
        mosaic = json_data["mosaic"]
        number = json_data["number"]
        copy_flag = False
        if number.startswith("FC2"):
            image_cut = "center"
            if "ignore_fc2" in download_files:
                copy_flag = True
        elif mosaic == "å›½äº§" or mosaic == "åœ‹ç”¢":
            image_cut = "right"
            if "ignore_guochan" in download_files:
                copy_flag = True
        elif mosaic == "æ— ç " or mosaic == "ç„¡ç¢¼" or mosaic == "ç„¡ä¿®æ­£":
            image_cut = "center"
            if "ignore_wuma" in download_files:
                copy_flag = True
        elif mosaic == "æœ‰ç " or mosaic == "æœ‰ç¢¼":
            if "ignore_youma" in download_files:
                copy_flag = True
        if copy_flag:
            copy_file(thumb_path, poster_final_path)
            json_data["poster_marked"] = json_data["thumb_marked"]
            json_data["poster_from"] = "copy thumb"
            json_data["poster_path"] = poster_final_path
            json_data["logs"] += "\n ğŸ€ Poster done! (copy thumb)(%ss)" % get_used_time(start_time)
            return True

    # è·å–é«˜æ¸… poster
    json_data = _get_big_poster(json_data)

    # ä¸‹è½½å›¾ç‰‡
    poster_url = json_data.get("poster")
    poster_from = json_data.get("poster_from")
    poster_final_path_temp = poster_final_path
    if os.path.exists(poster_final_path):
        poster_final_path_temp = poster_final_path + ".[DOWNLOAD].jpg"
    if json_data["image_download"]:
        start_time = time.time()
        if download_file_with_filepath(json_data, poster_url, poster_final_path_temp, folder_new_path):
            poster_size = check_pic(poster_final_path_temp)
            if poster_size:
                if (
                    not poster_from.startswith("Google")
                    or poster_size == json_data["poster_size"]
                    or "media-amazon.com" in poster_url
                ):
                    if poster_final_path_temp != poster_final_path:
                        move_file(poster_final_path_temp, poster_final_path)
                        delete_file(poster_final_path_temp)
                    if json_data["cd_part"]:
                        dic = {"poster": poster_final_path}
                        Flags.file_done_dic[json_data["number"]].update(dic)
                    json_data["poster_marked"] = False  # ä¸‹è½½çš„å›¾ï¼Œè¿˜æ²¡åŠ æ°´å°
                    json_data["poster_path"] = poster_final_path
                    json_data["logs"] += f"\n ğŸ€ Poster done! ({poster_from})({get_used_time(start_time)}s)"
                    return True
                else:
                    delete_file(poster_final_path_temp)
                    json_data["logs"] += f"\n ğŸŸ  æ£€æµ‹åˆ° Poster åˆ†è¾¨ç‡ä¸å¯¹{str(poster_size)}! å·²åˆ é™¤ ({poster_from})"

    # åˆ¤æ–­ä¹‹å‰æœ‰æ²¡æœ‰ poster å’Œ thumb
    if not poster_path and not thumb_path:
        json_data["poster_path"] = ""
        if "ignore_pic_fail" in download_files:
            json_data["logs"] += "\n ğŸŸ  Poster download failed! (ä½ å·²å‹¾é€‰ã€Œå›¾ç‰‡ä¸‹è½½å¤±è´¥æ—¶ï¼Œä¸è§†ä¸ºå¤±è´¥ï¼ã€) "
            json_data["logs"] += "\n ğŸ€ Poster done! (none)(%ss)" % get_used_time(start_time)
            return True
        else:
            json_data["logs"] += (
                "\n ğŸ”´ Poster download failed! ä½ å¯ä»¥åˆ°ã€Œè®¾ç½®ã€-ã€Œä¸‹è½½ã€ï¼Œå‹¾é€‰ã€Œå›¾ç‰‡ä¸‹è½½å¤±è´¥æ—¶ï¼Œä¸è§†ä¸ºå¤±è´¥ï¼ã€ "
            )
            json_data["error_info"] = (
                "Poster download failed! ä½ å¯ä»¥åˆ°ã€Œè®¾ç½®ã€-ã€Œä¸‹è½½ã€ï¼Œå‹¾é€‰ã€Œå›¾ç‰‡ä¸‹è½½å¤±è´¥æ—¶ï¼Œä¸è§†ä¸ºå¤±è´¥ï¼ã€"
            )
            return False

    # ä½¿ç”¨thumbè£å‰ª
    poster_final_path_temp = poster_final_path + ".[CUT].jpg"
    if fanart_path:
        thumb_path = fanart_path
    if cut_thumb_to_poster(json_data, thumb_path, poster_final_path_temp, image_cut):
        # è£å‰ªæˆåŠŸï¼Œæ›¿æ¢æ—§å›¾
        move_file(poster_final_path_temp, poster_final_path)
        if json_data["cd_part"]:
            dic = {"poster": poster_final_path}
            Flags.file_done_dic[json_data["number"]].update(dic)
        json_data["poster_path"] = poster_final_path
        json_data["poster_marked"] = False
        return True

    # è£å‰ªå¤±è´¥ï¼Œæœ¬åœ°æœ‰å›¾
    if poster_path:
        json_data["logs"] += "\n ğŸŸ  Poster cut failed! å°†ç»§ç»­ä½¿ç”¨ä¹‹å‰çš„å›¾ç‰‡ï¼"
        json_data["logs"] += "\n ğŸ€ Poster done! (old)(%ss) " % get_used_time(start_time)
        return True
    else:
        if "ignore_pic_fail" in download_files:
            json_data["logs"] += "\n ğŸŸ  Poster cut failed! (ä½ å·²å‹¾é€‰ã€Œå›¾ç‰‡ä¸‹è½½å¤±è´¥æ—¶ï¼Œä¸è§†ä¸ºå¤±è´¥ï¼ã€) "
            json_data["logs"] += "\n ğŸ€ Poster done! (none)(%ss)" % get_used_time(start_time)
            return True
        else:
            json_data["logs"] += (
                "\n ğŸ”´ Poster cut failed! ä½ å¯ä»¥åˆ°ã€Œè®¾ç½®ã€-ã€Œä¸‹è½½ã€ï¼Œå‹¾é€‰ã€Œå›¾ç‰‡ä¸‹è½½å¤±è´¥æ—¶ï¼Œä¸è§†ä¸ºå¤±è´¥ï¼ã€ "
            )
            json_data["error_info"] = "Poster failedï¼ä½ å¯ä»¥åˆ°ã€Œè®¾ç½®ã€-ã€Œä¸‹è½½ã€ï¼Œå‹¾é€‰ã€Œå›¾ç‰‡ä¸‹è½½å¤±è´¥æ—¶ï¼Œä¸è§†ä¸ºå¤±è´¥ï¼ã€"
            return False


def fanart_download(json_data, fanart_final_path):
    """
    å¤åˆ¶thumbä¸ºfanart
    """
    start_time = time.time()
    thumb_path = json_data["thumb_path"]
    fanart_path = json_data["fanart_path"]
    download_files = config.download_files
    keep_files = config.keep_files

    # ä¸ä¿ç•™ä¸ä¸‹è½½æ—¶åˆ é™¤è¿”å›
    if ",fanart" not in keep_files and ",fanart" not in download_files:
        if fanart_path and os.path.exists(fanart_path):
            delete_file(fanart_path)
        return True

    # ä¿ç•™ï¼Œå¹¶ä¸”æœ¬åœ°å­˜åœ¨ fanart.jpgï¼Œä¸ä¸‹è½½è¿”å›
    if ",fanart" in keep_files and fanart_path:
        json_data["logs"] += "\n ğŸ€ Fanart done! (old)(%ss)" % get_used_time(start_time)
        return True

    # ä¸ä¸‹è½½æ—¶ï¼Œè¿”å›
    if ",fanart" not in download_files:
        return True

    # å°è¯•å¤åˆ¶å…¶ä»–åˆ†é›†ã€‚çœ‹åˆ†é›†æœ‰æ²¡æœ‰ä¸‹è½½ï¼Œå¦‚æœä¸‹è½½å®Œæˆåˆ™å¯ä»¥å¤åˆ¶ï¼Œå¦åˆ™å°±è‡ªè¡Œä¸‹è½½
    if json_data["cd_part"]:
        done_fanart_path = Flags.file_done_dic.get(json_data["number"]).get("fanart")
        if (
            done_fanart_path
            and os.path.exists(done_fanart_path)
            and split_path(done_fanart_path)[0] == split_path(fanart_final_path)[0]
        ):
            if fanart_path:
                delete_file(fanart_path)
            copy_file(done_fanart_path, fanart_final_path)
            json_data["fanart_from"] = "copy cd-fanart"
            json_data["fanart_path"] = fanart_final_path
            json_data["logs"] += "\n ğŸ€ Fanart done! (copy cd-fanart)(%ss)" % get_used_time(start_time)
            return True

    # å¤åˆ¶thumb
    if thumb_path:
        if fanart_path:
            delete_file(fanart_path)
        copy_file(thumb_path, fanart_final_path)
        json_data["fanart_from"] = "copy thumb"
        json_data["fanart_path"] = fanart_final_path
        json_data["fanart_marked"] = json_data["thumb_marked"]
        json_data["logs"] += "\n ğŸ€ Fanart done! (copy thumb)(%ss)" % get_used_time(start_time)
        if json_data["cd_part"]:
            dic = {"fanart": fanart_final_path}
            Flags.file_done_dic[json_data["number"]].update(dic)
        return True
    else:
        # æœ¬åœ°æœ‰ fanart æ—¶ï¼Œä¸ä¸‹è½½
        if fanart_path:
            json_data["logs"] += "\n ğŸŸ  Fanart copy failed! æœªæ‰¾åˆ° thumb å›¾ç‰‡ï¼Œå°†ç»§ç»­ä½¿ç”¨ä¹‹å‰çš„å›¾ç‰‡ï¼"
            json_data["logs"] += "\n ğŸ€ Fanart done! (old)(%ss)" % get_used_time(start_time)
            return True

        else:
            if "ignore_pic_fail" in download_files:
                json_data["logs"] += "\n ğŸŸ  Fanart failed! (ä½ å·²å‹¾é€‰ã€Œå›¾ç‰‡ä¸‹è½½å¤±è´¥æ—¶ï¼Œä¸è§†ä¸ºå¤±è´¥ï¼ã€) "
                json_data["logs"] += "\n ğŸ€ Fanart done! (none)(%ss)" % get_used_time(start_time)
                return True
            else:
                json_data["logs"] += (
                    "\n ğŸ”´ Fanart failed! ä½ å¯ä»¥åˆ°ã€Œè®¾ç½®ã€-ã€Œä¸‹è½½ã€ï¼Œå‹¾é€‰ã€Œå›¾ç‰‡ä¸‹è½½å¤±è´¥æ—¶ï¼Œä¸è§†ä¸ºå¤±è´¥ï¼ã€ "
                )
                json_data["error_info"] = (
                    "Fanart ä¸‹è½½å¤±è´¥ï¼ä½ å¯ä»¥åˆ°ã€Œè®¾ç½®ã€-ã€Œä¸‹è½½ã€ï¼Œå‹¾é€‰ã€Œå›¾ç‰‡ä¸‹è½½å¤±è´¥æ—¶ï¼Œä¸è§†ä¸ºå¤±è´¥ï¼ã€"
                )
                return False


def extrafanart_download(json_data, folder_new_path):
    start_time = time.time()
    download_files = config.download_files
    keep_files = config.keep_files
    extrafanart_list = json_data.get("extrafanart")
    extrafanart_folder_path = os.path.join(folder_new_path, "extrafanart")

    # ä¸ä¸‹è½½ä¸ä¿ç•™æ—¶åˆ é™¤è¿”å›
    if "extrafanart" not in download_files and "extrafanart" not in keep_files:
        if os.path.exists(extrafanart_folder_path):
            shutil.rmtree(extrafanart_folder_path, ignore_errors=True)
        return

    # æœ¬åœ°å­˜åœ¨ extrafanart_folderï¼Œä¸”å‹¾é€‰ä¿ç•™æ—§æ–‡ä»¶æ—¶ï¼Œä¸ä¸‹è½½
    if "extrafanart" in keep_files and os.path.exists(extrafanart_folder_path):
        json_data["logs"] += "\n ğŸ€ Extrafanart done! (old)(%ss) " % get_used_time(start_time)
        return True

    # å¦‚æœ extrafanart ä¸ä¸‹è½½
    if "extrafanart" not in download_files:
        return True

    # æ£€æµ‹é“¾æ¥æœ‰æ•ˆæ€§
    if extrafanart_list and check_url(extrafanart_list[0]):
        extrafanart_folder_path_temp = extrafanart_folder_path
        if os.path.exists(extrafanart_folder_path_temp):
            extrafanart_folder_path_temp = extrafanart_folder_path + "[DOWNLOAD]"
            if not os.path.exists(extrafanart_folder_path_temp):
                os.makedirs(extrafanart_folder_path_temp)
        else:
            os.makedirs(extrafanart_folder_path_temp)

        extrafanart_count = 0
        extrafanart_count_succ = 0
        task_list = []
        for extrafanart_url in extrafanart_list:
            extrafanart_count += 1
            extrafanart_name = "fanart" + str(extrafanart_count) + ".jpg"
            extrafanart_file_path = os.path.join(extrafanart_folder_path_temp, extrafanart_name)
            task_list.append(
                [json_data, extrafanart_url, extrafanart_file_path, extrafanart_folder_path_temp, extrafanart_name]
            )
        extrafanart_pool = Pool(20)  # å‰§ç…§ä¸‹è½½çº¿ç¨‹æ± 
        result = extrafanart_pool.map(_mutil_extrafanart_download_thread, task_list)
        for res in result:
            if res:
                extrafanart_count_succ += 1
        if extrafanart_count_succ == extrafanart_count:
            if extrafanart_folder_path_temp != extrafanart_folder_path:
                shutil.rmtree(extrafanart_folder_path)
                os.rename(extrafanart_folder_path_temp, extrafanart_folder_path)
            json_data["logs"] += "\n ğŸ€ ExtraFanart done! ({} {}/{})({}s)".format(
                json_data["extrafanart_from"], extrafanart_count_succ, extrafanart_count, get_used_time(start_time)
            )
            return True
        else:
            json_data["logs"] += "\n ğŸŸ   ExtraFanart download failed! ({} {}/{})({}s)".format(
                json_data["extrafanart_from"], extrafanart_count_succ, extrafanart_count, get_used_time(start_time)
            )
            if extrafanart_folder_path_temp != extrafanart_folder_path:
                shutil.rmtree(extrafanart_folder_path_temp)
            else:
                json_data["logs"] += "\n ğŸ€ ExtraFanart done! (incomplete)(%ss)" % get_used_time(start_time)
                return False
        json_data["logs"] += "\n ğŸŸ  ExtraFanart download failed! å°†ç»§ç»­ä½¿ç”¨ä¹‹å‰çš„æœ¬åœ°æ–‡ä»¶ï¼"
    if os.path.exists(extrafanart_folder_path):  # ä½¿ç”¨æ—§æ–‡ä»¶
        json_data["logs"] += "\n ğŸ€ ExtraFanart done! (old)(%ss)" % get_used_time(start_time)
        return True


def show_netstatus():
    signal.show_net_info(time.strftime("%Y-%m-%d %H:%M:%S").center(80, "="))
    proxy_type = ""
    retry_count = 0
    proxy = ""
    timeout = 0
    try:
        proxy_type, proxy, timeout, retry_count = config.type, config.proxy, config.timeout, config.retry
    except:
        signal.show_traceback_log(traceback.format_exc())
        signal.show_net_info(traceback.format_exc())
    if proxy == "" or proxy_type == "" or proxy_type == "no":
        signal.show_net_info(
            " å½“å‰ç½‘ç»œçŠ¶æ€ï¼šâŒ æœªå¯ç”¨ä»£ç†\n   ç±»å‹ï¼š "
            + str(proxy_type)
            + "    åœ°å€ï¼š"
            + str(proxy)
            + "    è¶…æ—¶æ—¶é—´ï¼š"
            + str(timeout)
            + "    é‡è¯•æ¬¡æ•°ï¼š"
            + str(retry_count)
        )
    else:
        signal.show_net_info(
            " å½“å‰ç½‘ç»œçŠ¶æ€ï¼šâœ… å·²å¯ç”¨ä»£ç†\n   ç±»å‹ï¼š "
            + proxy_type
            + "    åœ°å€ï¼š"
            + proxy
            + "    è¶…æ—¶æ—¶é—´ï¼š"
            + str(timeout)
            + "    é‡è¯•æ¬¡æ•°ï¼š"
            + str(retry_count)
        )
    signal.show_net_info("=" * 80)


def check_proxyChange():
    new_proxy = (config.type, config.proxy, config.timeout, config.retry)
    if Flags.current_proxy:
        if new_proxy != Flags.current_proxy:
            signal.show_net_info("\nğŸŒˆ ä»£ç†è®¾ç½®å·²æ”¹å˜ï¼š")
            show_netstatus()
    Flags.current_proxy = new_proxy
